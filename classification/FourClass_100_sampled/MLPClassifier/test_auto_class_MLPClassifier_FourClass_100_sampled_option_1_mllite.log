          X_0       X_1       X_2  ...      X_98      X_99  target
0    0.935563  2.247500 -1.070940  ... -0.177791 -0.249523       3
1    0.293314 -1.260450 -3.448018  ...  0.164190  2.205145       2
2    0.596661  1.589408 -0.810968  ...  0.026429 -0.565740       0
3    1.456436 -2.080544  0.694122  ...  1.059889  0.328791       1
4   -1.193096 -0.499944  0.528137  ...  1.236806  1.097111       2
..        ...       ...       ...  ...       ...       ...     ...
123  0.634123 -0.617818 -2.241496  ... -0.966775 -0.710358       2
124  0.918349 -0.311832 -0.471041  ... -0.317160  0.826388       1
125 -1.275761  1.107036  0.345849  ...  0.566895  0.570030       1
126  0.529198  1.241322  0.500518  ... -1.251546 -1.179887       0
127 -0.098289 -0.628792 -0.750264  ...  0.882400 -0.271507       3

[128 rows x 101 columns]
MLLITE_MODEL_SET_OPTIONS MLPClassifier {"hidden_layer_sizes" : [4, 8, 6]}
MLLITE_MODEL_OPTIONS_DEFAULT MLPClassifier { "max_iter" : 32}
MLLITE_MODEL_OPTIONS_AFTER_SETTING MLPClassifier { "hidden_layer_sizes" : [4, 8, 6],  "max_iter" : 32 }
MLLITE_MODEL_JSON_AFTER_SETTING MLPClassifier None
('OPERATION_START', 'TRAINING')
[[ 9.35562551e-01  2.24750018e+00 -1.07094014e+00  5.08270264e-01
   1.43985808e-01 -4.33900356e-01  2.20938280e-01 -6.05712712e-01
   1.04623568e+00 -5.11191189e-01 -1.49217200e+00  5.78792810e-01
  -5.20725727e-01  8.71259570e-01 -1.55619383e+00  1.52518547e+00
  -7.36531466e-02 -1.17264986e+00 -6.90029681e-01 -1.00071573e+00
   3.37273180e-01 -5.52994728e-01  4.75280344e-01 -1.40307999e+00
   9.60445762e-01 -4.87094879e-01 -7.87123621e-01  4.54730177e+00
   1.16833878e+00 -5.16704082e-01  1.26904652e-01 -2.69020295e+00
  -1.60714972e+00 -1.60194361e+00  1.45940697e+00 -2.07469568e-01
  -7.90602565e-01 -2.78487932e-02 -1.21024287e+00  5.58430433e-01
  -1.63653836e-01  6.32869065e-01 -1.25206620e-01 -3.34407538e-01
   6.30639970e-01  1.33564019e+00  3.52602506e+00  7.06132531e-01
   1.95425427e+00 -2.52463102e-01 -4.60086882e-01 -1.19924438e+00
   6.41360343e-01 -2.27416492e+00 -1.08663130e+00 -1.79455566e+00
  -4.53341693e-01 -5.69205701e-01 -2.71141529e-01  6.19319558e-01
  -2.58924603e+00  9.84492600e-01  2.16141731e-01  8.48047435e-01
  -2.20009804e-01 -3.31227589e+00  1.16689853e-01  1.42259753e+00
   1.79406500e+00 -1.90514177e-01  7.55192041e-01  1.54058591e-01
   7.11078942e-01 -1.32698989e+00 -2.07093787e+00  9.34237540e-01
  -4.60939616e-01 -8.04052234e-01 -5.02367616e-02 -3.75201797e+00
   6.09228611e-02 -1.00079381e+00 -9.43618715e-01  2.28104413e-01
  -9.84539762e-02 -8.51119459e-02  8.69812146e-02 -1.41765642e+00
   2.71681398e-01 -8.74418080e-01 -2.53257823e+00 -5.78377783e-01
  -7.53983378e-01  9.52363431e-01 -9.63835180e-01 -6.48602486e-01
  -1.77499366e+00  6.77245975e-01 -1.77790895e-01 -2.49523029e-01]
 [ 2.93314219e-01 -1.26045048e+00 -3.44801831e+00  2.69382149e-01
   1.54421818e+00 -6.00797161e-02  2.80081749e-01 -3.49374235e-01
  -1.71003997e+00  8.22554767e-01  7.01598311e-03  7.73456335e-01
  -1.02132368e+00 -1.34428933e-01 -2.03461379e-01  1.28476620e+00
  -1.16496360e+00 -5.35073131e-02  8.57034981e-01  1.38256717e+00
   9.03933793e-02  8.63964017e-03 -5.40078878e-01 -4.88574624e-01
   4.52568620e-01  5.42306304e-01  1.07568896e+00  3.46140218e+00
   3.28359604e+00 -3.20053488e-01 -1.14804339e+00 -2.76097941e+00
   1.55631797e-02  8.74092340e-01 -7.98800826e-01 -2.42959595e+00
  -6.76247656e-01 -1.14995682e+00  4.39849287e-01 -1.87727064e-01
   1.34595573e+00 -9.83103573e-01 -8.30253839e-01 -1.39651984e-01
   2.03547105e-01  4.51490223e-01 -3.95970130e+00 -3.51787716e-01
   7.84379542e-01 -4.29442495e-01  1.05397455e-01  7.79593885e-01
  -1.13827668e-01 -5.40005386e-01 -1.16847146e+00 -3.36890638e-01
   9.90479112e-01  5.00425518e-01 -2.40916753e+00 -8.40187371e-01
  -2.07494095e-01 -4.10496503e-01  4.08632898e+00  9.31320012e-01
   9.54048574e-01 -9.11957398e-02 -7.51672363e+00  7.23851264e-01
  -3.96382175e-02  1.77101314e+00 -1.17600811e+00  1.22991554e-01
  -3.74346748e-02  1.72770336e-01 -4.08644319e-01  4.94354963e-01
   1.74994171e+00  9.89653885e-01  1.74723223e-01 -9.37665176e+00
  -4.31565335e-03 -1.70145118e+00 -1.74992526e+00 -1.72720656e-01
   5.82682490e-01 -5.26583374e-01  1.91324139e+00 -2.40155920e-01
  -1.35112417e+00 -3.65208060e-01 -1.35532707e-01  1.08672369e+00
   7.04294443e-01 -8.71233284e-01  1.04183960e+00 -6.46791577e-01
   3.74140948e-01  3.27620208e-01  1.64190233e-01  2.20514464e+00]
 [ 5.96660793e-01  1.58940768e+00 -8.10967982e-01 -4.73919630e-01
  -9.17869389e-01 -1.61617601e+00 -6.78639174e-01  1.03318654e-01
  -5.00028312e-01  1.78057873e+00 -1.34553814e+00 -5.28248453e+00
   7.20736444e-01  1.79546729e-01 -1.75809467e+00 -8.59387755e-01
  -2.27263141e-02 -1.87025774e+00  1.67022240e+00 -2.89478928e-01
   8.37067842e-01  6.59622908e-01  4.29693535e-02 -9.46969926e-01
   4.30729359e-01 -7.75742769e-01  3.41510355e-01 -2.02011466e+00
  -1.02778935e+00 -1.69667840e+00 -1.71012551e-01  4.23166782e-01
  -1.79182720e+00 -5.75773478e-01 -1.71246231e+00  5.68344474e-01
  -6.81339622e-01  3.23087126e-01 -1.44740534e+00 -1.38365650e+00
   3.77985537e-01 -6.37483299e-01  7.76762664e-01  5.55373728e-01
   1.11285400e+00 -1.27088690e+00  2.94207788e+00  4.38418001e-01
   7.77927995e-01  8.14674795e-01 -3.88954520e-01 -1.27854240e+00
   3.87139380e-01 -5.47252417e-01  7.68055022e-01  1.48158407e+00
  -5.30028522e-01  8.71521354e-01  2.20103472e-01 -1.80267835e+00
   1.62707639e+00  2.93988436e-01  4.04036474e+00 -1.38034463e+00
  -8.25688243e-01  7.04825699e-01  8.15395546e+00  1.22501051e+00
   1.66788650e+00  2.72147131e+00  6.49030805e-01 -9.60100591e-01
  -1.05595326e+00 -4.90769893e-02 -1.56489909e+00  3.93039435e-01
   8.80326271e-01  1.46846676e+00  2.43020996e-01 -1.33081412e+00
   2.29413652e+00  7.48559713e-01  1.10301085e-01  1.11530209e+00
  -3.07022452e-01 -3.43551278e-01  7.95005918e-01  2.63235778e-01
   7.99354970e-01 -5.73727824e-02  7.56435156e-01 -3.64444673e-01
   1.29907846e+00 -3.82006057e-02  7.76137769e-01 -8.31591904e-01
   1.53442824e+00 -1.27538812e+00  2.64287349e-02 -5.65739751e-01]
 [ 1.45643616e+00 -2.08054376e+00  6.94121957e-01 -5.21965921e-01
  -2.32917964e-01  1.52993643e+00 -1.81002557e-01 -9.00278687e-01
  -3.17071170e-01  4.05129343e-01 -9.13353622e-01  1.52238500e+00
   1.75047374e+00 -7.30361819e-01  1.55085921e+00  1.42706335e-01
   4.59196642e-02  4.07628447e-01 -3.26506257e-01 -4.81859356e-01
   9.39201862e-02 -2.45961189e-01 -1.96491504e+00  5.07482708e-01
   1.15683150e+00 -3.67391527e-01 -9.73286927e-01 -3.10479784e+00
  -2.73889601e-01  1.66504610e+00 -1.51400971e+00 -9.55533683e-01
   2.07410976e-01  1.52923656e+00 -1.69865060e+00  1.74417377e+00
  -1.26610243e+00 -5.80420077e-01 -8.55364025e-01  1.04407564e-01
   5.93953609e-01 -1.41621888e+00  1.08122841e-01 -4.84067738e-01
   1.69581461e+00  1.38722503e+00 -3.99619937e+00  1.27301514e+00
   6.38230085e-01  2.15354633e+00 -8.86631683e-02  9.06281710e-01
   1.31646347e+00  1.80131924e+00  5.43839991e-01 -1.20308149e+00
   1.03996921e+00  4.74938527e-02 -2.78281540e-01 -1.01100028e+00
   1.63605332e+00 -6.30008936e-01  2.86958098e+00  1.29696774e+00
   1.89161137e-01  2.85747457e+00 -4.18727446e+00 -7.44388819e-01
   2.98897564e-01  1.36116946e+00  1.38416719e+00 -2.11131483e-01
   1.18849802e+00  1.96934193e-01 -1.47064614e+00 -3.63175780e-01
   7.84732163e-01 -1.32636324e-01  1.00253135e-01  2.38688135e+00
  -2.33116910e-01  4.48122442e-01  2.28070378e+00 -1.26993442e+00
  -3.67890954e-01 -8.16320002e-01 -9.08941507e-01 -7.01003969e-01
   1.05856764e+00 -3.64899427e-01 -2.01056767e+00 -1.06769252e+00
   1.60698414e+00 -2.91990757e-01  2.26473403e+00 -4.49902713e-01
   4.62933123e-01  7.53116310e-01  1.05988944e+00  3.28791380e-01]
 [-1.19309628e+00 -4.99944448e-01  5.28136671e-01  1.85364544e-01
  -7.10495830e-01 -8.85336176e-02 -6.38042450e-01  8.81741822e-01
   1.25752389e+00 -8.73580158e-01 -9.55399573e-01 -1.35603696e-01
   4.83997911e-01 -9.96602833e-01  9.37519133e-01 -5.89864291e-02
  -1.13989949e+00 -1.25818241e+00  5.26024342e-01 -7.01516032e-01
   1.32973158e+00  2.44526839e+00 -7.72712529e-01  5.13766646e-01
   4.98695105e-01  1.14376044e+00 -6.54239535e-01 -6.17800094e-02
  -1.03205192e+00  2.77230233e-01 -9.69139159e-01  1.10734671e-01
  -5.05987525e-01  1.02873945e+00  1.02971303e+00 -2.58090401e+00
  -9.99437511e-01  2.12422752e+00 -3.89198899e-01 -9.32601035e-01
  -6.93674445e-01 -4.07995224e-01 -1.90250194e+00 -6.23405516e-01
  -1.66650927e+00 -2.14010692e+00 -3.13505024e-01  9.16595995e-01
  -1.29809892e+00  1.01324391e+00  1.43364155e+00 -1.01352572e+00
  -1.49118865e+00 -2.45811558e+00  1.11938548e+00  4.71531510e-01
   2.87433833e-01  6.17101133e-01  5.58946013e-01 -9.08716619e-01
   2.14687967e+00 -1.05833352e+00 -8.24340701e-01  1.51690736e-01
   1.60771608e+00  9.55081761e-01 -1.60897672e+00 -1.66303873e+00
   7.57982671e-01  3.17573786e-01  3.93135488e-01  1.26543090e-01
   1.48471398e-02 -1.58860159e+00 -5.91678858e-01  3.42316389e-01
  -2.10359603e-01 -1.42355669e+00 -2.23383650e-01 -4.08269119e+00
   9.26247478e-01  5.84456980e-01 -5.01928627e-01 -5.45637131e-01
   6.72929883e-01 -1.34969640e+00  1.73498929e+00 -2.95029342e-01
   7.98496783e-01  1.77032605e-01 -2.69206595e+00  1.26748353e-01
  -5.17131031e-01  9.58465040e-01 -5.24760962e-01  3.00141931e-01
   9.27737057e-02 -5.06614506e-01  1.23680639e+00  1.09711075e+00]] [3 2 0 1 2]
MLLITE_FIT_USING_MODEL mllite_mlp_class.MLPClassifier_ff4_il8
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT' 1 3
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT' 2 3
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 3 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 6 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 9 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 12 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 15 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 18 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 21 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 24 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 27 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 30 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 32 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT' 3 3
('OPERATION_END_ELAPSED', 0.031, 'TRAINING')
{ "hidden_layer_sizes" : [4, 8, 6],  "max_iter" : 32 }
{ "hidden_layer_sizes" : [4, 8, 6],  "max_iter" : 32 }
{
	"metadata" : { "model" : "Rosenblatt_MLP", "version" : "2024-W12", "signature" : "ff4_il8"},
	"options" : {  "hidden_layer_sizes" : [ 4, 8, 6 ], "activation" : "relu", "solver" : "adam", "alpha" : 0.000100, "batch_size" : null, "learning_rate" : "constant", "learning_rate_init" : 0.001000, "power_t" : 0.500000, "max_iter" : 32, "shuffle" : 1, "random_state" : 1789, "tol" : 0.000100, "verbose" : 0, "warm_start" : 0, "momentum" : 0.900000, "nesterovs_momentum" : 1, "early_stopping" : 0, "validation_fraction" : 0.100000, "beta_1" : 0.900000, "beta_2" : 0.999000, "epsilon" : 0.000000, "n_iter_no_change" : 10, "max_fun" : 15000 },
	"dataset" : { "dataset_rows" : 128, "dataset_features" : 100 },
	"classes" : [ 0, 1, 2, 3 ],
	"layers" : {
		"sizes" : [ 100, 4, 8, 6, 4 ],
		"Layer_0" : {
			"name" : "Input_Layer",
			"NbInputs" : 0,
			"NbOutputs" : 100 
		},
		"Layer_1" : {
			"name" : "Hidden_Layer_1",
			"NbInputs" : 100,
			"NbOutputs" : 4 ,
			"coeffs_00" : [ -0.019355, 0.149693, -0.129067, 0.002282 ],
			"coeffs_01" : [ 0.015553, -0.140160, 0.041765, 0.035907 ],
			"coeffs_02" : [ 0.194374, 0.031814, 0.195492, -0.014125 ],
			"coeffs_03" : [ 0.205989, -0.208396, -0.129875, 0.121435 ],
			"coeffs_04" : [ 0.099971, -0.095218, -0.092035, -0.154250 ],
			"coeffs_05" : [ 0.129976, -0.249272, -0.008127, -0.023140 ],
			"coeffs_06" : [ -0.078391, -0.207602, -0.089956, 0.226930 ],
			"coeffs_07" : [ -0.046052, -0.027160, -0.192428, -0.011281 ],
			"coeffs_08" : [ -0.052671, 0.091143, -0.021304, 0.069140 ],
			"coeffs_09" : [ -0.152010, 0.241699, -0.160632, 0.155803 ],
			"coeffs_10" : [ -0.140061, 0.162057, 0.025859, 0.240762 ],
			"coeffs_11" : [ -0.120596, -0.209715, 0.089899, 0.119256 ],
			"coeffs_12" : [ 0.183151, 0.180614, 0.215463, -0.124479 ],
			"coeffs_13" : [ 0.114719, 0.174346, -0.176249, 0.136254 ],
			"coeffs_14" : [ 0.074665, 0.021985, 0.062851, 0.120377 ],
			"coeffs_15" : [ -0.177158, 0.035164, 0.123622, 0.107133 ],
			"coeffs_16" : [ -0.237292, 0.171352, -0.153503, -0.200366 ],
			"coeffs_17" : [ 0.259349, 0.255341, -0.036393, -0.222130 ],
			"coeffs_18" : [ 0.025211, 0.212050, -0.204294, 0.066572 ],
			"coeffs_19" : [ 0.133861, 0.150866, 0.230406, -0.151780 ],
			"coeffs_20" : [ 0.092262, 0.146920, 0.204684, -0.055906 ],
			"coeffs_21" : [ -0.244309, 0.151481, 0.011869, -0.030979 ],
			"coeffs_22" : [ -0.144800, -0.243859, -0.071832, 0.187740 ],
			"coeffs_23" : [ 0.001169, -0.077694, -0.054806, -0.184164 ],
			"coeffs_24" : [ 0.050311, -0.102074, -0.074096, -0.077550 ],
			"coeffs_25" : [ 0.065578, -0.151179, -0.050791, 0.112867 ],
			"coeffs_26" : [ 0.128479, 0.175643, 0.206986, 0.161659 ],
			"coeffs_27" : [ -0.074724, -0.002575, 0.094973, 0.134391 ],
			"coeffs_28" : [ 0.086519, -0.028827, 0.166855, 0.179443 ],
			"coeffs_29" : [ 0.054648, -0.106955, -0.086993, -0.117506 ],
			"coeffs_30" : [ 0.099251, 0.151246, -0.179326, 0.048002 ],
			"coeffs_31" : [ -0.182476, 0.007832, 0.074438, -0.064549 ],
			"coeffs_32" : [ 0.077945, -0.252833, -0.182165, 0.194286 ],
			"coeffs_33" : [ 0.187337, -0.187939, -0.062305, -0.209211 ],
			"coeffs_34" : [ -0.155454, -0.215141, -0.113243, -0.061541 ],
			"coeffs_35" : [ -0.188516, -0.153019, -0.194296, 0.079293 ],
			"coeffs_36" : [ -0.187416, -0.211356, 0.074663, 0.088159 ],
			"coeffs_37" : [ -0.007274, 0.060781, 0.056130, -0.186288 ],
			"coeffs_38" : [ -0.014757, 0.223600, 0.271790, -0.071574 ],
			"coeffs_39" : [ -0.204582, -0.101483, 0.069441, -0.169580 ],
			"coeffs_40" : [ 0.110035, -0.157069, 0.249693, 0.256120 ],
			"coeffs_41" : [ -0.077283, 0.097032, -0.108430, 0.200057 ],
			"coeffs_42" : [ -0.221294, 0.166015, 0.018980, -0.115629 ],
			"coeffs_43" : [ 0.128277, 0.080914, 0.034774, -0.115864 ],
			"coeffs_44" : [ 0.177372, 0.231212, -0.127114, 0.057970 ],
			"coeffs_45" : [ 0.123948, 0.043120, -0.031011, 0.247174 ],
			"coeffs_46" : [ 0.056158, 0.121611, -0.142730, -0.064478 ],
			"coeffs_47" : [ 0.163646, 0.160522, -0.038776, -0.029537 ],
			"coeffs_48" : [ 0.146049, -0.149877, 0.051212, 0.169000 ],
			"coeffs_49" : [ -0.195782, -0.131677, -0.165480, -0.097907 ],
			"coeffs_50" : [ -0.088328, 0.025143, -0.222690, 0.172668 ],
			"coeffs_51" : [ 0.193034, 0.027301, -0.010092, 0.126134 ],
			"coeffs_52" : [ -0.004937, 0.139950, 0.011665, 0.174173 ],
			"coeffs_53" : [ 0.075899, -0.070096, -0.048574, -0.178577 ],
			"coeffs_54" : [ -0.148291, -0.167560, 0.085814, -0.006283 ],
			"coeffs_55" : [ -0.094346, -0.208343, 0.234332, -0.122382 ],
			"coeffs_56" : [ -0.043269, 0.115552, 0.082304, 0.125728 ],
			"coeffs_57" : [ -0.183482, 0.202782, 0.191279, 0.148644 ],
			"coeffs_58" : [ 0.016949, 0.268651, 0.181940, 0.032211 ],
			"coeffs_59" : [ -0.115364, 0.237111, -0.235184, -0.192146 ],
			"coeffs_60" : [ 0.007286, -0.002316, 0.065450, 0.164108 ],
			"coeffs_61" : [ 0.015301, -0.064198, -0.127554, -0.109597 ],
			"coeffs_62" : [ 0.034389, 0.062376, -0.183859, 0.004338 ],
			"coeffs_63" : [ 0.140557, -0.100127, -0.039528, -0.063891 ],
			"coeffs_64" : [ 0.117401, -0.094809, 0.018923, 0.130428 ],
			"coeffs_65" : [ 0.099262, 0.174515, 0.146989, 0.199337 ],
			"coeffs_66" : [ 0.048087, -0.150342, 0.006341, -0.193570 ],
			"coeffs_67" : [ 0.060317, 0.027220, 0.206012, 0.155103 ],
			"coeffs_68" : [ 0.193771, -0.097548, 0.073703, 0.172030 ],
			"coeffs_69" : [ 0.062687, 0.228924, -0.151159, 0.128541 ],
			"coeffs_70" : [ 0.083231, 0.262723, 0.018661, 0.110736 ],
			"coeffs_71" : [ 0.108115, 0.152258, 0.177087, -0.212342 ],
			"coeffs_72" : [ 0.115706, 0.200476, 0.073443, 0.161360 ],
			"coeffs_73" : [ -0.091020, 0.054858, -0.201604, 0.086841 ],
			"coeffs_74" : [ -0.207738, 0.134024, 0.020265, 0.004864 ],
			"coeffs_75" : [ 0.086297, -0.227725, -0.194198, -0.047163 ],
			"coeffs_76" : [ -0.178383, 0.124724, -0.012727, -0.062419 ],
			"coeffs_77" : [ -0.026138, 0.123986, 0.098124, -0.072369 ],
			"coeffs_78" : [ 0.153923, -0.172977, 0.165609, -0.079077 ],
			"coeffs_79" : [ 0.027984, -0.117596, -0.113836, 0.068228 ],
			"coeffs_80" : [ -0.104874, 0.213308, -0.244136, -0.136827 ],
			"coeffs_81" : [ 0.112390, 0.019945, 0.143514, 0.178247 ],
			"coeffs_82" : [ -0.114648, -0.194297, -0.232456, 0.033222 ],
			"coeffs_83" : [ 0.060448, 0.132977, -0.036531, -0.007948 ],
			"coeffs_84" : [ 0.230514, -0.147718, -0.077879, -0.041946 ],
			"coeffs_85" : [ 0.027703, -0.108722, -0.074161, -0.107346 ],
			"coeffs_86" : [ -0.016501, 0.143043, 0.002424, -0.079996 ],
			"coeffs_87" : [ -0.040798, -0.017428, 0.189978, 0.048763 ],
			"coeffs_88" : [ -0.188082, -0.115760, -0.228470, 0.129971 ],
			"coeffs_89" : [ -0.085723, -0.060652, -0.191042, 0.023256 ],
			"coeffs_90" : [ -0.178425, -0.054818, 0.223903, -0.170013 ],
			"coeffs_91" : [ 0.205125, -0.115292, 0.059827, -0.184789 ],
			"coeffs_92" : [ 0.094732, -0.090012, -0.148149, -0.108935 ],
			"coeffs_93" : [ -0.251496, 0.067727, -0.227940, 0.236312 ],
			"coeffs_94" : [ -0.264006, -0.012144, 0.193930, 0.032331 ],
			"coeffs_95" : [ -0.031720, -0.057206, 0.216792, -0.152717 ],
			"coeffs_96" : [ -0.182957, -0.102235, 0.038343, 0.200872 ],
			"coeffs_97" : [ -0.005871, 0.043317, 0.146071, 0.154456 ],
			"coeffs_98" : [ -0.187027, 0.127995, -0.171767, 0.157932 ],
			"coeffs_99" : [ -0.170446, -0.027141, 0.170878, -0.173331 ],
			"intercepts" : [ -0.153001, 0.039858, 0.155654, 0.199326 ]
		},
		"Layer_2" : {
			"name" : "Hidden_Layer_2",
			"NbInputs" : 4,
			"NbOutputs" : 8 ,
			"coeffs_0" : [ -0.325720, -0.013924, 0.676838, 0.611841, 0.503763, -0.147370, -0.301977, -0.276998 ],
			"coeffs_1" : [ 0.138947, -0.604671, -0.282254, -0.632901, -0.185688, -0.191374, -0.006546, -0.141184 ],
			"coeffs_2" : [ 0.722570, 0.479640, -0.408787, -0.029793, 0.398628, -0.667300, -0.371400, 0.020008 ],
			"coeffs_3" : [ -0.338682, 0.011783, -0.672961, -0.107281, -0.235181, 0.290634, -0.104482, 0.471177 ],
			"intercepts" : [ -0.026664, -0.675937, 0.529912, -0.616524, -0.022815, -0.054808, 0.295229, -0.687855 ]
		},
		"Layer_3" : {
			"name" : "Hidden_Layer_3",
			"NbInputs" : 8,
			"NbOutputs" : 6 ,
			"coeffs_0" : [ -0.264949, 0.563798, 0.367915, -0.320623, 0.453665, 0.121241 ],
			"coeffs_1" : [ 0.605018, 0.595693, -0.247366, 0.494229, 0.554471, 0.511753 ],
			"coeffs_2" : [ 0.403523, 0.079923, 0.090075, -0.068863, -0.114253, -0.002618 ],
			"coeffs_3" : [ 0.072698, -0.194039, -0.413525, 0.127796, 0.601730, -0.191184 ],
			"coeffs_4" : [ -0.332315, -0.419524, -0.295394, -0.288747, -0.161727, -0.212070 ],
			"coeffs_5" : [ 0.502782, -0.034800, 0.116373, 0.087805, 0.057084, -0.008349 ],
			"coeffs_6" : [ -0.277233, 0.552646, 0.651237, 0.267751, 0.112850, 0.143219 ],
			"coeffs_7" : [ -0.070488, 0.394018, 0.175121, -0.130391, -0.183469, -0.603254 ],
			"intercepts" : [ 0.610885, -0.564384, -0.362573, 0.629223, -0.461759, 0.556943 ]
		},
		"Layer_4" : {
			"name" : "Output_Layer",
			"NbInputs" : 6,
			"NbOutputs" : 4 ,
			"coeffs_0" : [ -0.405203, 0.705499, 0.256427, 0.675571 ],
			"coeffs_1" : [ -0.538052, 0.482592, -0.635644, 0.781816 ],
			"coeffs_2" : [ -0.499978, -0.524990, -0.444057, -0.174944 ],
			"coeffs_3" : [ 0.312359, 0.526865, -0.213524, 0.688975 ],
			"coeffs_4" : [ -0.321749, -0.650208, -0.388120, -0.023971 ],
			"coeffs_5" : [ 0.197224, -0.584747, 0.379202, 0.434909 ],
			"intercepts" : [ -0.155912, -0.339696, 0.470721, -0.562473 ]
		}
	}
}
WRITING_JSON_CODE 'logs/auto_tests/classification/MLPClassifier/mllite.MLPClassifier_FourClass_100_sampled_option_1.json'

MLLITE_MODEL_SET_OPTIONS MLPClassifier {"hidden_layer_sizes" : [4, 8, 6]}
MLLITE_MODEL_OPTIONS_DEFAULT MLPClassifier { "max_iter" : 32}
MLLITE_MODEL_OPTIONS_AFTER_SETTING MLPClassifier { "hidden_layer_sizes" : [4, 8, 6],  "max_iter" : 32 }
MLLITE_MODEL_JSON_AFTER_SETTING MLPClassifier None
MLLITE_RELOADING_MODEL mllite_mlp_class.MLPClassifier_ff4_il8
{
	"metadata" : { "model" : "Rosenblatt_MLP", "version" : "2024-W12", "signature" : "ff4_il8"},
	"options" : {  "hidden_layer_sizes" : [ 4, 8, 6 ], "activation" : "relu", "solver" : "adam", "alpha" : 0.000100, "batch_size" : null, "learning_rate" : "constant", "learning_rate_init" : 0.001000, "power_t" : 0.500000, "max_iter" : 32, "shuffle" : 1, "random_state" : 1789, "tol" : 0.000100, "verbose" : 0, "warm_start" : 0, "momentum" : 0.900000, "nesterovs_momentum" : 1, "early_stopping" : 0, "validation_fraction" : 0.100000, "beta_1" : 0.900000, "beta_2" : 0.999000, "epsilon" : 0.000000, "n_iter_no_change" : 10, "max_fun" : 15000 },
	"dataset" : { "dataset_rows" : 128, "dataset_features" : 100 },
	"classes" : [ 0, 1, 2, 3 ],
	"layers" : {
		"sizes" : [ 100, 4, 8, 6, 4 ],
		"Layer_0" : {
			"name" : "Input_Layer",
			"NbInputs" : 0,
			"NbOutputs" : 100 
		},
		"Layer_1" : {
			"name" : "Hidden_Layer_1",
			"NbInputs" : 100,
			"NbOutputs" : 4 ,
			"coeffs_00" : [ -0.019355, 0.149693, -0.129067, 0.002282 ],
			"coeffs_01" : [ 0.015553, -0.140160, 0.041765, 0.035907 ],
			"coeffs_02" : [ 0.194374, 0.031814, 0.195492, -0.014125 ],
			"coeffs_03" : [ 0.205989, -0.208396, -0.129875, 0.121435 ],
			"coeffs_04" : [ 0.099971, -0.095218, -0.092035, -0.154250 ],
			"coeffs_05" : [ 0.129976, -0.249272, -0.008127, -0.023140 ],
			"coeffs_06" : [ -0.078391, -0.207602, -0.089956, 0.226930 ],
			"coeffs_07" : [ -0.046052, -0.027160, -0.192428, -0.011281 ],
			"coeffs_08" : [ -0.052671, 0.091143, -0.021304, 0.069140 ],
			"coeffs_09" : [ -0.152010, 0.241699, -0.160632, 0.155803 ],
			"coeffs_10" : [ -0.140061, 0.162057, 0.025859, 0.240762 ],
			"coeffs_11" : [ -0.120596, -0.209715, 0.089899, 0.119256 ],
			"coeffs_12" : [ 0.183151, 0.180614, 0.215463, -0.124479 ],
			"coeffs_13" : [ 0.114719, 0.174346, -0.176249, 0.136254 ],
			"coeffs_14" : [ 0.074665, 0.021985, 0.062851, 0.120377 ],
			"coeffs_15" : [ -0.177158, 0.035164, 0.123622, 0.107133 ],
			"coeffs_16" : [ -0.237292, 0.171352, -0.153503, -0.200366 ],
			"coeffs_17" : [ 0.259349, 0.255341, -0.036393, -0.222130 ],
			"coeffs_18" : [ 0.025211, 0.212050, -0.204294, 0.066572 ],
			"coeffs_19" : [ 0.133861, 0.150866, 0.230406, -0.151780 ],
			"coeffs_20" : [ 0.092262, 0.146920, 0.204684, -0.055906 ],
			"coeffs_21" : [ -0.244309, 0.151481, 0.011869, -0.030979 ],
			"coeffs_22" : [ -0.144800, -0.243859, -0.071832, 0.187740 ],
			"coeffs_23" : [ 0.001169, -0.077694, -0.054806, -0.184164 ],
			"coeffs_24" : [ 0.050311, -0.102074, -0.074096, -0.077550 ],
			"coeffs_25" : [ 0.065578, -0.151179, -0.050791, 0.112867 ],
			"coeffs_26" : [ 0.128479, 0.175643, 0.206986, 0.161659 ],
			"coeffs_27" : [ -0.074724, -0.002575, 0.094973, 0.134391 ],
			"coeffs_28" : [ 0.086519, -0.028827, 0.166855, 0.179443 ],
			"coeffs_29" : [ 0.054648, -0.106955, -0.086993, -0.117506 ],
			"coeffs_30" : [ 0.099251, 0.151246, -0.179326, 0.048002 ],
			"coeffs_31" : [ -0.182476, 0.007832, 0.074438, -0.064549 ],
			"coeffs_32" : [ 0.077945, -0.252833, -0.182165, 0.194286 ],
			"coeffs_33" : [ 0.187337, -0.187939, -0.062305, -0.209211 ],
			"coeffs_34" : [ -0.155454, -0.215141, -0.113243, -0.061541 ],
			"coeffs_35" : [ -0.188516, -0.153019, -0.194296, 0.079293 ],
			"coeffs_36" : [ -0.187416, -0.211356, 0.074663, 0.088159 ],
			"coeffs_37" : [ -0.007274, 0.060781, 0.056130, -0.186288 ],
			"coeffs_38" : [ -0.014757, 0.223600, 0.271790, -0.071574 ],
			"coeffs_39" : [ -0.204582, -0.101483, 0.069441, -0.169580 ],
			"coeffs_40" : [ 0.110035, -0.157069, 0.249693, 0.256120 ],
			"coeffs_41" : [ -0.077283, 0.097032, -0.108430, 0.200057 ],
			"coeffs_42" : [ -0.221294, 0.166015, 0.018980, -0.115629 ],
			"coeffs_43" : [ 0.128277, 0.080914, 0.034774, -0.115864 ],
			"coeffs_44" : [ 0.177372, 0.231212, -0.127114, 0.057970 ],
			"coeffs_45" : [ 0.123948, 0.043120, -0.031011, 0.247174 ],
			"coeffs_46" : [ 0.056158, 0.121611, -0.142730, -0.064478 ],
			"coeffs_47" : [ 0.163646, 0.160522, -0.038776, -0.029537 ],
			"coeffs_48" : [ 0.146049, -0.149877, 0.051212, 0.169000 ],
			"coeffs_49" : [ -0.195782, -0.131677, -0.165480, -0.097907 ],
			"coeffs_50" : [ -0.088328, 0.025143, -0.222690, 0.172668 ],
			"coeffs_51" : [ 0.193034, 0.027301, -0.010092, 0.126134 ],
			"coeffs_52" : [ -0.004937, 0.139950, 0.011665, 0.174173 ],
			"coeffs_53" : [ 0.075899, -0.070096, -0.048574, -0.178577 ],
			"coeffs_54" : [ -0.148291, -0.167560, 0.085814, -0.006283 ],
			"coeffs_55" : [ -0.094346, -0.208343, 0.234332, -0.122382 ],
			"coeffs_56" : [ -0.043269, 0.115552, 0.082304, 0.125728 ],
			"coeffs_57" : [ -0.183482, 0.202782, 0.191279, 0.148644 ],
			"coeffs_58" : [ 0.016949, 0.268651, 0.181940, 0.032211 ],
			"coeffs_59" : [ -0.115364, 0.237111, -0.235184, -0.192146 ],
			"coeffs_60" : [ 0.007286, -0.002316, 0.065450, 0.164108 ],
			"coeffs_61" : [ 0.015301, -0.064198, -0.127554, -0.109597 ],
			"coeffs_62" : [ 0.034389, 0.062376, -0.183859, 0.004338 ],
			"coeffs_63" : [ 0.140557, -0.100127, -0.039528, -0.063891 ],
			"coeffs_64" : [ 0.117401, -0.094809, 0.018923, 0.130428 ],
			"coeffs_65" : [ 0.099262, 0.174515, 0.146989, 0.199337 ],
			"coeffs_66" : [ 0.048087, -0.150342, 0.006341, -0.193570 ],
			"coeffs_67" : [ 0.060317, 0.027220, 0.206012, 0.155103 ],
			"coeffs_68" : [ 0.193771, -0.097548, 0.073703, 0.172030 ],
			"coeffs_69" : [ 0.062687, 0.228924, -0.151159, 0.128541 ],
			"coeffs_70" : [ 0.083231, 0.262723, 0.018661, 0.110736 ],
			"coeffs_71" : [ 0.108115, 0.152258, 0.177087, -0.212342 ],
			"coeffs_72" : [ 0.115706, 0.200476, 0.073443, 0.161360 ],
			"coeffs_73" : [ -0.091020, 0.054858, -0.201604, 0.086841 ],
			"coeffs_74" : [ -0.207738, 0.134024, 0.020265, 0.004864 ],
			"coeffs_75" : [ 0.086297, -0.227725, -0.194198, -0.047163 ],
			"coeffs_76" : [ -0.178383, 0.124724, -0.012727, -0.062419 ],
			"coeffs_77" : [ -0.026138, 0.123986, 0.098124, -0.072369 ],
			"coeffs_78" : [ 0.153923, -0.172977, 0.165609, -0.079077 ],
			"coeffs_79" : [ 0.027984, -0.117596, -0.113836, 0.068228 ],
			"coeffs_80" : [ -0.104874, 0.213308, -0.244136, -0.136827 ],
			"coeffs_81" : [ 0.112390, 0.019945, 0.143514, 0.178247 ],
			"coeffs_82" : [ -0.114648, -0.194297, -0.232456, 0.033222 ],
			"coeffs_83" : [ 0.060448, 0.132977, -0.036531, -0.007948 ],
			"coeffs_84" : [ 0.230514, -0.147718, -0.077879, -0.041946 ],
			"coeffs_85" : [ 0.027703, -0.108722, -0.074161, -0.107346 ],
			"coeffs_86" : [ -0.016501, 0.143043, 0.002424, -0.079996 ],
			"coeffs_87" : [ -0.040798, -0.017428, 0.189978, 0.048763 ],
			"coeffs_88" : [ -0.188082, -0.115760, -0.228470, 0.129971 ],
			"coeffs_89" : [ -0.085723, -0.060652, -0.191042, 0.023256 ],
			"coeffs_90" : [ -0.178425, -0.054818, 0.223903, -0.170013 ],
			"coeffs_91" : [ 0.205125, -0.115292, 0.059827, -0.184789 ],
			"coeffs_92" : [ 0.094732, -0.090012, -0.148149, -0.108935 ],
			"coeffs_93" : [ -0.251496, 0.067727, -0.227940, 0.236312 ],
			"coeffs_94" : [ -0.264006, -0.012144, 0.193930, 0.032331 ],
			"coeffs_95" : [ -0.031720, -0.057206, 0.216792, -0.152717 ],
			"coeffs_96" : [ -0.182957, -0.102235, 0.038343, 0.200872 ],
			"coeffs_97" : [ -0.005871, 0.043317, 0.146071, 0.154456 ],
			"coeffs_98" : [ -0.187027, 0.127995, -0.171767, 0.157932 ],
			"coeffs_99" : [ -0.170446, -0.027141, 0.170878, -0.173331 ],
			"intercepts" : [ -0.153001, 0.039858, 0.155654, 0.199326 ]
		},
		"Layer_2" : {
			"name" : "Hidden_Layer_2",
			"NbInputs" : 4,
			"NbOutputs" : 8 ,
			"coeffs_0" : [ -0.325720, -0.013924, 0.676838, 0.611841, 0.503763, -0.147370, -0.301977, -0.276998 ],
			"coeffs_1" : [ 0.138947, -0.604671, -0.282254, -0.632901, -0.185688, -0.191374, -0.006546, -0.141184 ],
			"coeffs_2" : [ 0.722570, 0.479640, -0.408787, -0.029793, 0.398628, -0.667300, -0.371400, 0.020008 ],
			"coeffs_3" : [ -0.338682, 0.011783, -0.672961, -0.107281, -0.235181, 0.290634, -0.104482, 0.471177 ],
			"intercepts" : [ -0.026664, -0.675937, 0.529912, -0.616524, -0.022815, -0.054808, 0.295229, -0.687855 ]
		},
		"Layer_3" : {
			"name" : "Hidden_Layer_3",
			"NbInputs" : 8,
			"NbOutputs" : 6 ,
			"coeffs_0" : [ -0.264949, 0.563798, 0.367915, -0.320623, 0.453665, 0.121241 ],
			"coeffs_1" : [ 0.605018, 0.595693, -0.247366, 0.494229, 0.554471, 0.511753 ],
			"coeffs_2" : [ 0.403523, 0.079923, 0.090075, -0.068863, -0.114253, -0.002618 ],
			"coeffs_3" : [ 0.072698, -0.194039, -0.413525, 0.127796, 0.601730, -0.191184 ],
			"coeffs_4" : [ -0.332315, -0.419524, -0.295394, -0.288747, -0.161727, -0.212070 ],
			"coeffs_5" : [ 0.502782, -0.034800, 0.116373, 0.087805, 0.057084, -0.008349 ],
			"coeffs_6" : [ -0.277233, 0.552646, 0.651237, 0.267751, 0.112850, 0.143219 ],
			"coeffs_7" : [ -0.070488, 0.394018, 0.175121, -0.130391, -0.183469, -0.603254 ],
			"intercepts" : [ 0.610885, -0.564384, -0.362573, 0.629223, -0.461759, 0.556943 ]
		},
		"Layer_4" : {
			"name" : "Output_Layer",
			"NbInputs" : 6,
			"NbOutputs" : 4 ,
			"coeffs_0" : [ -0.405203, 0.705499, 0.256427, 0.675571 ],
			"coeffs_1" : [ -0.538052, 0.482592, -0.635644, 0.781816 ],
			"coeffs_2" : [ -0.499978, -0.524990, -0.444057, -0.174944 ],
			"coeffs_3" : [ 0.312359, 0.526865, -0.213524, 0.688975 ],
			"coeffs_4" : [ -0.321749, -0.650208, -0.388120, -0.023971 ],
			"coeffs_5" : [ 0.197224, -0.584747, 0.379202, 0.434909 ],
			"intercepts" : [ -0.155912, -0.339696, 0.470721, -0.562473 ]
		}
	}
}
BEAUTIFIED_JSON_START
{
	"classes" : [ 0, 1, 2, 3 ],
	"dataset" : 	{
		"dataset_features" : 100,
		"dataset_rows" : 128
	},
	"layers" : 	{
		"Layer_0" : 	{
			"NbInputs" : 0,
			"NbOutputs" : 100,
			"name" : "Input_Layer"
		},
		"Layer_1" : 	{
			"NbInputs" : 100,
			"NbOutputs" : 4,
			"coeffs_00" : [ -0.019355, 0.149693, -0.129067, 0.002282 ],
			"coeffs_01" : [ 0.015553, -0.14016, 0.041765, 0.035907 ],
			"coeffs_02" : [ 0.194374, 0.031814, 0.195492, -0.014125 ],
			"coeffs_03" : [ 0.205989, -0.208396, -0.129875, 0.121435 ],
			"coeffs_04" : [ 0.099971, -0.095218, -0.092035, -0.15425 ],
			"coeffs_05" : [ 0.129976, -0.249272, -0.008127, -0.02314 ],
			"coeffs_06" : [ -0.078391, -0.207602, -0.089956, 0.22693 ],
			"coeffs_07" : [ -0.046052, -0.02716, -0.192428, -0.011281 ],
			"coeffs_08" : [ -0.052671, 0.091143, -0.021304, 0.06914 ],
			"coeffs_09" : [ -0.15201, 0.241699, -0.160632, 0.155803 ],
			"coeffs_10" : [ -0.140061, 0.162057, 0.025859, 0.240762 ],
			"coeffs_11" : [ -0.120596, -0.209715, 0.089899, 0.119256 ],
			"coeffs_12" : [ 0.183151, 0.180614, 0.215463, -0.124479 ],
			"coeffs_13" : [ 0.114719, 0.174346, -0.176249, 0.136254 ],
			"coeffs_14" : [ 0.074665, 0.021985, 0.062851, 0.120377 ],
			"coeffs_15" : [ -0.177158, 0.035164, 0.123622, 0.107133 ],
			"coeffs_16" : [ -0.237292, 0.171352, -0.153503, -0.200366 ],
			"coeffs_17" : [ 0.259349, 0.255341, -0.036393, -0.22213 ],
			"coeffs_18" : [ 0.025211, 0.21205, -0.204294, 0.066572 ],
			"coeffs_19" : [ 0.133861, 0.150866, 0.230406, -0.15178 ],
			"coeffs_20" : [ 0.092262, 0.14692, 0.204684, -0.055906 ],
			"coeffs_21" : [ -0.244309, 0.151481, 0.011869, -0.030979 ],
			"coeffs_22" : [ -0.1448, -0.243859, -0.071832, 0.18774 ],
			"coeffs_23" : [ 0.001169, -0.077694, -0.054806, -0.184164 ],
			"coeffs_24" : [ 0.050311, -0.102074, -0.074096, -0.07755 ],
			"coeffs_25" : [ 0.065578, -0.151179, -0.050791, 0.112867 ],
			"coeffs_26" : [ 0.128479, 0.175643, 0.206986, 0.161659 ],
			"coeffs_27" : [ -0.074724, -0.002575, 0.094973, 0.134391 ],
			"coeffs_28" : [ 0.086519, -0.028827, 0.166855, 0.179443 ],
			"coeffs_29" : [ 0.054648, -0.106955, -0.086993, -0.117506 ],
			"coeffs_30" : [ 0.099251, 0.151246, -0.179326, 0.048002 ],
			"coeffs_31" : [ -0.182476, 0.007832, 0.074438, -0.064549 ],
			"coeffs_32" : [ 0.077945, -0.252833, -0.182165, 0.194286 ],
			"coeffs_33" : [ 0.187337, -0.187939, -0.062305, -0.209211 ],
			"coeffs_34" : [ -0.155454, -0.215141, -0.113243, -0.061541 ],
			"coeffs_35" : [ -0.188516, -0.153019, -0.194296, 0.079293 ],
			"coeffs_36" : [ -0.187416, -0.211356, 0.074663, 0.088159 ],
			"coeffs_37" : [ -0.007274, 0.060781, 0.05613, -0.186288 ],
			"coeffs_38" : [ -0.014757, 0.2236, 0.27179, -0.071574 ],
			"coeffs_39" : [ -0.204582, -0.101483, 0.069441, -0.16958 ],
			"coeffs_40" : [ 0.110035, -0.157069, 0.249693, 0.25612 ],
			"coeffs_41" : [ -0.077283, 0.097032, -0.10843, 0.200057 ],
			"coeffs_42" : [ -0.221294, 0.166015, 0.01898, -0.115629 ],
			"coeffs_43" : [ 0.128277, 0.080914, 0.034774, -0.115864 ],
			"coeffs_44" : [ 0.177372, 0.231212, -0.127114, 0.05797 ],
			"coeffs_45" : [ 0.123948, 0.04312, -0.031011, 0.247174 ],
			"coeffs_46" : [ 0.056158, 0.121611, -0.14273, -0.064478 ],
			"coeffs_47" : [ 0.163646, 0.160522, -0.038776, -0.029537 ],
			"coeffs_48" : [ 0.146049, -0.149877, 0.051212, 0.169 ],
			"coeffs_49" : [ -0.195782, -0.131677, -0.16548, -0.097907 ],
			"coeffs_50" : [ -0.088328, 0.025143, -0.22269, 0.172668 ],
			"coeffs_51" : [ 0.193034, 0.027301, -0.010092, 0.126134 ],
			"coeffs_52" : [ -0.004937, 0.13995, 0.011665, 0.174173 ],
			"coeffs_53" : [ 0.075899, -0.070096, -0.048574, -0.178577 ],
			"coeffs_54" : [ -0.148291, -0.16756, 0.085814, -0.006283 ],
			"coeffs_55" : [ -0.094346, -0.208343, 0.234332, -0.122382 ],
			"coeffs_56" : [ -0.043269, 0.115552, 0.082304, 0.125728 ],
			"coeffs_57" : [ -0.183482, 0.202782, 0.191279, 0.148644 ],
			"coeffs_58" : [ 0.016949, 0.268651, 0.18194, 0.032211 ],
			"coeffs_59" : [ -0.115364, 0.237111, -0.235184, -0.192146 ],
			"coeffs_60" : [ 0.007286, -0.002316, 0.06545, 0.164108 ],
			"coeffs_61" : [ 0.015301, -0.064198, -0.127554, -0.109597 ],
			"coeffs_62" : [ 0.034389, 0.062376, -0.183859, 0.004338 ],
			"coeffs_63" : [ 0.140557, -0.100127, -0.039528, -0.063891 ],
			"coeffs_64" : [ 0.117401, -0.094809, 0.018923, 0.130428 ],
			"coeffs_65" : [ 0.099262, 0.174515, 0.146989, 0.199337 ],
			"coeffs_66" : [ 0.048087, -0.150342, 0.006341, -0.19357 ],
			"coeffs_67" : [ 0.060317, 0.02722, 0.206012, 0.155103 ],
			"coeffs_68" : [ 0.193771, -0.097548, 0.073703, 0.17203 ],
			"coeffs_69" : [ 0.062687, 0.228924, -0.151159, 0.128541 ],
			"coeffs_70" : [ 0.083231, 0.262723, 0.018661, 0.110736 ],
			"coeffs_71" : [ 0.108115, 0.152258, 0.177087, -0.212342 ],
			"coeffs_72" : [ 0.115706, 0.200476, 0.073443, 0.16136 ],
			"coeffs_73" : [ -0.09102, 0.054858, -0.201604, 0.086841 ],
			"coeffs_74" : [ -0.207738, 0.134024, 0.020265, 0.004864 ],
			"coeffs_75" : [ 0.086297, -0.227725, -0.194198, -0.047163 ],
			"coeffs_76" : [ -0.178383, 0.124724, -0.012727, -0.062419 ],
			"coeffs_77" : [ -0.026138, 0.123986, 0.098124, -0.072369 ],
			"coeffs_78" : [ 0.153923, -0.172977, 0.165609, -0.079077 ],
			"coeffs_79" : [ 0.027984, -0.117596, -0.113836, 0.068228 ],
			"coeffs_80" : [ -0.104874, 0.213308, -0.244136, -0.136827 ],
			"coeffs_81" : [ 0.11239, 0.019945, 0.143514, 0.178247 ],
			"coeffs_82" : [ -0.114648, -0.194297, -0.232456, 0.033222 ],
			"coeffs_83" : [ 0.060448, 0.132977, -0.036531, -0.007948 ],
			"coeffs_84" : [ 0.230514, -0.147718, -0.077879, -0.041946 ],
			"coeffs_85" : [ 0.027703, -0.108722, -0.074161, -0.107346 ],
			"coeffs_86" : [ -0.016501, 0.143043, 0.002424, -0.079996 ],
			"coeffs_87" : [ -0.040798, -0.017428, 0.189978, 0.048763 ],
			"coeffs_88" : [ -0.188082, -0.11576, -0.22847, 0.129971 ],
			"coeffs_89" : [ -0.085723, -0.060652, -0.191042, 0.023256 ],
			"coeffs_90" : [ -0.178425, -0.054818, 0.223903, -0.170013 ],
			"coeffs_91" : [ 0.205125, -0.115292, 0.059827, -0.184789 ],
			"coeffs_92" : [ 0.094732, -0.090012, -0.148149, -0.108935 ],
			"coeffs_93" : [ -0.251496, 0.067727, -0.22794, 0.236312 ],
			"coeffs_94" : [ -0.264006, -0.012144, 0.19393, 0.032331 ],
			"coeffs_95" : [ -0.03172, -0.057206, 0.216792, -0.152717 ],
			"coeffs_96" : [ -0.182957, -0.102235, 0.038343, 0.200872 ],
			"coeffs_97" : [ -0.005871, 0.043317, 0.146071, 0.154456 ],
			"coeffs_98" : [ -0.187027, 0.127995, -0.171767, 0.157932 ],
			"coeffs_99" : [ -0.170446, -0.027141, 0.170878, -0.173331 ],
			"intercepts" : [ -0.153001, 0.039858, 0.155654, 0.199326 ],
			"name" : "Hidden_Layer_1"
		},
		"Layer_2" : 	{
			"NbInputs" : 4,
			"NbOutputs" : 8,
			"coeffs_0" : [ -0.32572, -0.013924, 0.676838, 0.611841, 0.503763, -0.14737, -0.301977, -0.276998 ],
			"coeffs_1" : [ 0.138947, -0.604671, -0.282254, -0.632901, -0.185688, -0.191374, -0.006546, -0.141184 ],
			"coeffs_2" : [ 0.72257, 0.47964, -0.408787, -0.029793, 0.398628, -0.6673, -0.3714, 0.020008 ],
			"coeffs_3" : [ -0.338682, 0.011783, -0.672961, -0.107281, -0.235181, 0.290634, -0.104482, 0.471177 ],
			"intercepts" : [ -0.026664, -0.675937, 0.529912, -0.616524, -0.022815, -0.054808, 0.295229, -0.687855 ],
			"name" : "Hidden_Layer_2"
		},
		"Layer_3" : 	{
			"NbInputs" : 8,
			"NbOutputs" : 6,
			"coeffs_0" : [ -0.264949, 0.563798, 0.367915, -0.320623, 0.453665, 0.121241 ],
			"coeffs_1" : [ 0.605018, 0.595693, -0.247366, 0.494229, 0.554471, 0.511753 ],
			"coeffs_2" : [ 0.403523, 0.079923, 0.090075, -0.068863, -0.114253, -0.002618 ],
			"coeffs_3" : [ 0.072698, -0.194039, -0.413525, 0.127796, 0.60173, -0.191184 ],
			"coeffs_4" : [ -0.332315, -0.419524, -0.295394, -0.288747, -0.161727, -0.21207 ],
			"coeffs_5" : [ 0.502782, -0.0348, 0.116373, 0.087805, 0.057084, -0.008349 ],
			"coeffs_6" : [ -0.277233, 0.552646, 0.651237, 0.267751, 0.11285, 0.143219 ],
			"coeffs_7" : [ -0.070488, 0.394018, 0.175121, -0.130391, -0.183469, -0.603254 ],
			"intercepts" : [ 0.610885, -0.564384, -0.362573, 0.629223, -0.461759, 0.556943 ],
			"name" : "Hidden_Layer_3"
		},
		"Layer_4" : 	{
			"NbInputs" : 6,
			"NbOutputs" : 4,
			"coeffs_0" : [ -0.405203, 0.705499, 0.256427, 0.675571 ],
			"coeffs_1" : [ -0.538052, 0.482592, -0.635644, 0.781816 ],
			"coeffs_2" : [ -0.499978, -0.52499, -0.444057, -0.174944 ],
			"coeffs_3" : [ 0.312359, 0.526865, -0.213524, 0.688975 ],
			"coeffs_4" : [ -0.321749, -0.650208, -0.38812, -0.023971 ],
			"coeffs_5" : [ 0.197224, -0.584747, 0.379202, 0.434909 ],
			"intercepts" : [ -0.155912, -0.339696, 0.470721, -0.562473 ],
			"name" : "Output_Layer"
		},
		"sizes" : [ 100, 4, 8, 6, 4 ]
	},
	"metadata" :  { "model" : "Rosenblatt_MLP", "signature" : "ff4_il8", "version" : "2024-W12" },
	"options" :  { "activation" : "relu", "alpha" : 0.0001, "batch_size" : null, "beta_1" : 0.9, "beta_2" : 0.999, "early_stopping" : 0, "epsilon" : 0.0, "hidden_layer_sizes" : [ 4, 8, 6 ], "learning_rate" : "constant", "learning_rate_init" : 0.001, "max_fun" : 15000, "max_iter" : 32, "momentum" : 0.9, "n_iter_no_change" : 10, "nesterovs_momentum" : 1, "power_t" : 0.5, "random_state" : 1789, "shuffle" : 1, "solver" : "adam", "tol" : 0.0001, "validation_fraction" : 0.1, "verbose" : 0, "warm_start" : 0 }
}
BEAUTIFIED_JSON_END
('OPERATION_START', 'PREDICT')
('OPERATION_END_ELAPSED', 0.0, 'PREDICT')
('OPERATION_START', 'PREDICT')
('OPERATION_END_ELAPSED', 0.0, 'PREDICT')
[[0.145  0.2042 0.3509 0.2999]
 [0.2291 0.1265 0.4716 0.1728]
 [0.1822 0.1647 0.3767 0.2763]
 [0.1386 0.2184 0.3412 0.3018]
 [0.1965 0.1506 0.4272 0.2257]
 [0.1896 0.1724 0.4308 0.2071]
 [0.1433 0.197  0.3434 0.3163]
 [0.1355 0.213  0.3794 0.2721]
 [0.1822 0.1608 0.4048 0.2523]
 [0.178  0.1712 0.3665 0.2843]
 [0.1433 0.197  0.3434 0.3163]
 [0.2274 0.1159 0.4697 0.1871]
 [0.1588 0.2754 0.3294 0.2364]
 [0.1728 0.1286 0.3791 0.3195]
 [0.1678 0.1832 0.3557 0.2933]
 [0.2056 0.132  0.4751 0.1873]
 [0.1366 0.2096 0.37   0.2837]
 [0.1799 0.1683 0.371  0.2808]
 [0.1311 0.2408 0.3847 0.2434]
 [0.1363 0.2034 0.3504 0.31  ]
 [0.144  0.1972 0.3474 0.3114]
 [0.1221 0.2694 0.3148 0.2937]
 [0.152  0.2089 0.3832 0.2559]
 [0.1536 0.1947 0.341  0.3106]
 [0.1622 0.1898 0.3506 0.2974]
 [0.1433 0.197  0.3434 0.3163]
 [0.118  0.2279 0.3692 0.2849]
 [0.1392 0.2107 0.3339 0.3162]
 [0.1184 0.2268 0.3693 0.2855]
 [0.177  0.1728 0.3639 0.2863]
 [0.1636 0.1868 0.4262 0.2234]
 [0.1433 0.197  0.3434 0.3163]
 [0.1989 0.1405 0.4313 0.2293]
 [0.1359 0.2038 0.3509 0.3094]
 [0.1061 0.2612 0.3632 0.2694]
 [0.1525 0.194  0.3429 0.3106]
 [0.1456 0.196  0.3457 0.3127]
 [0.1536 0.1938 0.3425 0.3101]
 [0.1146 0.3112 0.3096 0.2646]
 [0.1616 0.1901 0.3487 0.2997]
 [0.1395 0.2281 0.3346 0.2978]
 [0.1433 0.197  0.3434 0.3163]
 [0.1474 0.2004 0.3598 0.2924]
 [0.159  0.1839 0.3727 0.2845]
 [0.1433 0.197  0.3434 0.3163]
 [0.1433 0.197  0.3434 0.3163]
 [0.1439 0.1967 0.3435 0.3158]
 [0.1051 0.2644 0.3626 0.2679]
 [0.1376 0.2059 0.3554 0.301 ]
 [0.1698 0.1782 0.359  0.293 ]
 [0.099  0.278  0.3683 0.2548]
 [0.1331 0.1081 0.3038 0.4551]
 [0.1303 0.2097 0.3583 0.3018]
 [0.1372 0.2024 0.3491 0.3112]
 [0.1952 0.1736 0.4508 0.1804]
 [0.1667 0.1865 0.3473 0.2995]
 [0.229  0.1155 0.4739 0.1816]
 [0.1889 0.1375 0.4209 0.2527]
 [0.2236 0.1168 0.4748 0.1848]
 [0.1712 0.1263 0.391  0.3115]
 [0.1238 0.2176 0.3696 0.289 ]
 [0.1693 0.2535 0.3505 0.2267]
 [0.19   0.1956 0.3819 0.2324]
 [0.1499 0.1924 0.3475 0.3103]
 [0.134  0.2214 0.3622 0.2825]
 [0.1433 0.197  0.3434 0.3163]
 [0.1778 0.1192 0.354  0.349 ]
 [0.1463 0.2337 0.3374 0.2826]
 [0.207  0.1426 0.4431 0.2073]
 [0.1381 0.2143 0.3328 0.3148]
 [0.1689 0.1799 0.3564 0.2948]
 [0.1935 0.1153 0.4234 0.2678]
 [0.1641 0.2132 0.4327 0.19  ]
 [0.1695 0.1748 0.3797 0.2761]
 [0.1724 0.1706 0.3947 0.2623]
 [0.1882 0.1446 0.426  0.2413]
 [0.1887 0.1757 0.4466 0.1889]
 [0.2207 0.1195 0.4716 0.1881]
 [0.1063 0.2608 0.3633 0.2696]
 [0.1922 0.1862 0.3911 0.2304]
 [0.1187 0.2821 0.3104 0.2888]
 [0.1821 0.165  0.3763 0.2766]
 [0.2096 0.1329 0.4548 0.2027]
 [0.1642 0.1881 0.3472 0.3004]
 [0.0572 0.0519 0.1209 0.77  ]
 [0.1478 0.1964 0.3443 0.3115]
 [0.1923 0.1488 0.4254 0.2335]
 [0.2097 0.1306 0.4558 0.204 ]
 [0.103  0.0756 0.2094 0.612 ]
 [0.1165 0.2318 0.3686 0.2831]
 [0.133  0.2346 0.3275 0.3048]
 [0.1836 0.1646 0.4391 0.2127]
 [0.2104 0.1067 0.4747 0.2082]
 [0.1342 0.2072 0.3578 0.3007]
 [0.2103 0.1002 0.4495 0.24  ]
 [0.1433 0.197  0.3434 0.3163]
 [0.1585 0.1861 0.3568 0.2985]
 [0.1536 0.1898 0.3491 0.3075]
 [0.1336 0.2085 0.3604 0.2975]
 [0.1464 0.2035 0.3496 0.3006]
 [0.1298 0.2112 0.3618 0.2973]
 [0.1657 0.1868 0.3467 0.3009]
 [0.0978 0.2872 0.3574 0.2575]
 [0.1527 0.1953 0.3407 0.3113]
 [0.1461 0.2004 0.3578 0.2957]
 [0.149  0.1982 0.3425 0.3103]
 [0.1551 0.1937 0.3416 0.3096]
 [0.1139 0.2674 0.3583 0.2604]
 [0.1314 0.2091 0.3585 0.301 ]
 [0.1737 0.1729 0.4227 0.2307]
 [0.1843 0.1537 0.4136 0.2485]
 [0.1588 0.1954 0.3527 0.2932]
 [0.1502 0.197  0.3397 0.3131]
 [0.1363 0.2364 0.332  0.2954]
 [0.1731 0.1904 0.3707 0.2659]
 [0.1343 0.2055 0.353  0.3073]
 [0.1666 0.1804 0.3571 0.2959]
 [0.1757 0.1647 0.395  0.2645]
 [0.2297 0.1224 0.4683 0.1795]
 [0.1219 0.3037 0.3065 0.268 ]
 [0.112  0.3072 0.2999 0.2809]
 [0.0937 0.2983 0.3755 0.2325]
 [0.1447 0.1963 0.3436 0.3154]
 [0.1562 0.22   0.3452 0.2787]
 [0.1318 0.2352 0.3262 0.3069]
 [0.1721 0.12   0.3871 0.3208]
 [0.1794 0.1682 0.4329 0.2195]
 [0.153  0.1956 0.3455 0.3058]]
(128, 4)
(128, 4) float32
MODEL_PERFS {'class_name': 'mllite.MLPClassifier', 'model_name': 'MLPClassifier', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'FourClass_100_sampled', 'size': 128, 'accuracy': 0.265625, 'auc': 0.6879696310314949}
WRITING_PERF_CODE 'logs/auto_tests/classification/MLPClassifier/mllite.MLPClassifier_FourClass_100_sampled_option_1.perf'

MODEL_PERFS_TIMINGS {'class_name': 'mllite.MLPClassifier', 'model_name': 'MLPClassifier', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'FourClass_100_sampled', 'training_time_in_sec': 0.031, 'prediction_time_in_sec': 0.0}
WRITING_SQL_CODE 'logs/auto_tests/classification/MLPClassifier/mllite.MLPClassifier_FourClass_100_sampled_option_1.sql'



SQL_OUT_PUT_FIRST_LINES_START
WITH model_input AS 
 (SELECT "ADS"."index" AS "index",
    CAST("ADS"."X_0" AS FLOAT) AS "X_0", CAST("ADS"."X_1" AS FLOAT) AS "X_1", CAST("ADS"."X_2" AS FLOAT) AS "X_2", CAST("ADS"."X_3" AS FLOAT) AS "X_3", CAST("ADS"."X_4" AS FLOAT) AS "X_4", CAST("ADS"."X_5" AS FLOAT) AS "X_5", CAST("ADS"."X_6" AS FLOAT) AS "X_6", CAST("ADS"."X_7" AS FLOAT) AS "X_7", CAST("ADS"."X_8" AS FLOAT) AS "X_8", CAST("ADS"."X_9" AS FLOAT) AS "X_9", CAST("ADS"."X_10" AS FLOAT) AS "X_10", CAST("ADS"."X_11" AS FLOAT) AS "X_11", CAST("ADS"."X_12" AS FLOAT) AS "X_12", CAST("ADS"."X_13" AS FLOAT) AS "X_13", CAST("ADS"."X_14" AS FLOAT) AS "X_14", CAST("ADS"."X_15" AS FLOAT) AS "X_15", CAST("ADS"."X_16" AS FLOAT) AS "X_16", CAST("ADS"."X_17" AS FLOAT) AS "X_17", CAST("ADS"."X_18" AS FLOAT) AS "X_18", CAST("ADS"."X_19" AS FLOAT) AS "X_19", CAST("ADS"."X_20" AS FLOAT) AS "X_20", CAST("ADS"."X_21" AS FLOAT) AS "X_21", CAST("ADS"."X_22" AS FLOAT) AS "X_22", CAST("ADS"."X_23" AS FLOAT) AS "X_23", CAST("ADS"."X_24" AS FLOAT) AS "X_24", CAST("ADS
SQL_OUT_PUT_FIRST_LINES_END
SQL_OUT_PUT_LAST_LINES_START
ore_0" AS "Score_0",
  arg_max_cte."Proba_0" AS "Proba_0",
  CASE WHEN (arg_max_cte."Proba_0" IS NULL OR arg_max_cte."Proba_0" > 0.0) THEN LN( arg_max_cte."Proba_0" ) ELSE -1.79769313486231e+308 END AS "LogProba_0",
  arg_max_cte."Score_1" AS "Score_1",
  arg_max_cte."Proba_1" AS "Proba_1",
  CASE WHEN (arg_max_cte."Proba_1" IS NULL OR arg_max_cte."Proba_1" > 0.0) THEN LN( arg_max_cte."Proba_1" ) ELSE -1.79769313486231e+308 END AS "LogProba_1",
  arg_max_cte."Score_2" AS "Score_2",
  arg_max_cte."Proba_2" AS "Proba_2",
  CASE WHEN (arg_max_cte."Proba_2" IS NULL OR arg_max_cte."Proba_2" > 0.0) THEN LN( arg_max_cte."Proba_2" ) ELSE -1.79769313486231e+308 END AS "LogProba_2",
  arg_max_cte."Score_3" AS "Score_3",
  arg_max_cte."Proba_3" AS "Proba_3",
  CASE WHEN (arg_max_cte."Proba_3" IS NULL OR arg_max_cte."Proba_3" > 0.0) THEN LN( arg_max_cte."Proba_3" ) ELSE -1.79769313486231e+308 END AS "LogProba_3",
  arg_max_cte."argmax_class_idx" AS "Decision",
  arg_max_cte."Max_Proba" AS "DecisionProba"
FROM arg_max_cte
SQL_OUT_PUT_LAST_LINES_END




COPY_TRAINING_DATA_TO_SQLITE_START
<class 'pandas.core.frame.DataFrame'>
Index: 128 entries, 0 to 127
Data columns (total 100 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   X_0     128 non-null    float32
 1   X_1     128 non-null    float32
 2   X_2     128 non-null    float32
 3   X_3     128 non-null    float32
 4   X_4     128 non-null    float32
 5   X_5     128 non-null    float32
 6   X_6     128 non-null    float32
 7   X_7     128 non-null    float32
 8   X_8     128 non-null    float32
 9   X_9     128 non-null    float32
 10  X_10    128 non-null    float32
 11  X_11    128 non-null    float32
 12  X_12    128 non-null    float32
 13  X_13    128 non-null    float32
 14  X_14    128 non-null    float32
 15  X_15    128 non-null    float32
 16  X_16    128 non-null    float32
 17  X_17    128 non-null    float32
 18  X_18    128 non-null    float32
 19  X_19    128 non-null    float32
 20  X_20    128 non-null    float32
 21  X_21    128 non-null    float32
 22  X_22    128 non-null    float32
 23  X_23    128 non-null    float32
 24  X_24    128 non-null    float32
 25  X_25    128 non-null    float32
 26  X_26    128 non-null    float32
 27  X_27    128 non-null    float32
 28  X_28    128 non-null    float32
 29  X_29    128 non-null    float32
 30  X_30    128 non-null    float32
 31  X_31    128 non-null    float32
 32  X_32    128 non-null    float32
 33  X_33    128 non-null    float32
 34  X_34    128 non-null    float32
 35  X_35    128 non-null    float32
 36  X_36    128 non-null    float32
 37  X_37    128 non-null    float32
 38  X_38    128 non-null    float32
 39  X_39    128 non-null    float32
 40  X_40    128 non-null    float32
 41  X_41    128 non-null    float32
 42  X_42    128 non-null    float32
 43  X_43    128 non-null    float32
 44  X_44    128 non-null    float32
 45  X_45    128 non-null    float32
 46  X_46    128 non-null    float32
 47  X_47    128 non-null    float32
 48  X_48    128 non-null    float32
 49  X_49    128 non-null    float32
 50  X_50    128 non-null    float32
 51  X_51    128 non-null    float32
 52  X_52    128 non-null    float32
 53  X_53    128 non-null    float32
 54  X_54    128 non-null    float32
 55  X_55    128 non-null    float32
 56  X_56    128 non-null    float32
 57  X_57    128 non-null    float32
 58  X_58    128 non-null    float32
 59  X_59    128 non-null    float32
 60  X_60    128 non-null    float32
 61  X_61    128 non-null    float32
 62  X_62    128 non-null    float32
 63  X_63    128 non-null    float32
 64  X_64    128 non-null    float32
 65  X_65    128 non-null    float32
 66  X_66    128 non-null    float32
 67  X_67    128 non-null    float32
 68  X_68    128 non-null    float32
 69  X_69    128 non-null    float32
 70  X_70    128 non-null    float32
 71  X_71    128 non-null    float32
 72  X_72    128 non-null    float32
 73  X_73    128 non-null    float32
 74  X_74    128 non-null    float32
 75  X_75    128 non-null    float32
 76  X_76    128 non-null    float32
 77  X_77    128 non-null    float32
 78  X_78    128 non-null    float32
 79  X_79    128 non-null    float32
 80  X_80    128 non-null    float32
 81  X_81    128 non-null    float32
 82  X_82    128 non-null    float32
 83  X_83    128 non-null    float32
 84  X_84    128 non-null    float32
 85  X_85    128 non-null    float32
 86  X_86    128 non-null    float32
 87  X_87    128 non-null    float32
 88  X_88    128 non-null    float32
 89  X_89    128 non-null    float32
 90  X_90    128 non-null    float32
 91  X_91    128 non-null    float32
 92  X_92    128 non-null    float32
 93  X_93    128 non-null    float32
 94  X_94    128 non-null    float32
 95  X_95    128 non-null    float32
 96  X_96    128 non-null    float32
 97  X_97    128 non-null    float32
 98  X_98    128 non-null    float32
 99  X_99    128 non-null    float32
dtypes: float32(100)
memory usage: 51.0 KB
            X_0       X_1       X_2  ...      X_97      X_98      X_99
index                                ...                              
0      0.935563  2.247500 -1.070940  ...  0.677246 -0.177791 -0.249523
1      0.293314 -1.260450 -3.448018  ...  0.327620  0.164190  2.205145
2      0.596661  1.589408 -0.810968  ... -1.275388  0.026429 -0.565740
3      1.456436 -2.080544  0.694122  ...  0.753116  1.059889  0.328791
4     -1.193096 -0.499944  0.528137  ... -0.506615  1.236806  1.097111
...         ...       ...       ...  ...       ...       ...       ...
123    0.634123 -0.617818 -2.241496  ...  0.279937 -0.966775 -0.710358
124    0.918349 -0.311832 -0.471041  ... -0.320630 -0.317160  0.826388
125   -1.275761  1.107036  0.345849  ...  0.379130  0.566895  0.570030
126    0.529198  1.241322  0.500518  ... -0.490827 -1.251546 -1.179887
127   -0.098289 -0.628792 -0.750264  ... -1.144028  0.882400 -0.271507

[128 rows x 100 columns]
COPY_TRAINING_DATA_TO_SQLITE_END


PREDICT_MODEL_ON_TRAINING_DATA_INSIDE_SQLITE_START
