          X_0       X_1       X_2  ...      X_98      X_99  target
0   -0.001923 -2.371688 -1.181275  ... -0.370192  0.432550       0
1   -0.094974  0.975498  0.650450  ... -0.220261  0.222239       1
2    0.060564 -1.800755  0.053222  ...  0.793880 -0.469869       0
3   -0.113704  0.680545 -0.819322  ...  0.428237 -0.069851       1
4   -0.494085  0.103852  0.838905  ... -0.711862 -0.910029       1
..        ...       ...       ...  ...       ...       ...     ...
123 -1.641586 -1.435212 -1.466680  ...  1.768780  0.132030       0
124 -0.333624 -1.181805 -0.885047  ... -0.070404 -0.513488       0
125  0.378192  0.162965  0.389403  ... -0.939559 -1.597954       1
126 -0.067286  0.126249  0.403635  ...  1.155926  0.785249       1
127  0.956823 -0.959303  0.272337  ... -0.599084 -0.161305       0

[128 rows x 101 columns]
SKLEARN_MODEL_SET_OPTIONS MLPClassifier {"hidden_layer_sizes" : [4, 8, 6]}
('OPERATION_START', 'TRAINING')
[[-1.92301034e-03 -2.37168813e+00 -1.18127453e+00 -8.41143429e-01
  -2.27833748e+00  5.56165516e-01 -4.33400422e-01  6.68530583e-01
  -4.45269734e-01  1.08730054e+00  9.94111359e-01  6.15174830e-01
  -1.42712057e-01 -5.43691695e-01  2.51998758e+00 -5.76757491e-01
  -4.96940762e-01 -1.37897694e+00 -1.15515006e+00 -5.45445085e-01
   9.39146876e-01  5.58746934e-01  6.42707646e-02  9.41048622e-01
   3.70287567e-01  3.05628687e-01  6.59754872e-01 -5.34938514e-01
   3.41925710e-01 -9.02376890e-01  2.64374584e-01  1.05001062e-01
   1.58149529e+00 -2.39867258e+00  3.65284145e-01 -1.43215582e-01
  -6.91276610e-01 -1.09551179e+00  6.00470424e-01 -1.27839279e+00
  -1.51853383e+00 -7.39996552e-01 -1.09941888e+00 -1.00493443e+00
   5.02316713e-01  5.94181597e-01 -4.22067046e-01  5.80759704e-01
  -9.45604265e-01 -8.63473296e-01  1.73380065e+00  4.05579746e-01
  -1.31068170e+00 -2.56809741e-01  2.29151353e-01  3.84321451e-01
  -5.51349640e-01  1.19658613e+00  3.84441763e-01 -3.77902836e-01
   6.52891636e-01 -1.64592862e+00  1.05294979e+00  8.23636234e-01
   1.37784421e+00 -7.36219212e-02  1.61197138e+00 -1.98621261e+00
   1.18670213e+00  1.34875977e+00 -3.60972047e-01 -2.41051650e+00
  -1.94886446e+00 -6.28324568e-01 -1.05058098e+00 -2.54030657e+00
   1.56220317e-01  1.23859513e+00 -8.52460384e-01 -1.03192151e+00
   7.05807090e-01  3.12049419e-01  1.97033249e-02 -1.49339557e+00
  -9.82222795e-01 -2.90390223e-01  9.48299840e-02 -1.34775198e+00
  -5.61226130e-01 -4.61151361e-01  6.03559203e-02  1.79786313e+00
   4.18422371e-01  1.31290972e+00 -5.27386606e-01  7.88664445e-02
  -4.97837096e-01 -1.82821357e+00 -3.70192051e-01  4.32550102e-01]
 [-9.49740633e-02  9.75498080e-01  6.50450408e-01 -1.63987532e-01
   1.19377956e-01 -8.87404203e-01 -9.85744298e-01 -1.57699615e-01
  -5.53503856e-02 -4.89691913e-01  3.95107746e-01 -1.86406958e+00
   2.06569105e-01  1.19034803e+00  9.08305228e-01 -6.49589598e-01
   8.17796350e-01 -5.65862536e-01 -1.62303770e+00  1.96794593e+00
  -9.01196182e-01  2.21370864e+00 -1.26184642e+00  7.29401052e-01
   1.97460622e-01  5.53488255e-01 -4.16989103e-02  2.21864507e-01
  -1.60427725e+00 -1.14986289e+00  7.22467899e-01 -1.29073668e+00
  -6.89864635e-01 -9.29718554e-01  5.53363264e-01  7.69885540e-01
   3.78352642e-01  9.23741400e-01  2.08281234e-01  3.80240083e-02
  -8.87683213e-01  3.33101004e-01  1.00212204e+00  1.69690156e+00
   2.44307727e-01 -1.86227322e+00 -4.06760365e-01 -6.38921678e-01
  -1.67613909e-01  1.53844684e-01  8.42663527e-01 -1.62592280e+00
  -1.59404182e+00  7.38830745e-01 -8.74506593e-01 -1.22548962e+00
  -5.25257468e-01 -4.70152795e-01 -1.15468964e-01  4.96193357e-02
  -8.19156051e-01  2.99073744e+00  4.35046047e-01 -7.59149939e-02
  -1.56339154e-01 -6.03336692e-01 -6.89302310e-02 -2.13743591e+00
   2.04948735e+00 -1.69878006e+00  1.88749945e+00  4.05677646e-01
  -5.13272583e-01 -5.22737443e-01  1.29273057e+00  1.05886006e+00
  -3.68327975e-01  5.86420834e-01  1.94629407e+00  3.96328539e-01
  -2.22653337e-02  1.84837833e-01  1.55221510e+00  7.67667949e-01
   4.12257463e-01 -4.74151075e-02 -8.58929634e-01 -3.77886832e-01
   8.46213162e-01  3.82268399e-01  4.52266574e-01  3.67907345e-01
  -1.74708021e+00  2.97413111e-01  1.39319599e-01  6.18183054e-02
  -1.14589304e-01 -4.91930753e-01 -2.20261484e-01  2.22238824e-01]
 [ 6.05639964e-02 -1.80075538e+00  5.32221198e-02  1.13705254e+00
  -1.11454320e+00 -6.12991691e-01  6.58748865e-01 -6.18573092e-02
  -1.36188239e-01 -6.56564057e-01 -3.13579798e-01  2.24243671e-01
   4.27837819e-01  1.69016683e+00  1.13053429e+00 -1.34645033e+00
   4.66799051e-01 -1.12207568e+00 -1.80836928e+00  2.61885583e-01
  -1.46927464e+00 -2.81107831e+00  4.71088886e-01 -9.81634736e-01
   1.54162824e-01 -9.45118725e-01 -7.67153800e-01  2.78682500e-01
  -3.97465616e-01 -3.84304225e-01 -2.76822597e-02 -9.13487732e-01
   5.98323822e-01  7.34539688e-01  1.80665767e+00 -1.05402780e+00
  -2.37955928e+00 -1.01849586e-01 -1.25161910e+00 -3.40773277e-02
  -5.89699388e-01  4.64297593e-01 -7.69342899e-01 -4.94058549e-01
   1.90302110e+00 -1.28648996e+00 -2.02911228e-01  1.30722094e+00
  -9.35619593e-01 -7.99053609e-01 -8.04178536e-01 -1.16746537e-01
   1.43000674e+00  5.87966681e-01 -7.76776969e-01  1.36388794e-01
  -1.52828979e+00  1.08090746e+00  1.06615996e+00  6.13719285e-01
  -1.56638134e+00 -2.99079442e+00 -9.23732638e-01  1.24830830e+00
  -3.28231335e-01 -7.79168904e-01 -2.46622109e+00  7.15086102e-01
  -1.71361104e-01  1.16359353e+00  4.88843828e-01  8.57221186e-01
  -1.30257130e+00  7.77815223e-01  1.57720637e+00  2.45493579e+00
   8.09955537e-01  2.80593574e-01  4.77433234e-01 -2.73493845e-02
   1.58274734e+00  1.18173122e+00  1.69938195e+00  1.51718628e+00
  -1.84085965e-01  6.12297058e-01 -1.40703619e-01  9.34502482e-01
   3.93216848e-01 -9.26083088e-01  7.44229436e-01  2.88019562e+00
   1.62651956e+00 -1.24643338e+00  1.11573339e-02  1.65077448e-01
   3.82369637e+00  1.63663363e+00  7.93880284e-01 -4.69868749e-01]
 [-1.13704078e-01  6.80544913e-01 -8.19322228e-01 -9.69877481e-01
   7.71628141e-01 -1.57383764e+00  2.48730704e-01  5.41360080e-01
  -1.59036744e+00 -3.12054634e-01 -1.64359498e+00  1.50122166e+00
  -1.00549912e+00 -1.26486790e+00 -2.33127594e+00  1.01962104e-01
   5.75607657e-01 -1.20477211e+00 -9.19741020e-02 -1.16153848e+00
   2.32242107e+00  1.63334921e-01  1.69249582e+00  2.74418384e-01
  -1.81126118e+00  8.80577981e-01  7.02414274e-01  1.03724468e+00
   1.05812097e+00  1.13643229e+00 -7.69474387e-01  5.84956825e-01
  -9.34404075e-01 -1.12673771e+00 -3.59424800e-01 -1.02582085e+00
  -9.53075886e-01 -8.06699455e-01 -2.95903623e-01  1.28596336e-01
  -2.09618449e+00  1.96716547e-01 -1.33443916e+00  6.67538345e-01
  -2.04303360e+00  6.56667471e-01 -1.87788701e+00 -7.45100915e-01
   3.34480464e-01  2.30912209e-01 -1.05614233e+00  1.71417046e+00
   1.68882191e+00  1.25794733e+00  7.77477682e-01 -1.20922111e-01
   1.01463521e+00  7.94170141e-01  8.18609297e-01  1.13752067e-01
   7.90019155e-01  1.38778999e-01  8.86305422e-02  1.08791220e+00
  -1.64308107e+00 -9.54914749e-01  2.37339306e+00  9.95790303e-01
  -1.40296519e-01 -4.31657374e-01 -3.15623283e-02 -1.30086195e+00
  -5.68930566e-01  5.63574135e-01 -8.70503426e-01  4.14068997e-01
  -8.70141163e-02 -1.51894724e+00 -1.23855531e+00  1.29880026e-01
  -5.59461713e-01 -1.26385403e+00  1.59277141e-01 -1.86286375e-01
   5.10727882e-01  1.43785611e-01 -1.63213980e+00  3.82019728e-01
  -1.09840834e+00 -1.70634732e-01 -4.41302881e-02 -3.41793835e-01
   3.30759794e-01  4.17531244e-02 -3.37710887e-01  1.44820738e+00
   1.34170282e+00 -7.92602599e-01  4.28236961e-01 -6.98510483e-02]
 [-4.94084507e-01  1.03852168e-01  8.38904679e-01 -7.35904038e-01
  -7.04967678e-02  5.73717237e-01  2.18849331e-01  1.05623102e+00
  -1.25785077e+00  1.13013601e+00 -1.55437517e+00  9.51525688e-01
  -1.67592776e+00 -3.48637164e-01  1.60759163e+00  6.90805376e-01
  -1.13699162e+00  9.32226717e-01 -8.86185884e-01  6.50337562e-02
  -6.33093655e-01 -4.60071653e-01 -3.17828417e-01 -6.01315439e-01
  -7.02504814e-02 -2.66349375e-01 -1.10489130e+00  1.07519376e+00
  -4.23042387e-01  3.45855922e-01 -5.60104251e-01  3.57963622e-01
   1.71203125e+00  1.49234402e+00 -1.28127527e+00  3.17688972e-01
   2.95880270e+00  8.53792131e-01 -2.06153846e+00 -3.91467154e-01
  -2.43756063e-02  1.67333499e-01  1.17837632e+00 -4.12473530e-01
   1.56250536e-01 -3.51341099e-01 -9.60100651e-01  1.04467452e+00
   1.08441925e+00  5.52181721e-01 -1.01620758e+00 -2.87865996e+00
   5.76726556e-01 -5.82755208e-01  6.56063616e-01 -1.08061683e+00
  -5.74857712e-01  7.20476925e-01 -1.94190967e+00 -5.61854243e-01
   6.89151466e-01  5.53826809e-01 -3.96725759e-02 -2.25085169e-01
   2.28897452e+00  6.30537391e-01  1.64857304e+00 -5.28170049e-01
   5.29594898e-01 -1.10107467e-01  1.42280829e+00  1.06736279e+00
  -2.18277469e-01 -2.87699342e-01  4.86762732e-01 -1.59002244e+00
   5.11812679e-02 -4.70185161e-01  1.38356775e-01  7.31057286e-01
  -1.03853390e-01  4.13611174e-01  4.17555958e-01  1.10078800e+00
   8.31761479e-01  1.25986600e+00  2.73322630e+00  1.30618227e+00
  -1.60953015e-01 -1.14850438e+00 -1.69689417e+00 -4.59983468e-01
   1.92816043e+00 -1.38011467e+00  1.83180821e+00  5.52028716e-01
   3.90167475e-01 -7.98744082e-01 -7.11862028e-01 -9.10029233e-01]] [0 1 0 1 1]
('OPERATION_END_ELAPSED', 0.045, 'TRAINING')
CONVERT_MODEL  <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
BEAUTIFIED_JSON_START
{
	"classes" : [ 0, 1 ],
	"layers" : 	{
		"Layer_0" : 	{
			"NbInputs" : 0,
			"NbOutputs" : 100,
			"intercepts" : [  ],
			"name" : "Input_Layer"
		},
		"Layer_1" : 	{
			"NbInputs" : 100,
			"NbOutputs" : 4,
			"coeffs_00" : [ -0.0015244567766785622, -0.06721001863479614, 0.03809607774019241, 0.04140472039580345 ],
			"coeffs_01" : [ 0.23568572103977203, 0.2260177880525589, 0.20567093789577484, -0.13189797103405 ],
			"coeffs_02" : [ 0.09508470445871353, -0.15297077596187592, 0.12980695068836212, -0.013730979524552822 ],
			"coeffs_03" : [ -0.04337301105260849, -0.1413859724998474, 0.0025061871856451035, -0.18188078701496124 ],
			"coeffs_04" : [ -0.042482562363147736, -0.022101525217294693, -0.19178546965122223, -0.20741400122642517 ],
			"coeffs_05" : [ -0.2026892602443695, 0.0668865293264389, -0.16357940435409546, 0.10719098150730133 ],
			"coeffs_06" : [ 0.19052386283874512, 0.1547652631998062, 0.16102980077266693, -0.14075499773025513 ],
			"coeffs_07" : [ 0.06344511359930038, 0.0013416958972811699, -0.2149718850851059, 0.12939715385437012 ],
			"coeffs_08" : [ -0.18012836575508118, -0.1052926704287529, 0.21410773694515228, -0.06224113702774048 ],
			"coeffs_09" : [ -0.034073106944561005, -0.15069694817066193, 0.12168200314044952, 0.17836344242095947 ],
			"coeffs_10" : [ 0.07394970953464508, 0.14851854741573334, -0.19708535075187683, 0.03302571550011635 ],
			"coeffs_11" : [ -0.1456984132528305, -0.05999017134308815, -0.02239842526614666, -0.08470067381858826 ],
			"coeffs_12" : [ 0.10360682755708694, -0.0967426523566246, 0.1311759501695633, -0.09013640135526657 ],
			"coeffs_13" : [ 0.12433648109436035, 0.20284588634967804, -0.12841129302978516, 0.09855681657791138 ],
			"coeffs_14" : [ 0.13932988047599792, 0.12774400413036346, 0.052161239087581635, -0.09728849679231644 ],
			"coeffs_15" : [ 0.15733811259269714, -0.15277497470378876, -0.18672823905944824, 0.04883855581283569 ],
			"coeffs_16" : [ 0.12704132497310638, -0.19889293611049652, 0.2509596645832062, -0.11805879324674606 ],
			"coeffs_17" : [ -0.15553371608257294, -0.09992332011461258, -0.12425468862056732, -0.24335291981697083 ],
			"coeffs_18" : [ -0.12561196088790894, 0.07708105444908142, -0.06308282911777496, -0.003399827051907778 ],
			"coeffs_19" : [ -0.016210030764341354, 0.23525170981884003, -0.20596721768379211, 0.09050198644399643 ],
			"coeffs_20" : [ 0.11335231363773346, 0.24910375475883484, -0.13453170657157898, -0.08937283605337143 ],
			"coeffs_21" : [ -0.172960564494133, 0.06933194398880005, 0.07877995073795319, -0.014308393932878971 ],
			"coeffs_22" : [ 0.17167448997497559, -0.06498432904481888, 0.1628938764333725, 0.013096313923597336 ],
			"coeffs_23" : [ 0.052088163793087006, -0.15291160345077515, 0.16814196109771729, -0.0401734858751297 ],
			"coeffs_24" : [ 0.13789069652557373, 0.03199777379631996, -0.19042107462882996, -0.1330706924200058 ],
			"coeffs_25" : [ -0.12193499505519867, -0.165207639336586, 0.19322779774665833, 0.041325073689222336 ],
			"coeffs_26" : [ -0.008953019045293331, 0.04676822945475578, 0.14018650352954865, -0.08613088726997375 ],
			"coeffs_27" : [ -0.09941892325878143, 0.05956029146909714, -0.0691734030842781, 0.17890746891498566 ],
			"coeffs_28" : [ -0.026722287759184837, 0.07933741807937622, -0.24830962717533112, 0.1812411993741989 ],
			"coeffs_29" : [ 0.04227909445762634, 0.1753336638212204, -0.10774439573287964, -0.26498302817344666 ],
			"coeffs_30" : [ 0.007278045639395714, 0.12390521913766861, 0.0335347019135952, -0.18574464321136475 ],
			"coeffs_31" : [ 0.03592373803257942, -0.22208519279956818, 0.13537737727165222, 0.024579530581831932 ],
			"coeffs_32" : [ 0.05818391963839531, -0.026907838881015778, 0.0580081045627594, 0.1970335841178894 ],
			"coeffs_33" : [ 0.04816584289073944, -0.0011433359468355775, 0.11313475668430328, 0.25185275077819824 ],
			"coeffs_34" : [ 0.19208571314811707, 0.03264646977186203, 0.11868024617433548, -0.09899108856916428 ],
			"coeffs_35" : [ 0.0837368369102478, -0.014135986566543579, 0.16012543439865112, 0.11608533561229706 ],
			"coeffs_36" : [ 0.055930666625499725, 0.07561956346035004, -0.05604910850524902, -0.20057550072669983 ],
			"coeffs_37" : [ -0.20143844187259674, -0.019245438277721405, 0.1391199380159378, -0.17921549081802368 ],
			"coeffs_38" : [ -0.1379837989807129, -0.015366840176284313, -0.0848076343536377, 0.09913919121026993 ],
			"coeffs_39" : [ 0.1572791486978531, 0.2226266711950302, 0.02817412093281746, -0.14751438796520233 ],
			"coeffs_40" : [ -0.10782891511917114, -0.2437620311975479, 0.08516804873943329, 0.13195914030075073 ],
			"coeffs_41" : [ -0.17617112398147583, -0.1823272407054901, 0.06525681912899017, -0.09826051443815231 ],
			"coeffs_42" : [ 0.19782419502735138, -0.056083764880895615, 0.031124291941523552, -0.1101197600364685 ],
			"coeffs_43" : [ -0.020839815959334373, 0.06427786499261856, -0.0756991058588028, 0.17597787082195282 ],
			"coeffs_44" : [ -0.20670437812805176, -0.21169772744178772, -0.08290288597345352, -0.21133458614349365 ],
			"coeffs_45" : [ -0.14671629667282104, 0.16119077801704407, 0.20710907876491547, 0.06721431761980057 ],
			"coeffs_46" : [ 0.09278518706560135, -0.17461039125919342, -0.23419369757175446, -0.23241053521633148 ],
			"coeffs_47" : [ -0.2068624496459961, 0.20896519720554352, -0.07839527726173401, 0.21534118056297302 ],
			"coeffs_48" : [ -0.12996786832809448, 0.011350974440574646, -0.01370894443243742, 0.18025049567222595 ],
			"coeffs_49" : [ -0.12242615967988968, -0.18122583627700806, -0.19012384116649628, 0.18268625438213348 ],
			"coeffs_50" : [ -0.15437951683998108, 0.10149113088846207, -0.14441432058811188, 0.2698170840740204 ],
			"coeffs_51" : [ 0.1527918428182602, -0.13999809324741364, 0.07829353958368301, -0.07376082241535187 ],
			"coeffs_52" : [ -0.07102206349372864, 0.015155412256717682, 0.26575779914855957, -0.16040974855422974 ],
			"coeffs_53" : [ 0.14859184622764587, -0.1240033283829689, -0.08643877506256104, -0.25033053755760193 ],
			"coeffs_54" : [ -0.054139260202646255, -0.05598163977265358, 0.010097954422235489, 0.19918185472488403 ],
			"coeffs_55" : [ -0.04081686586141586, 0.08964476734399796, -0.1031900942325592, 0.17646656930446625 ],
			"coeffs_56" : [ 0.19421693682670593, 0.17634464800357819, -0.12542839348316193, 0.22372667491436005 ],
			"coeffs_57" : [ 0.1272704154253006, 0.06467953324317932, 0.0008583181770518422, 0.04992065206170082 ],
			"coeffs_58" : [ -0.18208518624305725, 0.20175181329250336, -0.09757811576128006, -0.06668733060359955 ],
			"coeffs_59" : [ -0.07795993983745575, 0.2226797491312027, 0.0825376883149147, 0.03102729469537735 ],
			"coeffs_60" : [ -0.060886137187480927, 0.23544318974018097, 0.04615182802081108, -0.021526554599404335 ],
			"coeffs_61" : [ 0.10513996332883835, -0.02844824083149433, 0.2024158537387848, -0.15306879580020905 ],
			"coeffs_62" : [ -0.18471111357212067, -0.13741935789585114, 0.1228145956993103, -0.12609994411468506 ],
			"coeffs_63" : [ -0.16727222502231598, -0.11769746243953705, -0.17555122077465057, 0.053826261311769485 ],
			"coeffs_64" : [ -0.025240346789360046, -0.12312925606966019, -0.1322605013847351, 0.019867422059178352 ],
			"coeffs_65" : [ 0.0987841859459877, -0.08464810252189636, 0.18472488224506378, 0.21689942479133606 ],
			"coeffs_66" : [ 0.054479315876960754, -0.23405194282531738, 0.06529176980257034, -0.09489016234874725 ],
			"coeffs_67" : [ 0.12046299129724503, -0.155033677816391, 0.16512684524059296, 0.2185337394475937 ],
			"coeffs_68" : [ -0.13826116919517517, 0.17243550717830658, -0.10938725620508194, -0.19037064909934998 ],
			"coeffs_69" : [ -0.010546577163040638, -0.1674300730228424, -0.014720270410180092, -0.20569616556167603 ],
			"coeffs_70" : [ 0.03398784622550011, -0.0011703731724992394, 0.22257216274738312, 0.00037241762038320303 ],
			"coeffs_71" : [ 0.18247513473033905, 0.09915527701377869, -0.17871396243572235, 0.1219334676861763 ],
			"coeffs_72" : [ 0.08925535529851913, -0.11891341209411621, -0.09558528661727905, -0.14093920588493347 ],
			"coeffs_73" : [ 0.04522634670138359, -0.2463349848985672, 0.03436959162354469, -0.10896270722150803 ],
			"coeffs_74" : [ -0.19955986738204956, 0.16430538892745972, -0.002822969341650605, -0.10072784125804901 ],
			"coeffs_75" : [ 0.11670443415641785, -0.1863681972026825, 0.1424412578344345, -0.2325492948293686 ],
			"coeffs_76" : [ 0.0611417219042778, 0.046352822333574295, -0.03407111018896103, 0.03152260184288025 ],
			"coeffs_77" : [ -0.01846012845635414, -0.1212334856390953, -0.16083866357803345, -0.2140110284090042 ],
			"coeffs_78" : [ 0.0371818020939827, 0.033461157232522964, 0.027166051790118217, -0.21074946224689484 ],
			"coeffs_79" : [ 0.011806844733655453, -0.1666933298110962, 0.03539440780878067, -0.12373673170804977 ],
			"coeffs_80" : [ -0.10117243230342865, 0.1783449351787567, 0.07858822494745255, 0.18499982357025146 ],
			"coeffs_81" : [ 0.11353451758623123, -0.036050841212272644, -0.1828448325395584, 0.19428010284900665 ],
			"coeffs_82" : [ -0.18019035458564758, -0.11688532680273056, 0.08628614246845245, 0.2550394833087921 ],
			"coeffs_83" : [ -0.019169282168149948, 0.17015500366687775, 0.176982581615448, -0.10581265389919281 ],
			"coeffs_84" : [ 0.0010747264605015516, -0.054237000644207, -0.16467995941638947, -0.16921992599964142 ],
			"coeffs_85" : [ 0.20086266100406647, -0.27125781774520874, -0.09408736228942871, -0.010125069878995419 ],
			"coeffs_86" : [ 0.18453188240528107, -0.06595515459775925, 0.138933002948761, 0.028751647099852562 ],
			"coeffs_87" : [ -0.13571497797966003, -0.10421285778284073, -0.15858107805252075, -0.20354801416397095 ],
			"coeffs_88" : [ -0.04032793268561363, -0.027742842212319374, 0.06722673773765564, 0.09350373595952988 ],
			"coeffs_89" : [ 0.009168799966573715, -0.20814967155456543, -0.10572760552167892, -0.14845667779445648 ],
			"coeffs_90" : [ -0.009050225839018822, -0.11392301321029663, 0.015949957072734833, -0.18523293733596802 ],
			"coeffs_91" : [ 0.19795839488506317, 0.009894166141748428, -0.2478179931640625, -0.23982571065425873 ],
			"coeffs_92" : [ -0.022755438461899757, -0.05498187988996506, -0.14486388862133026, -0.03062129020690918 ],
			"coeffs_93" : [ -0.08491251617670059, -0.11312789469957352, -0.19563700258731842, 0.013970879837870598 ],
			"coeffs_94" : [ -0.1966308057308197, -0.04345322027802467, 0.2326013147830963, -0.24510830640792847 ],
			"coeffs_95" : [ -0.10274890810251236, 0.07030694931745529, 0.0503946878015995, -0.07650654017925262 ],
			"coeffs_96" : [ 0.012590005062520504, 0.1891210973262787, 0.08844408392906189, -0.11098150908946991 ],
			"coeffs_97" : [ 0.09177974611520767, 0.22135567665100098, -0.0466504842042923, -0.16444054245948792 ],
			"coeffs_98" : [ 0.04176480695605278, -0.09035132825374603, -0.1978011429309845, 0.1323348432779312 ],
			"coeffs_99" : [ -0.2315565049648285, -0.19498957693576813, 0.23641549050807953, 0.21338164806365967 ],
			"intercepts" : [ 0.1268174946308136, -0.09912163764238358, 0.25677016377449036, -0.11853372305631638 ],
			"name" : "Hidden_Layer_1"
		},
		"Layer_2" : 	{
			"NbInputs" : 4,
			"NbOutputs" : 8,
			"coeffs_0" : [ -0.18276461958885193, -0.6371152997016907, -0.3800985515117645, 0.5397908091545105, 0.4143648147583008, -0.17535385489463806, 0.428505003452301, -0.6646261811256409 ],
			"coeffs_1" : [ -0.6797983050346375, 0.05277133360505104, 0.42756345868110657, 0.09108538180589676, 0.5697981715202332, -0.1153339147567749, -0.07926177233457565, -0.4409532845020294 ],
			"coeffs_2" : [ 0.3972497284412384, 0.37057891488075256, 0.6836466193199158, -0.30870604515075684, -0.30183887481689453, -0.05500641092658043, 0.23413360118865967, -0.5165139436721802 ],
			"coeffs_3" : [ 0.014181375503540039, 0.3246411979198456, -0.5956649780273438, -0.2664887607097626, -0.32592377066612244, 0.6775044202804565, -0.010498958639800549, -0.4452917277812958 ],
			"intercepts" : [ 0.48485997319221497, -0.4991047978401184, 0.1257968544960022, 0.6766412258148193, -0.5812490582466125, -0.0025847433134913445, 0.37375780940055847, 0.5658164620399475 ],
			"name" : "Hidden_Layer_2"
		},
		"Layer_3" : 	{
			"NbInputs" : 8,
			"NbOutputs" : 6,
			"coeffs_0" : [ -0.05976688116788864, -0.3056862950325012, -0.4381648004055023, -0.26179182529449463, 0.3581104278564453, 0.5906762480735779 ],
			"coeffs_1" : [ 0.3301319479942322, -0.395953893661499, -0.4989894926548004, 0.4596293568611145, -0.27836015820503235, 0.21175244450569153 ],
			"coeffs_2" : [ 0.16819226741790771, 0.003058793256059289, -0.2990300953388214, 0.34619972109794617, -0.5877001285552979, 0.2666107714176178 ],
			"coeffs_3" : [ 0.5453286170959473, 0.31020182371139526, 0.21507030725479126, 0.20208851993083954, 0.25329655408859253, -0.205678790807724 ],
			"coeffs_4" : [ 0.047022853046655655, 0.2457013577222824, 0.38778626918792725, -0.31817227602005005, 0.14406290650367737, -0.5939832329750061 ],
			"coeffs_5" : [ -0.35473331809043884, 0.06386185437440872, 0.540898323059082, -0.04262008145451546, -0.20939818024635315, -0.4621817171573639 ],
			"coeffs_6" : [ -0.5674819946289062, 0.2656237483024597, -0.6817947626113892, 0.41525375843048096, 0.37231823801994324, 0.3609375059604645 ],
			"coeffs_7" : [ -0.0536191463470459, -0.34237855672836304, 0.6076443791389465, 0.5702252984046936, 0.029616141691803932, 0.03403531759977341 ],
			"intercepts" : [ 0.6126305460929871, -0.2912937104701996, -0.4975835084915161, -0.24901072680950165, 0.45219966769218445, -0.5681837797164917 ],
			"name" : "Hidden_Layer_3"
		},
		"Layer_4" : 	{
			"NbInputs" : 6,
			"NbOutputs" : 1,
			"coeffs_0" : [ 0.7082767486572266 ],
			"coeffs_1" : [ -0.2526382803916931 ],
			"coeffs_2" : [ 0.6137092709541321 ],
			"coeffs_3" : [ 0.7441537976264954 ],
			"coeffs_4" : [ 0.6464970111846924 ],
			"coeffs_5" : [ -0.5375173687934875 ],
			"intercepts" : [ -0.3253110945224762 ],
			"name" : "Output_Layer"
		},
		"sizes" : [ 100, 4, 8, 6, 1 ]
	},
	"metadata" :  { "model" : "sklearn.neural_network._multilayer_perceptron.MLPClassifier", "version" : "1.4.1.post1" },
	"options" :  { "activation" : "relu", "alpha" : 0.0001, "batch_size" : "auto", "beta_1" : 0.9, "beta_2" : 0.999, "early_stopping" : false, "epsilon" : 1e-08, "hidden_layer_sizes" : [ 4, 8, 6 ], "learning_rate" : "constant", "learning_rate_init" : 0.001, "max_fun" : 15000, "max_iter" : 32, "momentum" : 0.9, "n_iter_no_change" : 10, "nesterovs_momentum" : true, "power_t" : 0.5, "random_state" : 1789, "shuffle" : false, "solver" : "adam", "tol" : 0.0001, "validation_fraction" : 0.1, "verbose" : false, "warm_start" : false }
}
BEAUTIFIED_JSON_END
('OPERATION_START', 'PREDICT')
('OPERATION_END_ELAPSED', 0.001, 'PREDICT')
[[0.5437 0.4563]
 [0.3569 0.6431]
 [0.2514 0.7486]
 [0.4549 0.5451]
 [0.4018 0.5982]
 [0.2757 0.7243]
 [0.2646 0.7354]
 [0.4817 0.5183]
 [0.4592 0.5408]
 [0.2932 0.7068]
 [0.4422 0.5578]
 [0.2359 0.7641]
 [0.4502 0.5498]
 [0.4486 0.5514]
 [0.2844 0.7156]
 [0.417  0.583 ]
 [0.3278 0.6722]
 [0.4125 0.5875]
 [0.3014 0.6986]
 [0.2658 0.7342]
 [0.4402 0.5598]
 [0.3367 0.6633]
 [0.5553 0.4447]
 [0.506  0.494 ]
 [0.5343 0.4657]
 [0.2649 0.7351]
 [0.3829 0.6171]
 [0.4454 0.5546]
 [0.4172 0.5828]
 [0.4607 0.5393]
 [0.443  0.557 ]
 [0.3067 0.6933]
 [0.4423 0.5577]
 [0.2929 0.7071]
 [0.4434 0.5566]
 [0.5383 0.4617]
 [0.2068 0.7932]
 [0.4524 0.5476]
 [0.3573 0.6427]
 [0.2828 0.7172]
 [0.2968 0.7032]
 [0.401  0.599 ]
 [0.4884 0.5116]
 [0.5649 0.4351]
 [0.2711 0.7289]
 [0.4876 0.5124]
 [0.4465 0.5535]
 [0.2551 0.7449]
 [0.3295 0.6705]
 [0.2378 0.7622]
 [0.3068 0.6932]
 [0.4949 0.5051]
 [0.3169 0.6831]
 [0.3915 0.6085]
 [0.3176 0.6824]
 [0.2908 0.7092]
 [0.4538 0.5462]
 [0.3321 0.6679]
 [0.4723 0.5277]
 [0.2052 0.7948]
 [0.4767 0.5233]
 [0.4485 0.5515]
 [0.2121 0.7879]
 [0.3206 0.6794]
 [0.2896 0.7104]
 [0.2521 0.7479]
 [0.3063 0.6937]
 [0.4642 0.5358]
 [0.5532 0.4468]
 [0.336  0.664 ]
 [0.4422 0.5578]
 [0.276  0.724 ]
 [0.2983 0.7017]
 [0.3361 0.6639]
 [0.2747 0.7253]
 [0.3601 0.6399]
 [0.2941 0.7059]
 [0.3454 0.6546]
 [0.448  0.552 ]
 [0.2747 0.7253]
 [0.4807 0.5193]
 [0.3861 0.6139]
 [0.3134 0.6866]
 [0.3343 0.6657]
 [0.2642 0.7358]
 [0.5414 0.4586]
 [0.2979 0.7021]
 [0.3482 0.6518]
 [0.307  0.693 ]
 [0.2699 0.7301]
 [0.2747 0.7253]
 [0.2626 0.7374]
 [0.3213 0.6787]
 [0.289  0.711 ]
 [0.2747 0.7253]
 [0.1647 0.8353]
 [0.5637 0.4363]
 [0.2733 0.7267]
 [0.4207 0.5793]
 [0.4107 0.5893]
 [0.3155 0.6845]
 [0.244  0.756 ]
 [0.2809 0.7191]
 [0.4013 0.5987]
 [0.1819 0.8181]
 [0.2931 0.7069]
 [0.4575 0.5425]
 [0.3171 0.6829]
 [0.3735 0.6265]
 [0.4385 0.5615]
 [0.3969 0.6031]
 [0.2468 0.7532]
 [0.2598 0.7402]
 [0.3181 0.6819]
 [0.3246 0.6754]
 [0.4867 0.5133]
 [0.3199 0.6801]
 [0.2723 0.7277]
 [0.232  0.768 ]
 [0.2884 0.7116]
 [0.5838 0.4162]
 [0.1725 0.8275]
 [0.2747 0.7253]
 [0.4822 0.5178]
 [0.2747 0.7253]
 [0.2875 0.7125]
 [0.4819 0.5181]
 [0.5235 0.4765]]
(128, 2)
(128,) float32
MODEL_PERFS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPClassifier', 'model_name': 'MLPClassifier', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'BinaryClass_100_sampled', 'size': 128, 'accuracy': 0.5390625, 'auc': 0.7145833333333333}
WRITING_PERF_CODE 'logs/auto_tests/classification/MLPClassifier/sklearn.neural_network._multilayer_perceptron.MLPClassifier_BinaryClass_100_sampled_option_1.perf'

MODEL_PERFS_TIMINGS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPClassifier', 'model_name': 'MLPClassifier', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'BinaryClass_100_sampled', 'training_time_in_sec': 0.045, 'prediction_time_in_sec': 0.001}
