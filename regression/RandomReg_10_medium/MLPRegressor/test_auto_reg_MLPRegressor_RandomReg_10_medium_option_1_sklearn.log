          X_0       X_1       X_2  ...       X_8       X_9      target
0   -0.406366  0.195863  2.332328  ...  0.796803 -0.085546  205.867746
1    1.116729 -0.779234  0.112861  ...  1.410749 -0.260525  159.579234
2    0.056620  1.388059  0.106218  ...  0.136112 -1.196185   52.888003
3   -1.297941  0.977779  1.024266  ...  2.801809  0.948310   -2.429778
4   -0.442130  1.306160 -1.825507  ... -0.018147  0.535711 -259.773644
..        ...       ...       ...  ...       ...       ...         ...
507  1.872097 -1.084281 -0.822479  ...  1.052999 -0.641940  166.451561
508  1.025244 -0.339974  1.301792  ... -0.413903 -0.913350   52.305087
509  0.532154 -0.480312  0.072516  ...  1.222674  0.906587  112.659076
510  1.231428 -1.048304 -0.670183  ... -0.795053  1.729585  -28.835578
511 -1.154770 -1.982372 -0.428635  ... -0.455936  0.296664 -175.589973

[512 rows x 11 columns]
SKLEARN_MODEL_SET_OPTIONS MLPRegressor {"hidden_layer_sizes" : [4, 8, 6]}
('OPERATION_START', 'TRAINING')
[[-0.40636584  0.19586295  2.3323276   0.35501477  0.30207458  0.06525005
   0.24026002  0.51502115  0.79680276 -0.08554616]
 [ 1.1167293  -0.77923435  0.11286136  1.0933083  -0.30106112  1.0466237
   0.08503766  0.44068342  1.4107493  -0.26052475]
 [ 0.05661978  1.3880594   0.10621787  2.47445    -0.01362495 -0.6425333
  -0.35128927  0.48866048  0.13611184 -1.1961848 ]
 [-1.2979405   0.97777945  1.024266   -0.56934214  0.7504386  -1.4885463
  -0.7754418   0.02992461  2.8018086   0.94831014]
 [-0.44212952  1.3061599  -1.8255073   1.100284   -1.4378009  -0.64940554
  -1.2646565  -1.2791994  -0.01814678  0.5357106 ]] [ 205.86775    159.57924     52.888004    -2.4297783 -259.77365  ]
('OPERATION_END_ELAPSED', 0.122, 'TRAINING')
CONVERT_MODEL  <class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
BEAUTIFIED_JSON_START
{
	"layers" : 	{
		"Layer_0" : 	{
			"NbInputs" : 0,
			"NbOutputs" : 10,
			"intercepts" : [  ],
			"name" : "Input_Layer"
		},
		"Layer_1" : 	{
			"NbInputs" : 10,
			"NbOutputs" : 4,
			"coeffs_0" : [ -0.11382082104682922, -0.14551348984241486, 0.2416761964559555, 0.0725693479180336 ],
			"coeffs_1" : [ 0.46987271308898926, 0.6697143316268921, 0.759326159954071, -0.38192084431648254 ],
			"coeffs_2" : [ 0.2640652358531952, -0.2178109735250473, 0.37955060601234436, -0.2180810421705246 ],
			"coeffs_3" : [ -0.21680492162704468, -0.22967052459716797, -0.009024594910442829, -0.6646118760108948 ],
			"coeffs_4" : [ -0.23437246680259705, -0.0463012233376503, -0.4031948745250702, -0.5423018336296082 ],
			"coeffs_5" : [ -0.43852582573890686, 0.2056673765182495, -0.3105825185775757, 0.10283520817756653 ],
			"coeffs_6" : [ 0.35702452063560486, 0.6216223835945129, 0.5127658247947693, -0.5097135305404663 ],
			"coeffs_7" : [ 0.14125293493270874, 0.18564918637275696, -0.45765915513038635, 0.17113476991653442 ],
			"coeffs_8" : [ -0.6274908185005188, -0.25028523802757263, 0.7556748390197754, -0.18918322026729584 ],
			"coeffs_9" : [ -0.05840088054537773, -0.4171432852745056, 0.29146885871887207, 0.507863461971283 ],
			"intercepts" : [ 0.17740143835544586, 0.6050698161125183, -0.48303842544555664, 0.11463958770036697 ],
			"name" : "Hidden_Layer_1"
		},
		"Layer_2" : 	{
			"NbInputs" : 4,
			"NbOutputs" : 8,
			"coeffs_0" : [ -0.4213460385799408, -0.1578054279088974, 0.010715058073401451, -0.05078994855284691, 0.1471826434135437, -0.34657761454582214, 0.3431798815727234, -0.1693343222141266 ],
			"coeffs_1" : [ 0.5507558584213257, 0.6197055578231812, -0.23243749141693115, 0.3213414251804352, 0.24234820902347565, 0.3446454703807831, 0.23465321958065033, -0.1341368407011032 ],
			"coeffs_2" : [ 0.4788919985294342, -0.3653040826320648, -0.375969260931015, 0.25509363412857056, 0.2220846265554428, -0.748633623123169, 0.7719483971595764, -0.19989822804927826 ],
			"coeffs_3" : [ -0.6075142621994019, -0.34045782685279846, -0.3877551853656769, -0.6512436270713806, -0.5017567873001099, 0.25748100876808167, -0.04867366701364517, 0.043982118368148804 ],
			"intercepts" : [ -0.007243866566568613, 0.7411391139030457, -0.6014647483825684, 0.4011615216732025, 0.15157516300678253, 0.7165362238883972, -0.2508145272731781, -0.34024760127067566 ],
			"name" : "Hidden_Layer_2"
		},
		"Layer_3" : 	{
			"NbInputs" : 8,
			"NbOutputs" : 6,
			"coeffs_0" : [ -0.43678018450737, 0.010306882672011852, 0.43239080905914307, -0.09797635674476624, 0.6754083037376404, -0.19943781197071075 ],
			"coeffs_1" : [ 0.42327266931533813, -0.0223348680883646, 0.2432558387517929, -0.4604409337043762, 0.5446455478668213, -0.10545842349529266 ],
			"coeffs_2" : [ 0.38495302200317383, 0.04529263824224472, -0.5249115824699402, -0.296774685382843, -0.1986963301897049, -0.45892682671546936 ],
			"coeffs_3" : [ 0.7269615530967712, -0.0871991440653801, 0.18411849439144135, -0.053354162722826004, 0.4140161871910095, -0.09612557291984558 ],
			"coeffs_4" : [ -0.23278973996639252, 0.08529040217399597, -0.17857487499713898, 0.4993423521518707, -0.05611598491668701, 0.36083343625068665 ],
			"coeffs_5" : [ -0.6138250231742859, 0.6655788421630859, -0.05166769400238991, 0.4920006990432739, -0.31205621361732483, -0.5702924728393555 ],
			"coeffs_6" : [ 0.06298269331455231, 0.14663830399513245, 0.22734764218330383, -0.5070385932922363, 0.27819785475730896, -0.5205153822898865 ],
			"coeffs_7" : [ 0.3386308252811432, -0.0040213544853031635, 0.16834694147109985, -0.0007641384727321565, 0.13926130533218384, 0.38629987835884094 ],
			"intercepts" : [ 0.3120209276676178, -0.12740620970726013, 0.3172626793384552, 0.6104769706726074, 0.6798255443572998, 0.21108345687389374 ],
			"name" : "Hidden_Layer_3"
		},
		"Layer_4" : 	{
			"NbInputs" : 6,
			"NbOutputs" : 1,
			"coeffs_0" : [ 0.4400911331176758 ],
			"coeffs_1" : [ -0.46179312467575073 ],
			"coeffs_2" : [ 0.5046334862709045 ],
			"coeffs_3" : [ -0.06045845150947571 ],
			"coeffs_4" : [ 0.6506946086883545 ],
			"coeffs_5" : [ 0.6278855204582214 ],
			"intercepts" : [ 0.41058653593063354 ],
			"name" : "Output_Layer"
		},
		"sizes" : [ 10, 4, 8, 6, 1 ]
	},
	"metadata" :  { "model" : "sklearn.neural_network._multilayer_perceptron.MLPRegressor", "version" : "1.4.1.post1" },
	"options" :  { "activation" : "relu", "alpha" : 0.0001, "batch_size" : "auto", "beta_1" : 0.9, "beta_2" : 0.999, "early_stopping" : false, "epsilon" : 1e-08, "hidden_layer_sizes" : [ 4, 8, 6 ], "learning_rate" : "constant", "learning_rate_init" : 0.001, "max_fun" : 15000, "max_iter" : 32, "momentum" : 0.9, "n_iter_no_change" : 10, "nesterovs_momentum" : true, "power_t" : 0.5, "random_state" : 1789, "shuffle" : false, "solver" : "adam", "tol" : 0.0001, "validation_fraction" : 0.1, "verbose" : false, "warm_start" : false }
}
BEAUTIFIED_JSON_END
('OPERATION_START', 'PREDICT')
[2.158954   1.3388398  2.131619   2.737789   1.3116856  1.4687812
 0.5314701  0.69303507 1.4046556  2.3646543  1.5337534  2.1564398
 1.6372509  1.1187166  2.0544114  0.42685294 3.770314   2.1526124
 0.7456342  3.5675242  0.2985465  3.739848   1.8660946  0.35107553
 2.0815794  2.4159303  1.3388398  2.742255   4.559337   2.223871
 1.7282488  1.9822004  3.3738968  0.78614223 1.534358   1.5359356
 0.80634564 0.6818959  0.62725616 0.749229   3.6795797  1.6469276
 0.42206126 3.2184737  2.204194   2.5684593  1.9753985  0.6899944
 0.5917597  0.68704116 0.2936186  0.3293892  4.5559235  1.3388398
 1.670238   0.16655222 3.5833774  1.4737179  0.62316656 1.3014903
 3.81915    0.36098093 0.3917119  3.965709   3.126899   1.5131509
 4.127061   2.3585603  1.5468259  0.98378307 1.0608087  0.5836009
 0.6405275  0.60801834 0.20736632 2.2083836  3.6924884  2.8849936
 1.4450641  0.8650185  0.7861788  1.3597414  0.7325541  0.84415215
 1.7430792  1.4057138  1.3388398  1.6726601  0.8136706  1.6149142
 0.44889495 2.4864972  0.5221389  2.387637   2.900466   3.1386197
 1.0139365  2.7908556  0.5715463  2.075759   0.5783191  2.795207
 1.793184   1.8407176  1.2463033  2.6885545  0.7292167  2.7049198
 0.859404   2.3951087  0.08090514 3.0577495  2.3715367  0.40697092
 0.26754662 1.6652095  2.659821   3.6142006  2.2476907  2.0735564
 1.3241992  2.1753063  0.5132971  3.1121054  1.9981742  3.5007255
 1.1438824  1.6313913  2.5738819  2.2917747  2.6984937  1.6445184
 2.1693008  0.60247123 1.3388398  1.4987392  0.39070916 1.6355667
 0.7620914  0.8560257  2.3058462  2.5317132  3.9563696  3.2656865
 2.124788   1.5876462  0.70047426 0.7778653  1.3876297  2.3074727
 1.6882148  0.54801506 1.113384   4.397936   2.5183737  1.7559178
 4.829702   1.8561902  0.8330042  0.9025693  2.5990384  4.5241866
 1.7820337  3.2749586  0.37505597 0.85763323 1.3688889  0.784334
 0.5518261  1.1669226  1.2429495  0.629063   4.0578647  1.6928778
 1.8687656  0.78651386 0.6223531  0.7976745  1.8677859  2.1825888
 1.3475311  1.0508912  0.38171998 2.4469843  0.26290894 0.80423284
 1.6739299  2.9848757  3.2862518  0.8474439  0.4300756  0.82428354
 1.220953   0.9018439  0.75395447 2.9562445  1.6349192  2.3482482
 1.3388398  2.2084591  2.0334504  2.14364    1.4063195  0.74507064
 2.7432187  0.72146374 2.8423617  2.7582555  2.2573442  2.1031146
 2.8353574  3.074128   1.856226   3.100217   1.6136594  1.3388398
 2.747914   4.4749646  4.116404   2.0027626  0.9647576  1.503345
 2.098981   2.0022705  1.597028   1.015799   1.3388398  0.76202345
 1.4096131  1.508383   0.20147854 1.9082513  4.6703887  1.0552188
 1.8449578  1.6961105  1.7984781  0.63978887 1.1877174  3.1701849
 0.30297917 0.6485453  1.3898717  2.220205   1.4181504  2.7210264
 2.6940818  2.3205242  2.3165376  1.6854529  1.6784644  2.0263028
 2.6818888  1.6214092  0.7961783  0.44625348 1.5770535  0.7664076
 2.276958   1.3455046  1.3388398  1.3423882  3.020264   2.154281
 1.8602016  2.648821   2.0553267  0.8603342  0.78860354 0.92795885
 0.7448628  3.4799268  1.5929832  1.7992961  1.3388398  3.6651597
 0.65771544 2.4762583  1.1615276  2.8288162  0.49272972 1.3388398
 0.9985614  0.8473779  1.3346744  1.3388398  0.85234565 0.9260538
 1.0439391  1.9215837  1.3694026  3.0271008  1.1612437  2.6079648
 1.31252    0.65117896 2.9136894  1.1910162  3.0948622  0.7425167
 3.020681   1.387544   3.4674814  4.2687817  2.43679    1.097005
 2.012332   1.032798   5.770081   1.2638123  1.3388398  1.2141128
 2.3146477  1.3829057  2.7690682  0.5170295  1.6662338  1.2568069
 0.34314018 2.2027726  1.1446967  2.439869   3.125771   0.2808382
 2.401332   0.7490092  1.4047205  1.041856   2.7075064  1.73908
 0.16237026 1.9235044  1.6647582  1.3388398  1.0992198  2.2807586
 2.2111456  0.84244215 0.12026906 1.9975364  3.251518   1.7668443
 0.5277377  0.30283874 1.3388398  1.3388398  3.357002   4.1033463
 1.9206483  2.9877741  3.2468002  2.7662623  2.5976746  0.13919681
 0.24253766 2.479346   1.5709977  1.9497209  3.6429603  1.2165387
 0.60561645 2.0155923  1.3011116  1.2458214  1.9796185  2.2088106
 2.7209992  0.7365229  3.5016286  0.65617657 4.3197803  1.9608305
 1.5886908  2.1973703  2.7231588  2.643647   1.0065693  2.8570564
 1.4057008  2.098884   3.1963155  1.7778313  0.7977015  0.45450845
 0.37942338 1.2950218  2.4653683  2.0555117  1.3388398  3.4637067
 1.5733776  1.9823     1.3582008  0.8775663  3.9126027  0.36404264
 1.9943078  1.1324062  0.30942756 1.9164741  1.5257554  1.5872431
 1.4419394  0.31256187 1.3901794  1.7280304  3.14899    0.2025215
 1.1953105  4.2180595  0.36367375 1.3388398  1.8768587  1.212769
 3.2976787  1.3504827  3.1593711  1.3573132  0.9672733  0.60137475
 2.2340643  0.5410311  2.5838523  3.840636   1.777487   2.0401616
 1.9767838  2.9292955  2.7618368  2.442082   1.5015988  2.5925486
 1.3644841  1.1401141  1.6060121  1.9182069  1.8424048  2.2388518
 1.5666597  2.0451224  1.642735   2.3647175  1.3388398  3.0163476
 2.619736   1.650976   2.1180584  1.7025337  0.82010895 2.071598
 1.2780058  1.3773239  1.9051194  2.272478   1.5405991  0.47462612
 2.4993737  1.1460297  2.1622753  4.1062784  1.6132901  1.4721539
 0.72286004 4.188864   1.7473607  0.43535107 0.09553486 3.0731294
 2.3358207  1.5603559  1.7527442  0.61826915 1.362395   3.3651516
 4.273203   1.9656589  1.0534849  1.3388398  1.8474422  0.8358177
 1.3794451  1.3503072  1.4376256  0.54102534 1.8964353  3.8163445
 1.8306327  0.5364043  1.3388398  2.324159   1.3697798  0.93360156
 3.2265537  0.63453937 2.3093321  3.349786   1.5676811  0.9766391
 2.6299527  3.4771864  2.011249   2.5998502  2.2182784  0.7242828
 2.4535568  1.5177429  0.71887624 1.3388398  1.5219378  1.7515948
 0.30247247 0.5998969 ]
('OPERATION_END_ELAPSED', 0.011, 'PREDICT')
MODEL_PERFS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'RandomReg_10_medium', 'size': 512, 'mse': 27336.832, 'mae': 132.4808, 'mape': 1.016233, 'r2': 0.002862784788213002}
WRITING_PERF_CODE 'logs/auto_tests/regression/MLPRegressor/sklearn.neural_network._multilayer_perceptron.MLPRegressor_RandomReg_10_medium_option_1.perf'

MODEL_PERFS_TIMINGS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'RandomReg_10_medium', 'training_time_in_sec': 0.122, 'prediction_time_in_sec': 0.011}
