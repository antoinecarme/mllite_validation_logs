          X_0       X_1       X_2  ...      X_98      X_99      target
0   -0.301125  1.387725  1.263659  ... -0.345798  0.839573  -54.486094
1   -0.155583 -1.181091  2.934304  ... -1.884572 -0.938959 -244.069750
2    1.079390 -0.496549 -1.240677  ... -0.569426  0.970424   50.603288
3   -1.027789 -0.022726  0.110301  ...  2.721471 -0.697773  200.907470
4    0.155319  0.126152  0.905780  ...  1.218228  0.672223 -138.968166
..        ...       ...       ...  ...       ...       ...         ...
123 -0.630035  1.213869  1.052461  ...  0.517329  1.067193  -69.245546
124  1.216598 -1.012178  1.188642  ...  0.291912  0.169619  121.314288
125  1.081204  0.991612 -2.290299  ...  1.216256  1.870381   61.276174
126 -0.144516 -2.153383  0.687067  ...  1.596720 -0.118417   60.875318
127 -0.133814 -1.941873  0.255980  ...  1.019502  0.667771   76.222011

[128 rows x 101 columns]
SKLEARN_MODEL_SET_OPTIONS MLPRegressor {"hidden_layer_sizes" : [4, 8, 6]}
('OPERATION_START', 'TRAINING')
[[-3.01124662e-01  1.38772476e+00  1.26365924e+00 -6.82732940e-01
  -1.34683120e+00  5.14777362e-01  1.62561178e-01 -1.08774674e+00
   2.71217942e-01  1.01093900e+00 -2.49382392e-01 -1.65061939e+00
  -9.93423760e-01  1.09921061e-01 -5.43385804e-01  1.32529700e+00
   1.88495958e+00  7.49242187e-01  1.60594404e+00  1.60841656e+00
   9.63199854e-01 -3.32925580e-02  1.82730234e+00 -8.89043733e-02
   1.03326552e-01  1.23202717e+00 -1.62939775e+00  8.68012488e-01
   4.37752008e-01  5.41352630e-01  4.58516777e-01 -1.81859002e-01
  -1.24955714e-01 -4.90596294e-02  1.27540767e+00  3.60171229e-01
   9.01617110e-01 -2.70372570e-01  5.28253436e-01  1.62672818e-01
  -7.59159401e-02 -1.01459682e+00 -1.15195643e-02  1.09213448e+00
   1.62921584e+00 -4.38633710e-01 -1.36951172e+00 -3.69843543e-01
  -8.60606253e-01 -5.75633466e-01  1.27469802e+00  4.84667391e-01
  -1.51542473e+00  1.16384223e-01 -8.54173601e-01 -7.89357781e-01
  -3.20154727e-01  6.83034718e-01 -4.90488261e-01 -1.72152758e+00
  -5.88555992e-01  2.26808488e-01  1.35770679e+00 -7.94542491e-01
  -2.14967608e+00 -2.36072704e-01 -8.44686925e-01 -1.26111269e+00
  -8.60300481e-01  1.04535091e+00  1.55022159e-01  3.35658103e-01
  -1.93393087e+00 -3.61082911e-01  1.09404229e-01 -1.99066043e-01
   4.23870265e-01  1.72599167e-01  8.43244314e-01  3.28669012e-01
   5.97395658e-01 -3.46147346e+00 -1.11853528e+00  1.95144802e-01
   1.27475369e+00 -6.44126832e-01  1.99323013e-01 -3.13881099e-01
   8.67388427e-01  1.60537791e+00 -4.69321199e-02  2.43432593e+00
   1.97670710e+00 -2.16416430e+00 -3.69843096e-01 -3.87633115e-01
  -1.22007036e+00 -1.98171616e+00 -3.45797569e-01  8.39573085e-01]
 [-1.55582651e-01 -1.18109095e+00  2.93430400e+00 -6.94847047e-01
   7.65346646e-01 -1.38265729e-01 -1.87197840e+00 -5.47255754e-01
  -1.02196300e+00 -2.40618020e-01  1.53626862e-03 -1.30697155e+00
   1.12571657e+00 -6.09562814e-01  1.84250534e+00  5.26184618e-01
  -1.34199113e-01 -4.64435697e-01 -2.08943471e-01 -7.25073040e-01
  -3.43274266e-01 -4.16672796e-01 -1.43184513e-01 -1.58227885e+00
   5.83837748e-01 -6.71794951e-01  1.48207486e+00  8.79976228e-02
  -1.44496679e+00 -1.04719102e+00 -7.33625665e-02  7.97347367e-01
  -1.49433851e+00 -4.77036446e-01  1.96962878e-01  9.63585079e-01
  -1.06893802e+00  6.35676011e-02 -2.22821608e-01  2.68229580e+00
  -1.22432196e+00  2.33570918e-01  3.37046921e-01 -8.80938113e-01
   3.74312043e-01  4.75478441e-01 -4.31405723e-01 -1.49277854e+00
  -7.05678761e-01  8.41879070e-01 -9.39537287e-01 -1.32261544e-01
   1.27473974e+00 -2.45601356e-01 -1.09500504e+00 -1.12451386e+00
  -1.68831348e+00 -1.66038191e+00  8.83513212e-01 -2.12799621e+00
   1.85301650e+00  1.06351781e+00 -1.55932271e+00 -3.31427574e-01
  -4.71284389e-01 -8.42893183e-01  2.66877890e-01 -2.19565415e+00
  -1.28610337e+00  2.07863837e-01  2.12718658e-02 -1.87917411e-01
   2.03328684e-01 -1.13658619e+00  7.13958859e-01  3.48399356e-02
   8.75229090e-02 -1.98537886e-01  8.98726761e-01 -1.00354958e+00
   3.02103549e-01  5.83664238e-01 -1.10631919e+00 -7.53618121e-01
   2.54740193e-02 -5.73981524e-01  6.41987205e-01  1.07664418e+00
   8.75026762e-01 -4.93790269e-01  7.25373566e-01  8.76517355e-01
   1.12196028e+00  2.01063013e+00  7.10269690e-01  3.98254842e-01
  -1.09351709e-01 -6.48999751e-01 -1.88457203e+00 -9.38958764e-01]
 [ 1.07939029e+00 -4.96548653e-01 -1.24067736e+00  2.76984543e-01
  -8.75897765e-01  3.70610416e-01  2.94699460e-01  2.08256388e+00
   1.13252175e+00  1.22751558e+00 -1.09523833e+00  5.95500886e-01
   1.12185657e+00  7.54166365e-01 -8.05529773e-01 -1.03034163e+00
   1.09279633e+00  1.50435269e+00  1.11762017e-01  1.07449919e-01
  -1.02401459e+00 -1.84641510e-01  7.72709399e-02 -4.99782264e-01
  -2.27677077e-01  6.49656951e-02  1.01747751e-01  1.98065881e-02
   5.62927604e-01  1.23536706e+00 -1.95974544e-01 -2.40673900e-01
   4.45345312e-01  5.92050970e-01  1.02269816e+00 -1.74345989e-02
  -1.73933551e-01 -6.29255697e-02 -2.20695183e-01  1.29462636e+00
   2.52112865e-01 -2.17039728e+00  9.85382497e-01 -7.44955420e-01
   1.57546902e+00  5.03206611e-01 -9.72602367e-01 -6.84496820e-01
   5.72718233e-02 -4.53426331e-01  5.94903529e-01  1.22255921e+00
   7.85897672e-01  6.81237221e-01  1.01942651e-01  4.92852837e-01
   5.66514671e-01 -1.17636359e+00  5.35346568e-01  1.32921374e+00
   1.24366212e+00 -5.73699355e-01  1.01102638e+00  8.36099803e-01
  -2.34281445e+00 -9.67368782e-01  2.25748569e-01 -9.44747999e-02
  -3.09640646e-01 -1.30452168e+00  2.80700207e-01 -1.33084834e+00
  -9.54967201e-01 -1.04418528e+00 -2.22839022e+00 -1.09560812e+00
   1.24782884e+00  1.80284277e-01 -1.25238645e+00  7.14607239e-01
   1.95957351e+00  2.53216553e+00 -8.34777504e-02  1.11373827e-01
  -8.84591460e-01  1.04834628e+00 -5.54076493e-01  8.74013126e-01
   5.28820276e-01 -4.34727043e-01  8.74114633e-01  5.12598276e-01
   1.89701116e+00 -2.48368159e-01  1.54528201e+00  9.61921632e-01
   3.33936542e-01  6.79747701e-01 -5.69426119e-01  9.70423698e-01]
 [-1.02778935e+00 -2.27263141e-02  1.10301085e-01  1.79546729e-01
  -4.90769893e-02  5.96660793e-01  1.15810835e+00  6.27954483e-01
   1.70029372e-01 -1.79182720e+00  1.67022240e+00 -6.81339622e-01
  -1.44740534e+00  1.85646936e-01  3.88607174e-01 -1.27854240e+00
  -1.10837603e+00 -7.75742769e-01  2.63235778e-01  1.52836013e+00
  -3.64444673e-01 -7.31038809e-01  1.15823066e+00  4.90437865e-01
  -5.65739751e-01 -9.17869389e-01  4.85316277e-01 -5.30028522e-01
  -3.88954520e-01 -9.46969926e-01  2.02791953e+00 -1.98417687e+00
   2.03697324e-01  2.72669315e-01  8.71521354e-01  1.61161527e-01
  -5.35239339e-01  7.95005918e-01 -1.75809467e+00 -1.27538812e+00
   6.49030805e-01  7.76762664e-01 -1.38365650e+00  2.20103472e-01
  -6.78639174e-01  5.56474961e-02 -2.88346857e-01 -9.60100591e-01
   6.68293357e-01  3.41510355e-01  1.58940768e+00  8.42409909e-01
  -8.10967982e-01  7.20736444e-01  1.03318654e-01 -1.99774706e+00
  -5.00028312e-01 -8.31591904e-01  5.86176157e-01  1.26994348e+00
   1.11285400e+00 -2.34507990e+00  1.84525335e+00 -2.89478928e-01
   2.72452831e+00 -1.85729042e-01 -5.73727824e-02  3.77985537e-01
   7.77927995e-01  1.63456053e-01 -8.48517776e-01 -1.21194494e+00
   9.50055778e-01  6.42497420e-01  5.55373728e-01 -8.59387755e-01
   1.78057873e+00  3.59947473e-01  1.22501051e+00  4.28557783e-01
  -9.43535328e-01  1.48158407e+00  2.29413652e+00 -1.71246231e+00
  -7.68941283e-01 -1.05595326e+00 -8.25688243e-01  4.38418001e-01
  -1.34553814e+00 -3.07022452e-01 -2.85520107e-01  1.29907846e+00
   2.93988436e-01  8.37067842e-01  3.93039435e-01 -3.51285487e-01
   8.80326271e-01  7.99354970e-01  2.72147131e+00 -6.97772563e-01]
 [ 1.55318558e-01  1.26152441e-01  9.05780315e-01  8.18991438e-02
  -1.29863977e-01  2.24427879e-01 -1.33973733e-01  4.19006675e-01
   1.31296360e+00 -1.32973254e-01 -4.47266810e-02 -1.72328055e-01
  -7.81214893e-01  1.23739958e+00 -1.91514134e-01 -1.37162611e-01
  -3.27861980e-02  7.96586096e-01 -1.01332128e+00  9.38049018e-01
  -9.92036402e-01 -1.93410134e+00  5.83719552e-01 -1.21997699e-01
  -7.61240780e-01  7.48465629e-03 -1.03534722e+00 -3.08909953e-01
   5.18409371e-01 -3.92305017e-01 -2.04970264e+00  1.69121325e+00
  -9.20890093e-01 -3.47137898e-01 -1.20613670e+00  3.00528228e-01
   9.43905175e-01 -1.59808314e+00  1.40788376e+00  1.41724432e+00
   8.80906701e-01 -5.68516105e-02 -1.04051435e+00  9.45277989e-01
   7.76422098e-02  6.56548738e-01 -6.62871718e-01 -6.30390286e-01
  -8.90351236e-01 -3.24517757e-01 -8.22051048e-01 -7.51260757e-01
   1.96293175e-01  7.01368928e-01 -1.39238060e+00 -6.68631792e-01
   1.55344999e+00  4.97449428e-01  9.80282068e-01  5.71124673e-01
  -2.32236490e-01  7.84225240e-02 -8.50019217e-01  5.46521485e-01
  -3.93892229e-01  1.12404597e+00 -4.27607358e-01 -2.33180244e-02
  -9.29678082e-01 -1.25660062e+00 -7.16505647e-01 -1.50908187e-01
  -1.59349561e+00 -1.10606849e+00 -2.74965316e-01 -1.46609589e-01
  -2.04783964e+00  5.78249320e-02  1.45206213e+00 -9.06186819e-01
  -4.90492165e-01  5.79007506e-01  2.22233844e+00  2.66923189e-01
  -1.62423000e-01 -2.91489512e-01 -5.97359419e-01  3.06709170e-01
  -1.82558179e+00  1.38704881e-01 -1.57723725e+00 -5.51332712e-01
  -2.92212635e-01 -8.04641563e-03 -7.65457824e-02  8.14916015e-01
  -1.61039865e+00 -7.35562503e-01  1.21822762e+00  6.72223270e-01]] [ -54.486095 -244.06975    50.603287  200.90747  -138.96817 ]
('OPERATION_END_ELAPSED', 0.043, 'TRAINING')
CONVERT_MODEL  <class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
BEAUTIFIED_JSON_START
{
	"layers" : 	{
		"Layer_0" : 	{
			"NbInputs" : 0,
			"NbOutputs" : 100,
			"intercepts" : [  ],
			"name" : "Input_Layer"
		},
		"Layer_1" : 	{
			"NbInputs" : 100,
			"NbOutputs" : 4,
			"coeffs_00" : [ 0.013567287474870682, -0.11838601529598236, 0.010902983136475086, 0.10038960725069046 ],
			"coeffs_01" : [ 0.1877700388431549, 0.23242700099945068, 0.20616179704666138, -0.1339327096939087 ],
			"coeffs_02" : [ 0.13091157376766205, -0.09222513437271118, 0.1353159099817276, -0.028793852776288986 ],
			"coeffs_03" : [ -0.08126816153526306, -0.1561807543039322, -0.045355092734098434, -0.18489250540733337 ],
			"coeffs_04" : [ -0.09130217880010605, -0.01686461642384529, -0.162350133061409, -0.1680511087179184 ],
			"coeffs_05" : [ -0.14001211524009705, 0.0242862980812788, -0.18404965102672577, 0.11235321313142776 ],
			"coeffs_06" : [ 0.12731949985027313, 0.1894882470369339, 0.17616817355155945, -0.11243008822202682 ],
			"coeffs_07" : [ 0.10509834438562393, 0.03860945627093315, -0.2358935922384262, 0.12649159133434296 ],
			"coeffs_08" : [ -0.2358492910861969, -0.1071968600153923, 0.26955822110176086, -0.06230777129530907 ],
			"coeffs_09" : [ 0.025606466457247734, -0.14791075885295868, 0.07610896974802017, 0.1774950474500656 ],
			"coeffs_10" : [ 0.06928446143865585, 0.2064720094203949, -0.18018777668476105, 0.03828377276659012 ],
			"coeffs_11" : [ -0.16139186918735504, -0.10565944761037827, -0.019802534952759743, -0.08810208737850189 ],
			"coeffs_12" : [ 0.11316939443349838, -0.06249701604247093, 0.0893210917711258, -0.11166100203990936 ],
			"coeffs_13" : [ 0.12670089304447174, 0.17439673840999603, -0.06926582008600235, 0.10210447013378143 ],
			"coeffs_14" : [ 0.08500747382640839, 0.11296440660953522, 0.008996386080980301, -0.03816662356257439 ],
			"coeffs_15" : [ 0.1591172069311142, -0.18867264688014984, -0.12110895663499832, 0.010624912567436695 ],
			"coeffs_16" : [ 0.07768791913986206, -0.24032044410705566, 0.20386599004268646, -0.0634612888097763 ],
			"coeffs_17" : [ -0.15314586460590363, -0.10569736361503601, -0.1796601265668869, -0.1793973743915558 ],
			"coeffs_18" : [ -0.1268727332353592, 0.04267022758722305, -0.02328953891992569, 0.06008187681436539 ],
			"coeffs_19" : [ -0.01040839683264494, 0.2094770222902298, -0.20872092247009277, 0.06685493141412735 ],
			"coeffs_20" : [ 0.10186643898487091, 0.2406378835439682, -0.08695561438798904, -0.12114842236042023 ],
			"coeffs_21" : [ -0.17023442685604095, 0.051293060183525085, 0.07925236225128174, -0.027484450489282608 ],
			"coeffs_22" : [ 0.23500590026378632, -0.06715001910924911, 0.09882348775863647, -0.03583713620901108 ],
			"coeffs_23" : [ 0.11296968907117844, -0.13685911893844604, 0.2020220160484314, -0.10371967405080795 ],
			"coeffs_24" : [ 0.20006254315376282, 0.04529190436005592, -0.2534315884113312, -0.1438063532114029 ],
			"coeffs_25" : [ -0.12079029530286789, -0.2024654895067215, 0.19473423063755035, 0.034606654196977615 ],
			"coeffs_26" : [ 0.05757848545908928, 0.04708988592028618, 0.08930183947086334, -0.10297003388404846 ],
			"coeffs_27" : [ -0.16227371990680695, 0.08888097107410431, -0.08728410303592682, 0.24545633792877197 ],
			"coeffs_28" : [ -0.05193540081381798, 0.08886337280273438, -0.24346287548542023, 0.1805071085691452 ],
			"coeffs_29" : [ -0.01956510730087757, 0.12086920440196991, -0.05090365558862686, -0.24390102922916412 ],
			"coeffs_30" : [ 0.007884493097662926, 0.12492679059505463, 0.012297268956899643, -0.18935126066207886 ],
			"coeffs_31" : [ 0.0959012433886528, -0.20011013746261597, 0.1213257685303688, -0.03934289887547493 ],
			"coeffs_32" : [ 0.11845874041318893, 0.02998172491788864, 0.03970695659518242, 0.13573849201202393 ],
			"coeffs_33" : [ 0.10967153310775757, -0.013619845733046532, 0.05762936919927597, 0.2033310979604721 ],
			"coeffs_34" : [ 0.18814773857593536, 0.04236161336302757, 0.11337468028068542, -0.09047499299049377 ],
			"coeffs_35" : [ 0.07888972759246826, 0.02579667791724205, 0.16277053952217102, 0.11550308018922806 ],
			"coeffs_36" : [ 0.11901853233575821, 0.07467538863420486, -0.110820472240448, -0.20140133798122406 ],
			"coeffs_37" : [ -0.2039232701063156, 0.030019888654351234, 0.1446181982755661, -0.1808924823999405 ],
			"coeffs_38" : [ -0.19432716071605682, 0.016751691699028015, -0.05890050157904625, 0.09390278160572052 ],
			"coeffs_39" : [ 0.09330029785633087, 0.2317359745502472, 0.09040255099534988, -0.1753588765859604 ],
			"coeffs_40" : [ -0.0419907309114933, -0.1844705492258072, 0.07238209992647171, 0.13097290694713593 ],
			"coeffs_41" : [ -0.11337345838546753, -0.22703556716442108, 0.03155875578522682, -0.10272464901208878 ],
			"coeffs_42" : [ 0.20281611382961273, -0.03623292222619057, 0.03892022743821144, -0.063308484852314 ],
			"coeffs_43" : [ -0.08309044688940048, 0.06074082478880882, -0.09660705924034119, 0.2391813099384308 ],
			"coeffs_44" : [ -0.22107458114624023, -0.16777129471302032, -0.08054579049348831, -0.14483529329299927 ],
			"coeffs_45" : [ -0.20894354581832886, 0.16311481595039368, 0.25883376598358154, 0.11772803962230682 ],
			"coeffs_46" : [ 0.15703532099723816, -0.11656437814235687, -0.25695565342903137, -0.25787678360939026 ],
			"coeffs_47" : [ -0.2009819895029068, 0.22877739369869232, -0.07988829910755157, 0.21382394433021545 ],
			"coeffs_48" : [ -0.160304993391037, 0.00026552181225270033, -0.03739830479025841, 0.12032895535230637 ],
			"coeffs_49" : [ -0.18642902374267578, -0.11928229033946991, -0.14349287748336792, 0.22961145639419556 ],
			"coeffs_50" : [ -0.14505775272846222, 0.08968053013086319, -0.09303653985261917, 0.2665335536003113 ],
			"coeffs_51" : [ 0.15119551122188568, -0.08102049678564072, 0.07969141751527786, -0.12593238055706024 ],
			"coeffs_52" : [ -0.05545009672641754, 0.007457009982317686, 0.2703833281993866, -0.1593780517578125 ],
			"coeffs_53" : [ 0.16977642476558685, -0.14444038271903992, -0.14285683631896973, -0.18636806309223175 ],
			"coeffs_54" : [ -0.03766151890158653, -0.07924768328666687, -0.04922456294298172, 0.15508683025836945 ],
			"coeffs_55" : [ 0.016749603673815727, 0.11696678400039673, -0.0683615654706955, 0.11024202406406403 ],
			"coeffs_56" : [ 0.2113974243402481, 0.2441556751728058, -0.1190115287899971, 0.1599172055721283 ],
			"coeffs_57" : [ 0.13940145075321198, 0.005465609487146139, -0.03936105966567993, -0.003064496209844947 ],
			"coeffs_58" : [ -0.11943040043115616, 0.20952068269252777, -0.13163477182388306, -0.06700412929058075 ],
			"coeffs_59" : [ -0.013051016256213188, 0.22265657782554626, 0.02003435045480728, -0.01272973045706749 ],
			"coeffs_60" : [ -0.1193493977189064, 0.2339790165424347, 0.06165655702352524, -0.04704971984028816 ],
			"coeffs_61" : [ 0.05350322276353836, -0.0870102196931839, 0.26167330145835876, -0.09073097258806229 ],
			"coeffs_62" : [ -0.12569579482078552, -0.13666212558746338, 0.0577799491584301, -0.13996221125125885 ],
			"coeffs_63" : [ -0.15132275223731995, -0.11628782749176025, -0.1705603301525116, 0.061281073838472366 ],
			"coeffs_64" : [ -0.022954309359192848, -0.12473122775554657, -0.13076534867286682, 0.024200253188610077 ],
			"coeffs_65" : [ 0.16040439903736115, -0.0729065090417862, 0.17781805992126465, 0.2334960550069809 ],
			"coeffs_66" : [ -0.005224630702286959, -0.22398361563682556, 0.013363496400415897, -0.0340024009346962 ],
			"coeffs_67" : [ 0.11907012015581131, -0.19845812022686005, 0.11652349680662155, 0.18190091848373413 ],
			"coeffs_68" : [ -0.13575634360313416, 0.17408181726932526, -0.11264953017234802, -0.12937287986278534 ],
			"coeffs_69" : [ -0.07307920604944229, -0.17486736178398132, 0.04048271104693413, -0.138299360871315 ],
			"coeffs_70" : [ 0.03160347789525986, -0.02677304856479168, 0.2554466128349304, -0.058305926620960236 ],
			"coeffs_71" : [ 0.11743546277284622, 0.08818057179450989, -0.11392482370138168, 0.13638116419315338 ],
			"coeffs_72" : [ 0.06149248033761978, -0.11333432793617249, -0.10910714417695999, -0.09457691013813019 ],
			"coeffs_73" : [ 0.11118366569280624, -0.23196102678775787, -0.0018705109832808375, -0.12145758420228958 ],
			"coeffs_74" : [ -0.15035699307918549, 0.15887999534606934, -0.004667453933507204, -0.11857614666223526 ],
			"coeffs_75" : [ 0.1810564249753952, -0.16012567281723022, 0.09138727933168411, -0.23922862112522125 ],
			"coeffs_76" : [ 0.05374714732170105, 0.09993408620357513, 0.022409146651625633, 0.02973181940615177 ],
			"coeffs_77" : [ -0.015312405303120613, -0.1197752058506012, -0.16057920455932617, -0.2162299007177353 ],
			"coeffs_78" : [ -0.025031153112649918, 0.09553128480911255, 0.02938271500170231, -0.15002961456775665 ],
			"coeffs_79" : [ 0.00400786055251956, -0.22250762581825256, 0.02806936576962471, -0.17186211049556732 ],
			"coeffs_80" : [ -0.16543574631214142, 0.23811085522174835, 0.1432959884405136, 0.12711770832538605 ],
			"coeffs_81" : [ 0.10098767280578613, -0.1041499450802803, -0.12457480281591415, 0.19792956113815308 ],
			"coeffs_82" : [ -0.14580272138118744, -0.14551867544651031, 0.09341516345739365, 0.25413230061531067 ],
			"coeffs_83" : [ -0.06336041539907455, 0.151900053024292, 0.17752784490585327, -0.1038350760936737 ],
			"coeffs_84" : [ -0.05450784042477608, -0.05346417427062988, -0.15057747066020966, -0.16017873585224152 ],
			"coeffs_85" : [ 0.26128044724464417, -0.26664578914642334, -0.08874164521694183, -0.0012107224902138114 ],
			"coeffs_86" : [ 0.18003873527050018, -0.11347881704568863, 0.1411956250667572, 0.07675695419311523 ],
			"coeffs_87" : [ -0.20075924694538116, -0.14273659884929657, -0.1508813500404358, -0.14197243750095367 ],
			"coeffs_88" : [ -0.10200279951095581, -0.030690303072333336, 0.10369375348091125, 0.09573712199926376 ],
			"coeffs_89" : [ -0.05592353269457817, -0.19416983425617218, -0.10714685171842575, -0.15292370319366455 ],
			"coeffs_90" : [ -0.00439748540520668, -0.15892082452774048, 0.005639589391648769, -0.21812699735164642 ],
			"coeffs_91" : [ 0.20188452303409576, -0.05093074589967728, -0.2660468518733978, -0.241578608751297 ],
			"coeffs_92" : [ -0.02836320735514164, -0.06335193663835526, -0.08363731950521469, 0.033354539424180984 ],
			"coeffs_93" : [ -0.14737731218338013, -0.15182431042194366, -0.1316036581993103, 0.07850782573223114 ],
			"coeffs_94" : [ -0.2334962785243988, -0.05696912109851837, 0.22491589188575745, -0.20468558371067047 ],
			"coeffs_95" : [ -0.11705663055181503, 0.09575746953487396, 0.05923476442694664, -0.08370551466941833 ],
			"coeffs_96" : [ 0.013998810201883316, 0.15281780064105988, 0.14926479756832123, -0.07741694152355194 ],
			"coeffs_97" : [ 0.15415121614933014, 0.2695345878601074, -0.09713172167539597, -0.22663579881191254 ],
			"coeffs_98" : [ 0.04193704202771187, -0.10236218571662903, -0.15309934318065643, 0.08972486108541489 ],
			"coeffs_99" : [ -0.2027391791343689, -0.19610236585140228, 0.18804539740085602, 0.19324661791324615 ],
			"intercepts" : [ 0.19069844484329224, -0.1205860823392868, 0.21781513094902039, -0.11341796070337296 ],
			"name" : "Hidden_Layer_1"
		},
		"Layer_2" : 	{
			"NbInputs" : 4,
			"NbOutputs" : 8,
			"coeffs_0" : [ -0.12556248903274536, -0.6417979598045349, -0.4187193512916565, 0.6015088558197021, 0.3634248971939087, -0.18924568593502045, 0.49361541867256165, -0.6171974539756775 ],
			"coeffs_1" : [ -0.6357513666152954, 0.09952858835458755, 0.4689285457134247, 0.15431031584739685, 0.5531511902809143, -0.11606933921575546, -0.03197576478123665, -0.3833795189857483 ],
			"coeffs_2" : [ 0.41886427998542786, 0.3419215679168701, 0.6651707887649536, -0.283683717250824, -0.2723327875137329, -0.04919262230396271, 0.24445006251335144, -0.5258257389068604 ],
			"coeffs_3" : [ 0.024514630436897278, 0.3231998682022095, -0.6113094687461853, -0.27096623182296753, -0.34501853585243225, 0.6801255345344543, -0.013050062581896782, -0.45945510268211365 ],
			"intercepts" : [ 0.5446482300758362, -0.5008590817451477, 0.15202584862709045, 0.7388614416122437, -0.6215298771858215, 0.00040139397606253624, 0.4350445866584778, 0.6255862712860107 ],
			"name" : "Hidden_Layer_2"
		},
		"Layer_3" : 	{
			"NbInputs" : 8,
			"NbOutputs" : 6,
			"coeffs_0" : [ -0.06464559584856033, -0.35967928171157837, -0.378379225730896, -0.26582270860671997, 0.3547782599925995, 0.592244565486908 ],
			"coeffs_1" : [ 0.3263840973377228, -0.3886035978794098, -0.4743886888027191, 0.45654937624931335, -0.2814547121524811, 0.2735561430454254 ],
			"coeffs_2" : [ 0.18584848940372467, 0.004691778216511011, -0.2990300953388214, 0.3646720349788666, -0.5755610466003418, 0.27600815892219543 ],
			"coeffs_3" : [ 0.607689619064331, 0.2481256127357483, 0.20839306712150574, 0.2646118700504303, 0.31632280349731445, -0.2192927896976471 ],
			"coeffs_4" : [ 0.10418561100959778, 0.19535048305988312, 0.39690321683883667, -0.2616834342479706, 0.20154710114002228, -0.5913419723510742 ],
			"coeffs_5" : [ -0.3576091527938843, 0.012639082036912441, 0.5547939538955688, -0.045520368963479996, -0.2114885449409485, -0.45193609595298767 ],
			"coeffs_6" : [ -0.5050475001335144, 0.20278577506542206, -0.6562281250953674, 0.478994756937027, 0.4347243905067444, 0.3632619380950928 ],
			"coeffs_7" : [ -0.023249708116054535, -0.39448171854019165, 0.6076443791389465, 0.6004999279975891, 0.05998411402106285, 0.04822267219424248 ],
			"intercepts" : [ 0.6754751205444336, -0.35470402240753174, -0.47783586382865906, -0.1859980970621109, 0.5120916366577148, -0.5658259391784668 ],
			"name" : "Hidden_Layer_3"
		},
		"Layer_4" : 	{
			"NbInputs" : 6,
			"NbOutputs" : 1,
			"coeffs_0" : [ 0.7713729739189148 ],
			"coeffs_1" : [ -0.1932390034198761 ],
			"coeffs_2" : [ 0.6318441033363342 ],
			"coeffs_3" : [ 0.8060404062271118 ],
			"coeffs_4" : [ 0.7105996012687683 ],
			"coeffs_5" : [ -0.5374246835708618 ],
			"intercepts" : [ -0.262901246547699 ],
			"name" : "Output_Layer"
		},
		"sizes" : [ 100, 4, 8, 6, 1 ]
	},
	"metadata" :  { "model" : "sklearn.neural_network._multilayer_perceptron.MLPRegressor", "version" : "1.4.1.post1" },
	"options" :  { "activation" : "relu", "alpha" : 0.0001, "batch_size" : "auto", "beta_1" : 0.9, "beta_2" : 0.999, "early_stopping" : false, "epsilon" : 1e-08, "hidden_layer_sizes" : [ 4, 8, 6 ], "learning_rate" : "constant", "learning_rate_init" : 0.001, "max_fun" : 15000, "max_iter" : 32, "momentum" : 0.9, "n_iter_no_change" : 10, "nesterovs_momentum" : true, "power_t" : 0.5, "random_state" : 1789, "shuffle" : false, "solver" : "adam", "tol" : 0.0001, "validation_fraction" : 0.1, "verbose" : false, "warm_start" : false }
}
BEAUTIFIED_JSON_END
('OPERATION_START', 'PREDICT')
[ 1.30763316e+00  7.15823054e-01  7.76257336e-01  1.53725290e+00
  2.79271603e-03  1.52167082e+00  2.01279730e-01  2.66802120e+00
  1.56943560e+00  1.79947901e+00  1.90916681e+00  2.24928856e+00
  1.49295950e+00  1.51075077e+00  1.90282178e+00  8.56554925e-01
  1.59081626e+00  9.11477864e-01  2.65623331e-01  1.38893366e+00
  1.20492554e+00  9.79863465e-01  5.91707468e-01  1.05309784e-01
  1.59081626e+00  1.06639481e+00  2.84790337e-01  3.18021441e+00
  7.81459630e-01  1.15621090e+00  1.09722614e+00  2.39781308e+00
  1.41773081e+00  1.75325847e+00  7.61831939e-01  2.57037354e+00
  2.56950617e+00  1.49947476e+00  3.53135109e-01  1.56783175e+00
  1.46523428e+00  2.05851078e+00 -4.79295850e-03  1.04019213e+00
  5.44847727e-01  7.08902359e-01  1.77192116e+00  2.18431377e+00
  1.35118437e+00  1.09891725e+00  3.18317711e-01  1.52668381e+00
  1.12557030e+00  3.01084399e-01  1.33599734e+00  1.10465360e+00
  2.14487672e+00  1.26344538e+00  1.13230395e+00  1.59081626e+00
  1.58210921e+00  1.03890157e+00  2.20184612e+00  3.54292202e+00
  1.59081626e+00  1.23183370e+00  2.15619421e+00  1.53776121e+00
  2.28456020e+00  2.05743623e+00  1.56941676e+00  1.86368322e+00
  8.05849850e-01  2.59876299e+00  5.17959952e-01  2.30053806e+00
  1.91342640e+00  1.20904613e+00  4.59077263e+00  6.58732712e-01
  1.93106771e+00  2.92464232e+00  7.02718854e-01  3.47109675e-01
  1.59749150e+00  7.02596784e-01  1.75132871e+00  2.18034059e-01
  4.23753500e-01  1.20576835e+00  3.25558007e-01  1.99973285e-01
  2.59614944e+00  1.94750786e+00  2.93568516e+00  1.91169739e+00
  1.69309664e+00  1.59081626e+00  2.19559240e+00  1.55648017e+00
  3.21422052e+00  1.34595108e+00  6.43240452e-01  1.54887462e+00
  1.17475510e+00  1.59081626e+00  4.98411715e-01  1.74177766e+00
  1.59081626e+00  2.49093533e+00  4.60037768e-01  2.29713249e+00
  1.15171695e+00  2.69849157e+00  1.64699650e+00  3.78978550e-01
  1.59081626e+00  1.90323067e+00  9.55021679e-01  3.01722813e+00
  1.99186611e+00  1.31844568e+00  7.45496929e-01  9.06349003e-01
  2.50185132e+00  1.24532866e+00  3.76320839e+00  1.50812554e+00]
('OPERATION_END_ELAPSED', 0.003, 'PREDICT')
MODEL_PERFS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'RandomReg_100_sampled', 'size': 128, 'mse': 23520.242, 'mae': 120.79663, 'mape': 0.99674755, 'r2': 0.003576905626038518}
WRITING_PERF_CODE 'logs/auto_tests/regression/MLPRegressor/sklearn.neural_network._multilayer_perceptron.MLPRegressor_RandomReg_100_sampled_option_1.perf'

MODEL_PERFS_TIMINGS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'RandomReg_100_sampled', 'training_time_in_sec': 0.043, 'prediction_time_in_sec': 0.003}
