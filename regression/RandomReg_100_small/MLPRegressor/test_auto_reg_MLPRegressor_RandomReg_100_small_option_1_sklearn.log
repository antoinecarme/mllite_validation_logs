         X_0       X_1       X_2  ...      X_98      X_99      target
0  -0.301125  1.387725  1.263659  ... -0.345798  0.839573  -54.486094
1  -0.155583 -1.181091  2.934304  ... -1.884572 -0.938959 -244.069750
2   1.079390 -0.496549 -1.240677  ... -0.569426  0.970424   50.603288
3  -1.027789 -0.022726  0.110301  ...  2.721471 -0.697773  200.907470
4   0.155319  0.126152  0.905780  ...  1.218228  0.672223 -138.968166
..       ...       ...       ...  ...       ...       ...         ...
59  0.389605 -0.609059 -0.373385  ... -0.652537  0.037929  211.129681
60 -0.553760 -0.158458  0.247315  ...  0.434919 -1.537177   -7.629069
61  0.261579  0.836463  0.899340  ... -0.770119 -0.320833   18.431094
62  0.578196 -0.097941  0.019108  ...  1.115285  0.315342  -11.260213
63 -0.080896  0.160145  1.489778  ... -0.028132  0.637110  266.257878

[64 rows x 101 columns]
SKLEARN_MODEL_SET_OPTIONS MLPRegressor {"hidden_layer_sizes" : [4, 8, 6]}
('OPERATION_START', 'TRAINING')
[[-3.01124662e-01  1.38772476e+00  1.26365924e+00 -6.82732940e-01
  -1.34683120e+00  5.14777362e-01  1.62561178e-01 -1.08774674e+00
   2.71217942e-01  1.01093900e+00 -2.49382392e-01 -1.65061939e+00
  -9.93423760e-01  1.09921061e-01 -5.43385804e-01  1.32529700e+00
   1.88495958e+00  7.49242187e-01  1.60594404e+00  1.60841656e+00
   9.63199854e-01 -3.32925580e-02  1.82730234e+00 -8.89043733e-02
   1.03326552e-01  1.23202717e+00 -1.62939775e+00  8.68012488e-01
   4.37752008e-01  5.41352630e-01  4.58516777e-01 -1.81859002e-01
  -1.24955714e-01 -4.90596294e-02  1.27540767e+00  3.60171229e-01
   9.01617110e-01 -2.70372570e-01  5.28253436e-01  1.62672818e-01
  -7.59159401e-02 -1.01459682e+00 -1.15195643e-02  1.09213448e+00
   1.62921584e+00 -4.38633710e-01 -1.36951172e+00 -3.69843543e-01
  -8.60606253e-01 -5.75633466e-01  1.27469802e+00  4.84667391e-01
  -1.51542473e+00  1.16384223e-01 -8.54173601e-01 -7.89357781e-01
  -3.20154727e-01  6.83034718e-01 -4.90488261e-01 -1.72152758e+00
  -5.88555992e-01  2.26808488e-01  1.35770679e+00 -7.94542491e-01
  -2.14967608e+00 -2.36072704e-01 -8.44686925e-01 -1.26111269e+00
  -8.60300481e-01  1.04535091e+00  1.55022159e-01  3.35658103e-01
  -1.93393087e+00 -3.61082911e-01  1.09404229e-01 -1.99066043e-01
   4.23870265e-01  1.72599167e-01  8.43244314e-01  3.28669012e-01
   5.97395658e-01 -3.46147346e+00 -1.11853528e+00  1.95144802e-01
   1.27475369e+00 -6.44126832e-01  1.99323013e-01 -3.13881099e-01
   8.67388427e-01  1.60537791e+00 -4.69321199e-02  2.43432593e+00
   1.97670710e+00 -2.16416430e+00 -3.69843096e-01 -3.87633115e-01
  -1.22007036e+00 -1.98171616e+00 -3.45797569e-01  8.39573085e-01]
 [-1.55582651e-01 -1.18109095e+00  2.93430400e+00 -6.94847047e-01
   7.65346646e-01 -1.38265729e-01 -1.87197840e+00 -5.47255754e-01
  -1.02196300e+00 -2.40618020e-01  1.53626862e-03 -1.30697155e+00
   1.12571657e+00 -6.09562814e-01  1.84250534e+00  5.26184618e-01
  -1.34199113e-01 -4.64435697e-01 -2.08943471e-01 -7.25073040e-01
  -3.43274266e-01 -4.16672796e-01 -1.43184513e-01 -1.58227885e+00
   5.83837748e-01 -6.71794951e-01  1.48207486e+00  8.79976228e-02
  -1.44496679e+00 -1.04719102e+00 -7.33625665e-02  7.97347367e-01
  -1.49433851e+00 -4.77036446e-01  1.96962878e-01  9.63585079e-01
  -1.06893802e+00  6.35676011e-02 -2.22821608e-01  2.68229580e+00
  -1.22432196e+00  2.33570918e-01  3.37046921e-01 -8.80938113e-01
   3.74312043e-01  4.75478441e-01 -4.31405723e-01 -1.49277854e+00
  -7.05678761e-01  8.41879070e-01 -9.39537287e-01 -1.32261544e-01
   1.27473974e+00 -2.45601356e-01 -1.09500504e+00 -1.12451386e+00
  -1.68831348e+00 -1.66038191e+00  8.83513212e-01 -2.12799621e+00
   1.85301650e+00  1.06351781e+00 -1.55932271e+00 -3.31427574e-01
  -4.71284389e-01 -8.42893183e-01  2.66877890e-01 -2.19565415e+00
  -1.28610337e+00  2.07863837e-01  2.12718658e-02 -1.87917411e-01
   2.03328684e-01 -1.13658619e+00  7.13958859e-01  3.48399356e-02
   8.75229090e-02 -1.98537886e-01  8.98726761e-01 -1.00354958e+00
   3.02103549e-01  5.83664238e-01 -1.10631919e+00 -7.53618121e-01
   2.54740193e-02 -5.73981524e-01  6.41987205e-01  1.07664418e+00
   8.75026762e-01 -4.93790269e-01  7.25373566e-01  8.76517355e-01
   1.12196028e+00  2.01063013e+00  7.10269690e-01  3.98254842e-01
  -1.09351709e-01 -6.48999751e-01 -1.88457203e+00 -9.38958764e-01]
 [ 1.07939029e+00 -4.96548653e-01 -1.24067736e+00  2.76984543e-01
  -8.75897765e-01  3.70610416e-01  2.94699460e-01  2.08256388e+00
   1.13252175e+00  1.22751558e+00 -1.09523833e+00  5.95500886e-01
   1.12185657e+00  7.54166365e-01 -8.05529773e-01 -1.03034163e+00
   1.09279633e+00  1.50435269e+00  1.11762017e-01  1.07449919e-01
  -1.02401459e+00 -1.84641510e-01  7.72709399e-02 -4.99782264e-01
  -2.27677077e-01  6.49656951e-02  1.01747751e-01  1.98065881e-02
   5.62927604e-01  1.23536706e+00 -1.95974544e-01 -2.40673900e-01
   4.45345312e-01  5.92050970e-01  1.02269816e+00 -1.74345989e-02
  -1.73933551e-01 -6.29255697e-02 -2.20695183e-01  1.29462636e+00
   2.52112865e-01 -2.17039728e+00  9.85382497e-01 -7.44955420e-01
   1.57546902e+00  5.03206611e-01 -9.72602367e-01 -6.84496820e-01
   5.72718233e-02 -4.53426331e-01  5.94903529e-01  1.22255921e+00
   7.85897672e-01  6.81237221e-01  1.01942651e-01  4.92852837e-01
   5.66514671e-01 -1.17636359e+00  5.35346568e-01  1.32921374e+00
   1.24366212e+00 -5.73699355e-01  1.01102638e+00  8.36099803e-01
  -2.34281445e+00 -9.67368782e-01  2.25748569e-01 -9.44747999e-02
  -3.09640646e-01 -1.30452168e+00  2.80700207e-01 -1.33084834e+00
  -9.54967201e-01 -1.04418528e+00 -2.22839022e+00 -1.09560812e+00
   1.24782884e+00  1.80284277e-01 -1.25238645e+00  7.14607239e-01
   1.95957351e+00  2.53216553e+00 -8.34777504e-02  1.11373827e-01
  -8.84591460e-01  1.04834628e+00 -5.54076493e-01  8.74013126e-01
   5.28820276e-01 -4.34727043e-01  8.74114633e-01  5.12598276e-01
   1.89701116e+00 -2.48368159e-01  1.54528201e+00  9.61921632e-01
   3.33936542e-01  6.79747701e-01 -5.69426119e-01  9.70423698e-01]
 [-1.02778935e+00 -2.27263141e-02  1.10301085e-01  1.79546729e-01
  -4.90769893e-02  5.96660793e-01  1.15810835e+00  6.27954483e-01
   1.70029372e-01 -1.79182720e+00  1.67022240e+00 -6.81339622e-01
  -1.44740534e+00  1.85646936e-01  3.88607174e-01 -1.27854240e+00
  -1.10837603e+00 -7.75742769e-01  2.63235778e-01  1.52836013e+00
  -3.64444673e-01 -7.31038809e-01  1.15823066e+00  4.90437865e-01
  -5.65739751e-01 -9.17869389e-01  4.85316277e-01 -5.30028522e-01
  -3.88954520e-01 -9.46969926e-01  2.02791953e+00 -1.98417687e+00
   2.03697324e-01  2.72669315e-01  8.71521354e-01  1.61161527e-01
  -5.35239339e-01  7.95005918e-01 -1.75809467e+00 -1.27538812e+00
   6.49030805e-01  7.76762664e-01 -1.38365650e+00  2.20103472e-01
  -6.78639174e-01  5.56474961e-02 -2.88346857e-01 -9.60100591e-01
   6.68293357e-01  3.41510355e-01  1.58940768e+00  8.42409909e-01
  -8.10967982e-01  7.20736444e-01  1.03318654e-01 -1.99774706e+00
  -5.00028312e-01 -8.31591904e-01  5.86176157e-01  1.26994348e+00
   1.11285400e+00 -2.34507990e+00  1.84525335e+00 -2.89478928e-01
   2.72452831e+00 -1.85729042e-01 -5.73727824e-02  3.77985537e-01
   7.77927995e-01  1.63456053e-01 -8.48517776e-01 -1.21194494e+00
   9.50055778e-01  6.42497420e-01  5.55373728e-01 -8.59387755e-01
   1.78057873e+00  3.59947473e-01  1.22501051e+00  4.28557783e-01
  -9.43535328e-01  1.48158407e+00  2.29413652e+00 -1.71246231e+00
  -7.68941283e-01 -1.05595326e+00 -8.25688243e-01  4.38418001e-01
  -1.34553814e+00 -3.07022452e-01 -2.85520107e-01  1.29907846e+00
   2.93988436e-01  8.37067842e-01  3.93039435e-01 -3.51285487e-01
   8.80326271e-01  7.99354970e-01  2.72147131e+00 -6.97772563e-01]
 [ 1.55318558e-01  1.26152441e-01  9.05780315e-01  8.18991438e-02
  -1.29863977e-01  2.24427879e-01 -1.33973733e-01  4.19006675e-01
   1.31296360e+00 -1.32973254e-01 -4.47266810e-02 -1.72328055e-01
  -7.81214893e-01  1.23739958e+00 -1.91514134e-01 -1.37162611e-01
  -3.27861980e-02  7.96586096e-01 -1.01332128e+00  9.38049018e-01
  -9.92036402e-01 -1.93410134e+00  5.83719552e-01 -1.21997699e-01
  -7.61240780e-01  7.48465629e-03 -1.03534722e+00 -3.08909953e-01
   5.18409371e-01 -3.92305017e-01 -2.04970264e+00  1.69121325e+00
  -9.20890093e-01 -3.47137898e-01 -1.20613670e+00  3.00528228e-01
   9.43905175e-01 -1.59808314e+00  1.40788376e+00  1.41724432e+00
   8.80906701e-01 -5.68516105e-02 -1.04051435e+00  9.45277989e-01
   7.76422098e-02  6.56548738e-01 -6.62871718e-01 -6.30390286e-01
  -8.90351236e-01 -3.24517757e-01 -8.22051048e-01 -7.51260757e-01
   1.96293175e-01  7.01368928e-01 -1.39238060e+00 -6.68631792e-01
   1.55344999e+00  4.97449428e-01  9.80282068e-01  5.71124673e-01
  -2.32236490e-01  7.84225240e-02 -8.50019217e-01  5.46521485e-01
  -3.93892229e-01  1.12404597e+00 -4.27607358e-01 -2.33180244e-02
  -9.29678082e-01 -1.25660062e+00 -7.16505647e-01 -1.50908187e-01
  -1.59349561e+00 -1.10606849e+00 -2.74965316e-01 -1.46609589e-01
  -2.04783964e+00  5.78249320e-02  1.45206213e+00 -9.06186819e-01
  -4.90492165e-01  5.79007506e-01  2.22233844e+00  2.66923189e-01
  -1.62423000e-01 -2.91489512e-01 -5.97359419e-01  3.06709170e-01
  -1.82558179e+00  1.38704881e-01 -1.57723725e+00 -5.51332712e-01
  -2.92212635e-01 -8.04641563e-03 -7.65457824e-02  8.14916015e-01
  -1.61039865e+00 -7.35562503e-01  1.21822762e+00  6.72223270e-01]] [ -54.486095 -244.06975    50.603287  200.90747  -138.96817 ]
('OPERATION_END_ELAPSED', 0.036, 'TRAINING')
CONVERT_MODEL  <class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
BEAUTIFIED_JSON_START
{
	"layers" : 	{
		"Layer_0" : 	{
			"NbInputs" : 0,
			"NbOutputs" : 100,
			"intercepts" : [  ],
			"name" : "Input_Layer"
		},
		"Layer_1" : 	{
			"NbInputs" : 100,
			"NbOutputs" : 4,
			"coeffs_00" : [ -0.014497306197881699, -0.1316748410463333, 0.03636784479022026, 0.10115250945091248 ],
			"coeffs_01" : [ 0.23908452689647675, 0.2316962629556656, 0.2084246277809143, -0.13420884311199188 ],
			"coeffs_02" : [ 0.09372919797897339, -0.11379408091306686, 0.13640452921390533, -0.004455056972801685 ],
			"coeffs_03" : [ -0.07984891533851624, -0.1539023220539093, 0.008816723711788654, -0.19505134224891663 ],
			"coeffs_04" : [ -0.08765226602554321, -0.029481275007128716, -0.1432257443666458, -0.20469684898853302 ],
			"coeffs_05" : [ -0.20488518476486206, 0.05089055746793747, -0.12965846061706543, 0.11352939903736115 ],
			"coeffs_06" : [ 0.12714709341526031, 0.21290524303913116, 0.13614514470100403, -0.11146897077560425 ],
			"coeffs_07" : [ 0.10483602434396744, 0.04964869096875191, -0.23155853152275085, 0.06503944844007492 ],
			"coeffs_08" : [ -0.17362110316753387, -0.10749569535255432, 0.22245679795742035, -0.06408434361219406 ],
			"coeffs_09" : [ 0.011582176201045513, -0.14437465369701385, 0.07655544579029083, 0.17765364050865173 ],
			"coeffs_10" : [ 0.06829346716403961, 0.1567053347826004, -0.18380782008171082, 0.005748862400650978 ],
			"coeffs_11" : [ -0.1762477457523346, -0.10919685661792755, -0.020360687747597694, -0.08678413182497025 ],
			"coeffs_12" : [ 0.1137874498963356, -0.06686937808990479, 0.06899698078632355, -0.11154113709926605 ],
			"coeffs_13" : [ 0.17497651278972626, 0.2033255398273468, -0.07203203439712524, 0.04497832432389259 ],
			"coeffs_14" : [ 0.08893038332462311, 0.1704854518175125, 0.04766838997602463, -0.08342175930738449 ],
			"coeffs_15" : [ 0.12042848020792007, -0.1766224056482315, -0.1232411190867424, 0.060260262340307236 ],
			"coeffs_16" : [ 0.10043417662382126, -0.1973191648721695, 0.20106613636016846, -0.12193705886602402 ],
			"coeffs_17" : [ -0.18232087790966034, -0.09093545377254486, -0.1888258010149002, -0.18016448616981506 ],
			"coeffs_18" : [ -0.12972404062747955, 0.04884898662567139, -0.02424457110464573, 0.06103594973683357 ],
			"coeffs_19" : [ -0.06065726280212402, 0.2247772216796875, -0.20158225297927856, 0.06697380542755127 ],
			"coeffs_20" : [ 0.07365258038043976, 0.18710722029209137, -0.08129290491342545, -0.08240097016096115 ],
			"coeffs_21" : [ -0.17044204473495483, 0.05236870422959328, 0.08089609444141388, -0.02337050437927246 ],
			"coeffs_22" : [ 0.2350543737411499, -0.06680687516927719, 0.09877240657806396, -0.03938978910446167 ],
			"coeffs_23" : [ 0.11667466908693314, -0.13585422933101654, 0.17725831270217896, -0.10234009474515915 ],
			"coeffs_24" : [ 0.19857735931873322, 0.03966731205582619, -0.24839262664318085, -0.10381125658750534 ],
			"coeffs_25" : [ -0.06514667719602585, -0.1858101636171341, 0.19515417516231537, -0.0205942802131176 ],
			"coeffs_26" : [ 0.058591295033693314, 0.04569133371114731, 0.09349148720502853, -0.09723079949617386 ],
			"coeffs_27" : [ -0.15164552628993988, 0.08548366278409958, -0.0739777535200119, 0.24299496412277222 ],
			"coeffs_28" : [ -0.05896659567952156, 0.09890568256378174, -0.20123712718486786, 0.17961043119430542 ],
			"coeffs_29" : [ -0.017911529168486595, 0.17759372293949127, -0.062334779649972916, -0.26791059970855713 ],
			"coeffs_30" : [ 0.008069821633398533, 0.12484768778085709, 0.01133655197918415, -0.18852536380290985 ],
			"coeffs_31" : [ 0.09610490500926971, -0.18450576066970825, 0.13043184578418732, -0.001132679171860218 ],
			"coeffs_32" : [ 0.11846205592155457, -0.0006643530214205384, 0.04032959043979645, 0.13704872131347656 ],
			"coeffs_33" : [ 0.0973786860704422, -0.007647410966455936, 0.059161677956581116, 0.2059364914894104 ],
			"coeffs_34" : [ 0.18821826577186584, 0.02426736429333687, 0.10355278104543686, -0.09229522943496704 ],
			"coeffs_35" : [ 0.07890477031469345, 0.024633247405290604, 0.15551196038722992, 0.11577139049768448 ],
			"coeffs_36" : [ 0.117802195250988, 0.0743107870221138, -0.11159022897481918, -0.2016436904668808 ],
			"coeffs_37" : [ -0.2635136842727661, 0.0227546077221632, 0.14482545852661133, -0.17521031200885773 ],
			"coeffs_38" : [ -0.14989358186721802, 0.016584284603595734, -0.08794091641902924, 0.09624341130256653 ],
			"coeffs_39" : [ 0.09506907314062119, 0.23148685693740845, 0.08590421825647354, -0.17413771152496338 ],
			"coeffs_40" : [ -0.042478494346141815, -0.18557791411876678, 0.07611198723316193, 0.1332116574048996 ],
			"coeffs_41" : [ -0.13784338533878326, -0.1660488396883011, 0.09087308496236801, -0.10310309380292892 ],
			"coeffs_42" : [ 0.23955953121185303, -0.0481828898191452, 0.03634928539395332, -0.0988292545080185 ],
			"coeffs_43" : [ -0.08388802409172058, 0.05920468270778656, -0.08047928661108017, 0.23887105286121368 ],
			"coeffs_44" : [ -0.219878688454628, -0.1760837435722351, -0.08427279442548752, -0.15372984111309052 ],
			"coeffs_45" : [ -0.20511126518249512, 0.17896561324596405, 0.26680514216423035, 0.060454241931438446 ],
			"coeffs_46" : [ 0.15750765800476074, -0.11671710759401321, -0.2561367154121399, -0.2568883001804352 ],
			"coeffs_47" : [ -0.22866672277450562, 0.17070172727108002, -0.07823728770017624, 0.2174980491399765 ],
			"coeffs_48" : [ -0.1884496659040451, -0.020085351541638374, -0.020319640636444092, 0.1193753033876419 ],
			"coeffs_49" : [ -0.18649549782276154, -0.12010770291090012, -0.1418350487947464, 0.23161044716835022 ],
			"coeffs_50" : [ -0.15575405955314636, 0.09717438369989395, -0.09291202574968338, 0.26355063915252686 ],
			"coeffs_51" : [ 0.18715061247348785, -0.08985485881567001, 0.07356847822666168, -0.12620800733566284 ],
			"coeffs_52" : [ -0.061977386474609375, 0.004694697912782431, 0.2693769335746765, -0.1596798300743103 ],
			"coeffs_53" : [ 0.1690370738506317, -0.15963919460773468, -0.15121136605739594, -0.1875353753566742 ],
			"coeffs_54" : [ -0.045188046991825104, -0.07222230732440948, 0.0054186140187084675, 0.15956540405750275 ],
			"coeffs_55" : [ -0.040147412568330765, 0.11153677850961685, -0.09870542585849762, 0.12118265777826309 ],
			"coeffs_56" : [ 0.21165943145751953, 0.24299561977386475, -0.13116313517093658, 0.15999388694763184 ],
			"coeffs_57" : [ 0.1860775649547577, 0.008214068599045277, -0.06232894957065582, -0.004509743303060532 ],
			"coeffs_58" : [ -0.12043270468711853, 0.20850123465061188, -0.12403158098459244, -0.11493313312530518 ],
			"coeffs_59" : [ -0.013139990158379078, 0.22169314324855804, 0.021539106965065002, -0.011697182431817055 ],
			"coeffs_60" : [ -0.1185898631811142, 0.24276040494441986, 0.06372781842947006, -0.05975506082177162 ],
			"coeffs_61" : [ 0.042107243090867996, -0.08583016693592072, 0.26744142174720764, -0.08981864154338837 ],
			"coeffs_62" : [ -0.12409594655036926, -0.11451491713523865, 0.05739235505461693, -0.1895608901977539 ],
			"coeffs_63" : [ -0.18420124053955078, -0.12028015404939651, -0.14292947947978973, 0.057513393461704254 ],
			"coeffs_64" : [ -0.022971568629145622, -0.12263960391283035, -0.14036622643470764, 0.01824222505092621 ],
			"coeffs_65" : [ 0.15851128101348877, -0.0747700110077858, 0.12471487373113632, 0.23301656544208527 ],
			"coeffs_66" : [ -0.005725948140025139, -0.19275575876235962, 0.05211064592003822, -0.03557682782411575 ],
			"coeffs_67" : [ 0.11924634873867035, -0.20521655678749084, 0.1025511771440506, 0.23812435567378998 ],
			"coeffs_68" : [ -0.1345164179801941, 0.11347626149654388, -0.11232398450374603, -0.14369098842144012 ],
			"coeffs_69" : [ -0.016129475086927414, -0.17328476905822754, -0.02067716047167778, -0.14164705574512482 ],
			"coeffs_70" : [ 0.032128676772117615, -0.022514918819069862, 0.22216388583183289, -0.05419640988111496 ],
			"coeffs_71" : [ 0.11714284121990204, 0.11889861524105072, -0.11569992452859879, 0.13611699640750885 ],
			"coeffs_72" : [ 0.06351935863494873, -0.14797942340373993, -0.10186091810464859, -0.1386115550994873 ],
			"coeffs_73" : [ 0.11023898422718048, -0.21573364734649658, -1.7419690266251564e-05, -0.11102555692195892 ],
			"coeffs_74" : [ -0.1676318347454071, 0.15648074448108673, -0.004223720170557499, -0.1197533831000328 ],
			"coeffs_75" : [ 0.18034040927886963, -0.16101811826229095, 0.09364169090986252, -0.23865248262882233 ],
			"coeffs_76" : [ 0.06641851365566254, 0.09728989750146866, -0.024580931290984154, 0.028567437082529068 ],
			"coeffs_77" : [ -0.014920073561370373, -0.11824242770671844, -0.15924707055091858, -0.21589811146259308 ],
			"coeffs_78" : [ -0.026547279208898544, 0.03214677795767784, 0.05548693612217903, -0.1485317349433899 ],
			"coeffs_79" : [ 0.006654631346464157, -0.1665404736995697, 0.007280796300619841, -0.17141640186309814 ],
			"coeffs_80" : [ -0.16596868634223938, 0.21104183793067932, 0.1400885432958603, 0.1295379102230072 ],
			"coeffs_81" : [ 0.11969154328107834, -0.1006055474281311, -0.1257505863904953, 0.19670526683330536 ],
			"coeffs_82" : [ -0.12078491598367691, -0.14770719408988953, 0.07506833970546722, 0.2530288100242615 ],
			"coeffs_83" : [ 0.0036433618515729904, 0.1801038235425949, 0.17936451733112335, -0.10535639524459839 ],
			"coeffs_84" : [ -0.06078349053859711, -0.06441466510295868, -0.11284773051738739, -0.11109273135662079 ],
			"coeffs_85" : [ 0.2590061128139496, -0.23341214656829834, -0.07807354629039764, 0.00032054458279162645 ],
			"coeffs_86" : [ 0.1978420466184616, -0.11214753985404968, 0.1668558567762375, 0.07680415362119675 ],
			"coeffs_87" : [ -0.20088113844394684, -0.11859466880559921, -0.1553572565317154, -0.14230696856975555 ],
			"coeffs_88" : [ -0.08982375264167786, -0.03201248496770859, 0.11422830820083618, 0.0936415046453476 ],
			"coeffs_89" : [ -0.05534583702683449, -0.25838378071784973, -0.10760752111673355, -0.15916787087917328 ],
			"coeffs_90" : [ -0.053838495165109634, -0.15832769870758057, 0.013340320438146591, -0.15383204817771912 ],
			"coeffs_91" : [ 0.16199178993701935, -0.053843822330236435, -0.22600239515304565, -0.18207548558712006 ],
			"coeffs_92" : [ -0.027185071259737015, -0.06691566109657288, -0.08424117416143417, 0.03250045329332352 ],
			"coeffs_93" : [ -0.11644598841667175, -0.1287430077791214, -0.13823285698890686, 0.010524475015699863 ],
			"coeffs_94" : [ -0.23540504276752472, -0.08971431851387024, 0.1866386979818344, -0.17998994886875153 ],
			"coeffs_95" : [ -0.10235938429832458, 0.0960826724767685, 0.08333838731050491, -0.08440731465816498 ],
			"coeffs_96" : [ 0.05770910531282425, 0.153391495347023, 0.14446385204792023, -0.11944156885147095 ],
			"coeffs_97" : [ 0.1536157876253128, 0.2064799964427948, -0.09562229365110397, -0.22189036011695862 ],
			"coeffs_98" : [ 0.02524198591709137, -0.14764109253883362, -0.1417098045349121, 0.08797036111354828 ],
			"coeffs_99" : [ -0.20663732290267944, -0.1962328404188156, 0.17394021153450012, 0.19006817042827606 ],
			"intercepts" : [ 0.19082070887088776, -0.11133299022912979, 0.20338182151317596, -0.11613728851079941 ],
			"name" : "Hidden_Layer_1"
		},
		"Layer_2" : 	{
			"NbInputs" : 4,
			"NbOutputs" : 8,
			"coeffs_0" : [ -0.1286308616399765, -0.6135954856872559, -0.400738000869751, 0.6013012528419495, 0.36534351110458374, -0.18565580248832703, 0.49334481358528137, -0.6356704831123352 ],
			"coeffs_1" : [ -0.6661893725395203, 0.061779558658599854, 0.4489153325557709, 0.15376681089401245, 0.5379357933998108, -0.12467391043901443, -0.016195863485336304, -0.3834795653820038 ],
			"coeffs_2" : [ 0.41832199692726135, 0.34116968512535095, 0.6644743084907532, -0.25772568583488464, -0.3207281827926636, -0.050613902509212494, 0.2701481282711029, -0.5207337737083435 ],
			"coeffs_3" : [ 0.011547310277819633, 0.3223654329776764, -0.5951177477836609, -0.2708601653575897, -0.3371856212615967, 0.6795210838317871, -0.0073534781113266945, -0.4563060402870178 ],
			"intercepts" : [ 0.5277714729309082, -0.5042417645454407, 0.1499699503183365, 0.7381629943847656, -0.6396352648735046, -0.0012190781999379396, 0.43670323491096497, 0.6256707906723022 ],
			"name" : "Hidden_Layer_2"
		},
		"Layer_3" : 	{
			"NbInputs" : 8,
			"NbOutputs" : 6,
			"coeffs_0" : [ -0.06466735154390335, -0.3689579963684082, -0.3761264979839325, -0.26590660214424133, 0.3574067950248718, 0.5925407409667969 ],
			"coeffs_1" : [ 0.32614701986312866, -0.3816532492637634, -0.4658799171447754, 0.4563256800174713, -0.2810284495353699, 0.2733156085014343 ],
			"coeffs_2" : [ 0.1927570104598999, -0.03839173913002014, -0.2955440878868103, 0.37150678038597107, -0.5613501071929932, 0.2768997251987457 ],
			"coeffs_3" : [ 0.6076924800872803, 0.24879670143127441, 0.20904949307441711, 0.264594167470932, 0.3162061870098114, -0.2583678662776947 ],
			"coeffs_4" : [ 0.10412389039993286, 0.1953975409269333, 0.3847540020942688, -0.26174604892730713, 0.2018098384141922, -0.5306535363197327 ],
			"coeffs_5" : [ -0.35795432329177856, 0.007198826409876347, 0.5556963682174683, -0.04519103094935417, -0.21149486303329468, -0.45157623291015625 ],
			"coeffs_6" : [ -0.5057291388511658, 0.20377635955810547, -0.6484406590461731, 0.47810325026512146, 0.434205025434494, 0.3628600239753723 ],
			"coeffs_7" : [ 0.0053563048131763935, -0.3887906074523926, 0.6053612232208252, 0.6291233897209167, 0.08857903629541397, -0.010019696317613125 ],
			"intercepts" : [ 0.6752883791923523, -0.3538983166217804, -0.47332632541656494, -0.18598613142967224, 0.5124585032463074, -0.5860925912857056 ],
			"name" : "Hidden_Layer_3"
		},
		"Layer_4" : 	{
			"NbInputs" : 6,
			"NbOutputs" : 1,
			"coeffs_0" : [ 0.7715550661087036 ],
			"coeffs_1" : [ -0.19369667768478394 ],
			"coeffs_2" : [ 0.6322245001792908 ],
			"coeffs_3" : [ 0.8055725693702698 ],
			"coeffs_4" : [ 0.7103548049926758 ],
			"coeffs_5" : [ -0.5374941229820251 ],
			"intercepts" : [ -0.2628578245639801 ],
			"name" : "Output_Layer"
		},
		"sizes" : [ 100, 4, 8, 6, 1 ]
	},
	"metadata" :  { "model" : "sklearn.neural_network._multilayer_perceptron.MLPRegressor", "version" : "1.4.1.post1" },
	"options" :  { "activation" : "relu", "alpha" : 0.0001, "batch_size" : "auto", "beta_1" : 0.9, "beta_2" : 0.999, "early_stopping" : false, "epsilon" : 1e-08, "hidden_layer_sizes" : [ 4, 8, 6 ], "learning_rate" : "constant", "learning_rate_init" : 0.001, "max_fun" : 15000, "max_iter" : 32, "momentum" : 0.9, "n_iter_no_change" : 10, "nesterovs_momentum" : true, "power_t" : 0.5, "random_state" : 1789, "shuffle" : false, "solver" : "adam", "tol" : 0.0001, "validation_fraction" : 0.1, "verbose" : false, "warm_start" : false }
}
BEAUTIFIED_JSON_END
('OPERATION_START', 'PREDICT')
[1.4049885  0.6953074  1.1306022  1.5868205  0.06759387 1.5635473
 0.1335372  3.0046315  1.6351429  1.85928    1.8575925  2.277598
 1.2734392  1.6607646  1.6403562  1.2167037  1.6143366  0.6179974
 0.442552   1.4508193  1.3020734  1.2236811  0.44043407 0.21541765
 1.570568   1.272099   0.36714777 3.5799165  0.5683762  1.2493657
 1.910961   2.561193   1.4627224  1.8583385  0.49794325 2.9236712
 2.4337735  1.5801034  0.37687716 1.6116523  1.4240576  1.9625026
 0.08102074 1.2454714  0.5447111  0.8213736  1.7261909  2.1785913
 1.353512   1.2181846  0.22614703 1.5531105  1.4901519  0.55665374
 1.827716   1.5379304  2.3502197  1.3620008  0.83225596 1.6351429
 1.5953193  1.3642154  2.1566887  3.7237554 ]
('OPERATION_END_ELAPSED', 0.001, 'PREDICT')
MODEL_PERFS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'RandomReg_100_small', 'size': 64, 'mse': 29387.205, 'mae': 135.76074, 'mape': 1.0007143, 'r2': 0.0025339729543431}
WRITING_PERF_CODE 'logs/auto_tests/regression/MLPRegressor/sklearn.neural_network._multilayer_perceptron.MLPRegressor_RandomReg_100_small_option_1.perf'

MODEL_PERFS_TIMINGS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'RandomReg_100_small', 'training_time_in_sec': 0.036, 'prediction_time_in_sec': 0.001}
