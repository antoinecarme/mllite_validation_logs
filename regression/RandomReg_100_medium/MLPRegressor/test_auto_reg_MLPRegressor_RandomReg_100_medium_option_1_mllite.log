          X_0       X_1       X_2  ...      X_98      X_99      target
0    1.331256 -0.890219  0.415263  ... -1.017662  0.850023   46.659290
1    0.101475  1.673349  0.646130  ... -0.082562 -0.235254   17.096290
2    1.143388 -0.121749  1.763748  ... -1.210605  0.186911 -214.157384
3   -0.664377  0.702461 -2.030148  ... -1.509694  1.665004  -45.132339
4   -0.183233  0.167641 -1.018050  ... -0.518277 -0.467736 -472.368048
..        ...       ...       ...  ...       ...       ...         ...
507 -0.085407  0.635354 -0.165095  ... -0.632373 -2.073853   44.160388
508  0.689048 -0.025340 -0.142676  ... -0.697890  0.794571   89.670862
509 -0.929090 -0.892634  1.709303  ... -2.478970 -0.503891    7.769367
510  1.078776 -0.630185  0.944048  ... -0.233485 -0.721341  -26.929418
511  0.057106  0.428869 -0.742754  ... -0.139843 -0.534295 -333.558668

[512 rows x 101 columns]
MLLITE_MODEL_SET_OPTIONS MLPRegressor {"hidden_layer_sizes" : [4, 8, 6]}
MLLITE_MODEL_OPTIONS_DEFAULT MLPRegressor { "max_iter" : 32}
MLLITE_MODEL_OPTIONS_AFTER_SETTING MLPRegressor { "hidden_layer_sizes" : [4, 8, 6],  "max_iter" : 32 , }
MLLITE_MODEL_JSON_AFTER_SETTING MLPRegressor None
('OPERATION_START', 'TRAINING')
[[ 1.3312556  -0.8902193   0.41526318  0.788659   -0.7130887   1.4502782
  -0.90123427  0.7631844   0.13805741  1.6139201  -0.38690123  0.03777119
  -1.1557323  -0.5858095   1.6626205   0.68594664  0.39176562 -0.2828408
   0.76795787  0.3757667  -2.7420852  -0.01939904  0.40063587  0.17057581
  -0.44771507 -1.2317778  -0.8443254   0.7879428  -0.58345294  1.3596091
   0.6738356   1.2624362  -0.65831184 -0.88525283  0.04143903 -1.1881486
   0.83240134  0.49906567  0.11895992 -0.8198342   0.57035816 -1.4580479
  -0.7348064  -0.4322117   1.670222    0.49406192  0.47490185  0.86712915
   0.06314228 -2.2807853   0.7397412   0.7191424  -0.25634202 -0.75440276
   0.6664157   1.004504   -0.7802194   1.2289945  -0.3151011  -0.52080745
   2.0017505  -0.65958494 -0.06995643  1.3200244  -2.0877314  -2.0024903
  -0.02926418  1.5691092   0.85446763  0.5201753  -0.8084577   1.7062956
   0.8540257  -0.4603799  -0.06352598 -0.8714725  -0.5609252  -0.3273548
   0.74484634 -1.6906458  -0.33099878  0.06832191  1.3731502   0.84443164
   0.45396343  0.7861103   0.26812124 -0.06078732 -1.0477233   1.4219131
   1.2072196   0.7279877  -0.68208903 -1.8471589   0.77888066 -0.9317782
   0.2455851   0.06674263 -1.017662    0.8500229 ]
 [ 0.10147522  1.6733488   0.6461298  -1.2563368   1.1040853  -1.4227151
  -0.6017069  -0.9161108   0.16968192 -1.2134422   0.2153484  -0.08051679
   1.2387321  -0.48785964  0.27010813  1.5061171  -1.2551115  -1.1685889
   0.85681057 -0.3032395   0.34517288 -2.404879   -0.5924876  -0.11890611
  -1.1995318  -2.3414571   0.2762635  -4.5431676  -0.53269297  0.8078485
  -1.5695755  -1.10812     0.8487407  -0.2972983   0.58910424  0.82803386
   1.6629032  -0.71414495  1.0407996   0.00796571 -0.5988979  -1.67421
  -1.17802    -0.22742864  2.2339568   0.3043066   0.37049994 -1.5326284
  -0.64369285  0.96310157 -0.85642195  1.1327137  -1.4508079  -0.13960138
   1.4753928  -0.45546088  0.90462786  1.6403875   0.37488905  1.2000089
   1.3891938  -0.12287517 -0.7553856   0.8262001  -0.0514452   1.0170072
  -0.67795897  0.09011087  0.2844104   0.6818215  -0.8403749  -0.68216765
  -0.05586801  0.47741953 -0.56251854 -0.308212    0.13523772 -0.60912365
   0.6620477   0.10418119 -0.9861369   0.09003809 -2.009931   -0.989154
   0.5694446  -0.7199399  -1.0182965  -0.67762125  1.3876666  -1.4770343
   1.3187295   0.10140626 -0.21362238 -2.1321723  -0.9681868  -1.3000042
  -0.23745911 -0.96780366 -0.08256175 -0.2352544 ]
 [ 1.1433884  -0.12174941  1.763748    0.1567726  -1.074581    0.3090569
  -0.59582543  0.75060105  1.2144792   1.2156659   0.5765553  -0.4355099
   0.2547612  -0.9373168   1.2934183  -0.13537018  0.74200237  0.43883675
  -0.6069965   0.1554797   0.5401852   1.1975131  -1.1406852  -0.20755301
  -0.31834334 -0.6807571   0.24679533  1.5937762   0.1117221   0.80259067
  -1.2313136   0.917576    0.50131375 -0.5743615  -0.17214447 -2.0382028
   0.39552817  2.1280468   1.0293733  -0.02239049 -0.67557096 -1.5386295
   1.4090686  -0.59869367  0.6779711   0.37315732 -0.3413207  -0.21079636
   0.34230128  1.0776794   0.30119348 -0.24192244  0.23708598 -2.070677
  -0.48128286  0.3670155   1.0218917  -0.82419974  0.02379109  0.3317229
   1.5780679  -1.4464464  -1.6030818  -1.4349504  -0.11372316 -0.05236021
   0.37578964  1.7915944  -0.87417036  0.19483742 -0.11213797  0.07997598
  -0.89980906 -0.26016855  0.50538015 -1.2370168   1.1031808  -0.8198793
  -1.211      -0.5554547  -0.3449861   0.52403086 -0.2534899   1.0931107
   0.6548259   0.05409036 -0.2681419  -0.06177664  0.09527746  0.8096554
  -0.2925497   0.3124124  -0.37333974  1.0691838  -1.405716   -2.0550418
  -0.42661822 -0.00682518 -1.2106051   0.18691105]
 [-0.6643772   0.70246065 -2.0301476  -2.0342605   0.87254155  0.8668411
  -0.26858473 -0.73312426  1.0275607  -0.7275404   2.1678174  -1.1313968
   0.40187567 -0.71089333  0.2914241  -0.7301117  -1.095154   -2.268191
  -0.4189619  -0.03287303 -1.6552861   0.06898359 -1.912857   -0.55441886
   0.5538053   1.8425944   0.4861901   0.15487543 -0.10841709 -1.1439068
   0.09484072  0.54181457  0.55301255 -0.16916783 -1.6026491   0.4181876
   1.2507617   0.34678614 -1.1298385   2.87664     0.13642831 -0.22074103
  -1.260425    0.59103775  0.71680933 -1.1945326   0.18375565  0.6792565
  -1.1802012   1.2635881   0.08254713 -1.2404718   0.6709144  -0.07065742
  -0.2673846   0.35829198 -0.9844122  -0.8540105   0.54115516  0.78829515
   0.6353548  -0.6730554  -0.280788   -1.2170262   0.28105965 -1.9465499
   0.2613469  -0.85210556 -0.43554437 -0.4999505   1.1504256  -0.28339145
   0.96043223  0.3302896   0.04703231  0.7185348  -0.7703047  -1.8150443
  -0.29500493  0.47207424  1.062915   -0.8516409   0.34324142 -0.0487177
   0.5544652   0.7159857   0.26749176 -0.28686863 -0.10642244  2.2316606
  -1.3776829  -0.18294522 -1.7351075  -0.15755087  1.2045766   0.8150365
  -0.62745374  1.0366299  -1.5096936   1.665004  ]
 [-0.18323281  0.16764143 -1.0180501  -1.1114113  -0.6096793  -1.203273
  -0.91351366 -1.728868    1.45258    -0.7503452  -0.9816971   2.4864185
   0.2919757   0.7608395   0.02673802  0.01965859  1.7714319   0.10731484
   1.5014844   0.04669442  0.01499307  2.2903414  -1.5327556   0.2251275
   0.13013346 -1.3958306   0.477327   -2.448051   -1.034129   -0.36448112
  -0.27540234 -0.0670136  -0.41763452  1.1593167   0.36466834  0.51342684
  -1.9805648  -1.26423     0.1809458   0.09143253 -0.7548743   0.2509495
  -0.38412192 -1.5486463  -0.01736598  1.3488103  -2.0970352   0.540929
  -0.69701564  0.10016023 -0.34506392  0.5776158   2.3233109  -1.0692806
  -0.67232996 -0.8802096   0.67767256 -0.98172724  1.5671804  -1.077151
   0.6932878  -0.29901332  0.7109348  -0.6178621   0.13290192 -1.3579625
   0.8258206   0.2063345   0.6937211  -0.23165497  1.6014408  -0.33389077
  -0.52463293  1.2932607   0.24274994 -0.84197    -1.8947318  -0.31777015
   0.011854    1.0398142   1.1872678   0.5696282   0.77042353  0.04953871
  -1.2782155  -0.57868254  0.8079945   1.9365364   1.0325052   0.6288556
  -1.8749372   0.49370095  0.64584726  0.73780465  0.23192818 -1.2330554
   0.8333075   0.13102144 -0.5182774  -0.46773615]] [  46.65929   17.09629 -214.15738  -45.13234 -472.36804]
MLLITE_FIT_USING_MODEL mllite_mlp_reg.MLPRegressor_ff4_ff4
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT' 1 3
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT' 2 3
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 3 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 6 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 9 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 12 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 15 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 18 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 21 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 24 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 27 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 30 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT_BACK_PROP_ITERATION' 32 32
PROGRESS_REPORT_ELAPSED_TOTAL 'MLP_MODEL_FIT' 3 3
('OPERATION_END_ELAPSED', 0.104, 'TRAINING')
{ "hidden_layer_sizes" : [4, 8, 6],  "max_iter" : 32 , }
{ "hidden_layer_sizes" : [4, 8, 6],  "max_iter" : 32 , }
{
	"metadata" : { "model" : "Rosenblatt_MLP", "version" : "2024-W13", "signature" : "ff4_ff4"},
	"options" : {  "hidden_layer_sizes" : [ 4, 8, 6 ], "activation" : "relu", "solver" : "adam", "alpha" : 0.000100, "batch_size" : null, "learning_rate" : "constant", "learning_rate_init" : 0.001000, "power_t" : 0.500000, "max_iter" : 32, "shuffle" : 1, "random_state" : 1789, "tol" : 0.000100, "verbose" : 0, "warm_start" : 0, "momentum" : 0.900000, "nesterovs_momentum" : 1, "early_stopping" : 0, "validation_fraction" : 0.100000, "beta_1" : 0.900000, "beta_2" : 0.999000, "epsilon" : 0.000000, "n_iter_no_change" : 10, "max_fun" : 15000 },
	"dataset" : { "dataset_rows" : 512, "dataset_features" : 100 },
	"layers" : {
		"sizes" : [ 100, 4, 8, 6, 1 ],
		"Layer_0" : {
			"name" : "Input_Layer",
			"NbInputs" : 0,
			"NbOutputs" : 100 
		},
		"Layer_1" : {
			"name" : "Hidden_Layer_1",
			"NbInputs" : 100,
			"NbOutputs" : 4 ,
			"coeffs_00" : [ 0.074652, 0.064476, -0.092252, -0.036509 ],
			"coeffs_01" : [ 0.105610, -0.216481, 0.123136, 0.013785 ],
			"coeffs_02" : [ 0.102108, 0.021546, 0.106328, 0.050539 ],
			"coeffs_03" : [ 0.311378, -0.267084, -0.190324, 0.029166 ],
			"coeffs_04" : [ 0.138274, -0.115451, -0.140077, -0.181674 ],
			"coeffs_05" : [ 0.087345, -0.183635, -0.080666, -0.014058 ],
			"coeffs_06" : [ -0.143272, -0.239325, -0.040754, 0.250623 ],
			"coeffs_07" : [ -0.022732, 0.066196, -0.316204, -0.010908 ],
			"coeffs_08" : [ -0.074583, -0.000356, -0.059482, 0.103736 ],
			"coeffs_09" : [ -0.236491, 0.149116, -0.228560, 0.132025 ],
			"coeffs_10" : [ -0.142815, 0.195092, 0.118665, 0.155456 ],
			"coeffs_11" : [ -0.186328, -0.116449, 0.101921, 0.060023 ],
			"coeffs_12" : [ 0.230070, 0.175697, 0.110402, -0.091816 ],
			"coeffs_13" : [ 0.207100, 0.203715, -0.191845, 0.044751 ],
			"coeffs_14" : [ 0.154776, 0.001515, 0.082485, 0.069023 ],
			"coeffs_15" : [ -0.295819, 0.079204, 0.040724, 0.163873 ],
			"coeffs_16" : [ -0.204030, 0.231835, -0.126212, -0.171418 ],
			"coeffs_17" : [ 0.195807, 0.193229, -0.010255, -0.201466 ],
			"coeffs_18" : [ -0.075384, 0.267300, -0.116736, 0.128434 ],
			"coeffs_19" : [ 0.039131, 0.166118, 0.205066, -0.164048 ],
			"coeffs_20" : [ 0.068914, 0.184791, 0.140904, 0.047009 ],
			"coeffs_21" : [ -0.256738, 0.145080, 0.035879, -0.037218 ],
			"coeffs_22" : [ -0.284093, -0.123145, -0.150655, 0.257037 ],
			"coeffs_23" : [ -0.061516, -0.134511, -0.076451, -0.214398 ],
			"coeffs_24" : [ 0.130088, -0.078163, -0.049463, -0.094016 ],
			"coeffs_25" : [ 0.015450, -0.123933, -0.064898, 0.121920 ],
			"coeffs_26" : [ 0.050657, 0.287736, 0.165309, 0.165223 ],
			"coeffs_27" : [ -0.092509, 0.078041, 0.150565, 0.115261 ],
			"coeffs_28" : [ 0.120853, -0.018335, 0.139181, 0.153540 ],
			"coeffs_29" : [ 0.131829, -0.065462, -0.095617, -0.103307 ],
			"coeffs_30" : [ 0.019922, 0.237758, -0.256723, 0.151194 ],
			"coeffs_31" : [ -0.104386, 0.069819, -0.008840, -0.150150 ],
			"coeffs_32" : [ 0.001018, -0.142761, -0.268709, 0.240441 ],
			"coeffs_33" : [ 0.113236, -0.104585, -0.141993, -0.107000 ],
			"coeffs_34" : [ -0.258801, -0.091881, -0.059979, -0.023522 ],
			"coeffs_35" : [ -0.148439, -0.175015, -0.163500, 0.017022 ],
			"coeffs_36" : [ -0.265726, -0.097001, 0.016083, 0.066348 ],
			"coeffs_37" : [ -0.029682, -0.040742, 0.094178, -0.168623 ],
			"coeffs_38" : [ -0.025222, 0.138239, 0.231274, -0.005562 ],
			"coeffs_39" : [ -0.270399, -0.031887, 0.028386, -0.107510 ],
			"coeffs_40" : [ 0.051034, -0.154586, 0.221770, 0.184844 ],
			"coeffs_41" : [ -0.074065, 0.157550, -0.132250, 0.242348 ],
			"coeffs_42" : [ -0.283891, 0.192961, 0.110498, -0.138920 ],
			"coeffs_43" : [ 0.017549, 0.066535, 0.057403, -0.108456 ],
			"coeffs_44" : [ 0.149341, 0.156510, -0.131272, -0.036975 ],
			"coeffs_45" : [ 0.080375, 0.002422, 0.000873, 0.289339 ],
			"coeffs_46" : [ -0.021062, 0.206412, -0.261533, -0.027849 ],
			"coeffs_47" : [ 0.120735, 0.111307, -0.098424, -0.027622 ],
			"coeffs_48" : [ 0.224690, -0.118949, -0.009679, 0.202087 ],
			"coeffs_49" : [ -0.307597, -0.096818, -0.145052, -0.054237 ],
			"coeffs_50" : [ -0.058708, 0.009019, -0.214735, 0.062373 ],
			"coeffs_51" : [ 0.289716, -0.038393, -0.038633, 0.035707 ],
			"coeffs_52" : [ 0.133781, 0.074152, 0.009447, 0.146943 ],
			"coeffs_53" : [ 0.044896, -0.039947, -0.004951, -0.188238 ],
			"coeffs_54" : [ -0.222990, -0.055603, 0.086181, -0.013246 ],
			"coeffs_55" : [ -0.044483, -0.211837, 0.180523, -0.139798 ],
			"coeffs_56" : [ 0.030050, 0.052518, 0.061671, 0.048869 ],
			"coeffs_57" : [ -0.117082, 0.112437, 0.167637, 0.095974 ],
			"coeffs_58" : [ -0.047453, 0.302001, 0.185343, 0.131241 ],
			"coeffs_59" : [ -0.178850, 0.292974, -0.341014, -0.205932 ],
			"coeffs_60" : [ 0.048958, -0.040240, 0.103842, 0.105429 ],
			"coeffs_61" : [ 0.134756, -0.066965, -0.149757, -0.123341 ],
			"coeffs_62" : [ 0.135374, 0.073523, -0.175887, -0.053189 ],
			"coeffs_63" : [ 0.197467, -0.147256, 0.000396, -0.031357 ],
			"coeffs_64" : [ 0.100503, -0.043224, 0.063438, 0.158291 ],
			"coeffs_65" : [ -0.015256, 0.071973, 0.234728, 0.186406 ],
			"coeffs_66" : [ 0.146557, -0.215573, -0.032971, -0.200127 ],
			"coeffs_67" : [ 0.077890, -0.010084, 0.145922, 0.187177 ],
			"coeffs_68" : [ 0.178020, -0.088086, 0.100934, 0.093760 ],
			"coeffs_69" : [ 0.134266, 0.165798, -0.127456, 0.214310 ],
			"coeffs_70" : [ 0.150786, 0.235712, -0.062250, 0.060202 ],
			"coeffs_71" : [ 0.146165, 0.030796, 0.132675, -0.252044 ],
			"coeffs_72" : [ 0.063341, 0.198162, 0.108862, 0.233114 ],
			"coeffs_73" : [ -0.189081, 0.173726, -0.118848, 0.081630 ],
			"coeffs_74" : [ -0.244161, 0.119648, 0.062326, -0.019331 ],
			"coeffs_75" : [ 0.120268, -0.188416, -0.201815, -0.068148 ],
			"coeffs_76" : [ -0.214060, 0.101883, -0.016226, 0.019865 ],
			"coeffs_77" : [ -0.163112, 0.255112, 0.040114, -0.045976 ],
			"coeffs_78" : [ 0.098694, -0.127981, 0.142090, 0.046760 ],
			"coeffs_79" : [ 0.083326, -0.210711, -0.165272, 0.031248 ],
			"coeffs_80" : [ -0.058399, 0.238643, -0.196851, -0.164604 ],
			"coeffs_81" : [ 0.125903, 0.046367, 0.195467, 0.169848 ],
			"coeffs_82" : [ -0.210841, -0.110131, -0.191862, 0.061846 ],
			"coeffs_83" : [ 0.010094, 0.167837, -0.049787, -0.009504 ],
			"coeffs_84" : [ 0.333128, -0.268852, -0.010123, -0.133946 ],
			"coeffs_85" : [ -0.007847, 0.005912, -0.050291, -0.079454 ],
			"coeffs_86" : [ -0.059059, 0.053741, 0.117477, -0.189213 ],
			"coeffs_87" : [ -0.037844, -0.064580, 0.292585, 0.018855 ],
			"coeffs_88" : [ -0.213049, -0.220569, -0.145965, 0.088511 ],
			"coeffs_89" : [ -0.035499, -0.136580, -0.129602, -0.024174 ],
			"coeffs_90" : [ -0.286534, -0.041185, 0.097069, -0.121450 ],
			"coeffs_91" : [ 0.315372, -0.111008, 0.043620, -0.182070 ],
			"coeffs_92" : [ 0.162238, -0.150160, -0.078061, -0.179276 ],
			"coeffs_93" : [ -0.230829, 0.144062, -0.180562, 0.219356 ],
			"coeffs_94" : [ -0.273999, 0.005930, 0.127382, 0.036143 ],
			"coeffs_95" : [ -0.145576, -0.000392, 0.100015, -0.068494 ],
			"coeffs_96" : [ -0.252850, -0.104429, 0.039624, 0.242551 ],
			"coeffs_97" : [ -0.081516, 0.044762, 0.131536, 0.118360 ],
			"coeffs_98" : [ -0.186390, 0.112045, -0.164326, 0.119009 ],
			"coeffs_99" : [ -0.218543, -0.014716, 0.231745, -0.173017 ],
			"intercepts" : [ -0.017874, 0.090907, 0.105334, 0.193145 ]
		},
		"Layer_2" : {
			"name" : "Hidden_Layer_2",
			"NbInputs" : 4,
			"NbOutputs" : 8 ,
			"coeffs_0" : [ -0.331280, 0.046097, 0.811087, 0.703846, 0.512195, -0.201322, -0.336610, -0.265199 ],
			"coeffs_1" : [ 0.153989, -0.604201, -0.319473, -0.552422, -0.212213, -0.300222, 0.087404, -0.152977 ],
			"coeffs_2" : [ 0.728207, 0.509332, -0.265833, 0.120303, 0.425989, -0.632764, -0.314250, 0.090953 ],
			"coeffs_3" : [ -0.362170, -0.013230, -0.689326, -0.062566, -0.214084, 0.280723, -0.074905, 0.504726 ],
			"intercepts" : [ -0.048210, -0.621718, 0.603805, -0.525778, -0.011712, -0.058220, 0.375335, -0.689780 ]
		},
		"Layer_3" : {
			"name" : "Hidden_Layer_3",
			"NbInputs" : 8,
			"NbOutputs" : 6 ,
			"coeffs_0" : [ -0.141912, 0.515112, 0.426891, -0.361549, 0.575399, 0.049486 ],
			"coeffs_1" : [ 0.663677, 0.506011, -0.332814, 0.461518, 0.617647, 0.455842 ],
			"coeffs_2" : [ 0.537087, 0.085966, 0.151818, -0.143469, 0.038973, -0.054153 ],
			"coeffs_3" : [ 0.196481, -0.215494, -0.413525, -0.004945, 0.751044, -0.290411 ],
			"coeffs_4" : [ -0.193794, -0.485722, -0.295285, -0.387067, -0.004740, -0.275788 ],
			"coeffs_5" : [ 0.505710, 0.037936, 0.208026, 0.068700, 0.047888, 0.022317 ],
			"coeffs_6" : [ -0.364613, 0.559322, 0.740924, 0.357655, 0.111032, 0.276608 ],
			"coeffs_7" : [ -0.073557, 0.461395, 0.266721, -0.096627, -0.147877, -0.631406 ],
			"intercepts" : [ 0.652651, -0.575520, -0.232738, 0.661436, -0.315827, 0.607363 ]
		},
		"Layer_4" : {
			"name" : "Output_Layer",
			"NbInputs" : 6,
			"NbOutputs" : 1 ,
			"coeffs_0" : [ -0.570000 ],
			"coeffs_1" : [ 0.840942 ],
			"coeffs_2" : [ 0.453743 ],
			"coeffs_3" : [ 0.861368 ],
			"coeffs_4" : [ -0.717354 ],
			"coeffs_5" : [ 0.592693 ],
			"intercepts" : [ -0.723294 ]
		}
	}
}
WRITING_JSON_CODE 'logs/auto_tests/regression/MLPRegressor/mllite.MLPRegressor_RandomReg_100_medium_option_1.json'

MLLITE_MODEL_SET_OPTIONS MLPRegressor {"hidden_layer_sizes" : [4, 8, 6]}
MLLITE_MODEL_OPTIONS_DEFAULT MLPRegressor { "max_iter" : 32}
MLLITE_MODEL_OPTIONS_AFTER_SETTING MLPRegressor { "hidden_layer_sizes" : [4, 8, 6],  "max_iter" : 32 , }
MLLITE_MODEL_JSON_AFTER_SETTING MLPRegressor None
MLLITE_RELOADING_MODEL mllite_mlp_reg.MLPRegressor_ff4_ff4
{
	"metadata" : { "model" : "Rosenblatt_MLP", "version" : "2024-W13", "signature" : "ff4_ff4"},
	"options" : {  "hidden_layer_sizes" : [ 4, 8, 6 ], "activation" : "relu", "solver" : "adam", "alpha" : 0.000100, "batch_size" : null, "learning_rate" : "constant", "learning_rate_init" : 0.001000, "power_t" : 0.500000, "max_iter" : 32, "shuffle" : 1, "random_state" : 1789, "tol" : 0.000100, "verbose" : 0, "warm_start" : 0, "momentum" : 0.900000, "nesterovs_momentum" : 1, "early_stopping" : 0, "validation_fraction" : 0.100000, "beta_1" : 0.900000, "beta_2" : 0.999000, "epsilon" : 0.000000, "n_iter_no_change" : 10, "max_fun" : 15000 },
	"dataset" : { "dataset_rows" : 512, "dataset_features" : 100 },
	"layers" : {
		"sizes" : [ 100, 4, 8, 6, 1 ],
		"Layer_0" : {
			"name" : "Input_Layer",
			"NbInputs" : 0,
			"NbOutputs" : 100 
		},
		"Layer_1" : {
			"name" : "Hidden_Layer_1",
			"NbInputs" : 100,
			"NbOutputs" : 4 ,
			"coeffs_00" : [ 0.074652, 0.064476, -0.092252, -0.036509 ],
			"coeffs_01" : [ 0.105610, -0.216481, 0.123136, 0.013785 ],
			"coeffs_02" : [ 0.102108, 0.021546, 0.106328, 0.050539 ],
			"coeffs_03" : [ 0.311378, -0.267084, -0.190324, 0.029166 ],
			"coeffs_04" : [ 0.138274, -0.115451, -0.140077, -0.181674 ],
			"coeffs_05" : [ 0.087345, -0.183635, -0.080666, -0.014058 ],
			"coeffs_06" : [ -0.143272, -0.239325, -0.040754, 0.250623 ],
			"coeffs_07" : [ -0.022732, 0.066196, -0.316204, -0.010908 ],
			"coeffs_08" : [ -0.074583, -0.000356, -0.059482, 0.103736 ],
			"coeffs_09" : [ -0.236491, 0.149116, -0.228560, 0.132025 ],
			"coeffs_10" : [ -0.142815, 0.195092, 0.118665, 0.155456 ],
			"coeffs_11" : [ -0.186328, -0.116449, 0.101921, 0.060023 ],
			"coeffs_12" : [ 0.230070, 0.175697, 0.110402, -0.091816 ],
			"coeffs_13" : [ 0.207100, 0.203715, -0.191845, 0.044751 ],
			"coeffs_14" : [ 0.154776, 0.001515, 0.082485, 0.069023 ],
			"coeffs_15" : [ -0.295819, 0.079204, 0.040724, 0.163873 ],
			"coeffs_16" : [ -0.204030, 0.231835, -0.126212, -0.171418 ],
			"coeffs_17" : [ 0.195807, 0.193229, -0.010255, -0.201466 ],
			"coeffs_18" : [ -0.075384, 0.267300, -0.116736, 0.128434 ],
			"coeffs_19" : [ 0.039131, 0.166118, 0.205066, -0.164048 ],
			"coeffs_20" : [ 0.068914, 0.184791, 0.140904, 0.047009 ],
			"coeffs_21" : [ -0.256738, 0.145080, 0.035879, -0.037218 ],
			"coeffs_22" : [ -0.284093, -0.123145, -0.150655, 0.257037 ],
			"coeffs_23" : [ -0.061516, -0.134511, -0.076451, -0.214398 ],
			"coeffs_24" : [ 0.130088, -0.078163, -0.049463, -0.094016 ],
			"coeffs_25" : [ 0.015450, -0.123933, -0.064898, 0.121920 ],
			"coeffs_26" : [ 0.050657, 0.287736, 0.165309, 0.165223 ],
			"coeffs_27" : [ -0.092509, 0.078041, 0.150565, 0.115261 ],
			"coeffs_28" : [ 0.120853, -0.018335, 0.139181, 0.153540 ],
			"coeffs_29" : [ 0.131829, -0.065462, -0.095617, -0.103307 ],
			"coeffs_30" : [ 0.019922, 0.237758, -0.256723, 0.151194 ],
			"coeffs_31" : [ -0.104386, 0.069819, -0.008840, -0.150150 ],
			"coeffs_32" : [ 0.001018, -0.142761, -0.268709, 0.240441 ],
			"coeffs_33" : [ 0.113236, -0.104585, -0.141993, -0.107000 ],
			"coeffs_34" : [ -0.258801, -0.091881, -0.059979, -0.023522 ],
			"coeffs_35" : [ -0.148439, -0.175015, -0.163500, 0.017022 ],
			"coeffs_36" : [ -0.265726, -0.097001, 0.016083, 0.066348 ],
			"coeffs_37" : [ -0.029682, -0.040742, 0.094178, -0.168623 ],
			"coeffs_38" : [ -0.025222, 0.138239, 0.231274, -0.005562 ],
			"coeffs_39" : [ -0.270399, -0.031887, 0.028386, -0.107510 ],
			"coeffs_40" : [ 0.051034, -0.154586, 0.221770, 0.184844 ],
			"coeffs_41" : [ -0.074065, 0.157550, -0.132250, 0.242348 ],
			"coeffs_42" : [ -0.283891, 0.192961, 0.110498, -0.138920 ],
			"coeffs_43" : [ 0.017549, 0.066535, 0.057403, -0.108456 ],
			"coeffs_44" : [ 0.149341, 0.156510, -0.131272, -0.036975 ],
			"coeffs_45" : [ 0.080375, 0.002422, 0.000873, 0.289339 ],
			"coeffs_46" : [ -0.021062, 0.206412, -0.261533, -0.027849 ],
			"coeffs_47" : [ 0.120735, 0.111307, -0.098424, -0.027622 ],
			"coeffs_48" : [ 0.224690, -0.118949, -0.009679, 0.202087 ],
			"coeffs_49" : [ -0.307597, -0.096818, -0.145052, -0.054237 ],
			"coeffs_50" : [ -0.058708, 0.009019, -0.214735, 0.062373 ],
			"coeffs_51" : [ 0.289716, -0.038393, -0.038633, 0.035707 ],
			"coeffs_52" : [ 0.133781, 0.074152, 0.009447, 0.146943 ],
			"coeffs_53" : [ 0.044896, -0.039947, -0.004951, -0.188238 ],
			"coeffs_54" : [ -0.222990, -0.055603, 0.086181, -0.013246 ],
			"coeffs_55" : [ -0.044483, -0.211837, 0.180523, -0.139798 ],
			"coeffs_56" : [ 0.030050, 0.052518, 0.061671, 0.048869 ],
			"coeffs_57" : [ -0.117082, 0.112437, 0.167637, 0.095974 ],
			"coeffs_58" : [ -0.047453, 0.302001, 0.185343, 0.131241 ],
			"coeffs_59" : [ -0.178850, 0.292974, -0.341014, -0.205932 ],
			"coeffs_60" : [ 0.048958, -0.040240, 0.103842, 0.105429 ],
			"coeffs_61" : [ 0.134756, -0.066965, -0.149757, -0.123341 ],
			"coeffs_62" : [ 0.135374, 0.073523, -0.175887, -0.053189 ],
			"coeffs_63" : [ 0.197467, -0.147256, 0.000396, -0.031357 ],
			"coeffs_64" : [ 0.100503, -0.043224, 0.063438, 0.158291 ],
			"coeffs_65" : [ -0.015256, 0.071973, 0.234728, 0.186406 ],
			"coeffs_66" : [ 0.146557, -0.215573, -0.032971, -0.200127 ],
			"coeffs_67" : [ 0.077890, -0.010084, 0.145922, 0.187177 ],
			"coeffs_68" : [ 0.178020, -0.088086, 0.100934, 0.093760 ],
			"coeffs_69" : [ 0.134266, 0.165798, -0.127456, 0.214310 ],
			"coeffs_70" : [ 0.150786, 0.235712, -0.062250, 0.060202 ],
			"coeffs_71" : [ 0.146165, 0.030796, 0.132675, -0.252044 ],
			"coeffs_72" : [ 0.063341, 0.198162, 0.108862, 0.233114 ],
			"coeffs_73" : [ -0.189081, 0.173726, -0.118848, 0.081630 ],
			"coeffs_74" : [ -0.244161, 0.119648, 0.062326, -0.019331 ],
			"coeffs_75" : [ 0.120268, -0.188416, -0.201815, -0.068148 ],
			"coeffs_76" : [ -0.214060, 0.101883, -0.016226, 0.019865 ],
			"coeffs_77" : [ -0.163112, 0.255112, 0.040114, -0.045976 ],
			"coeffs_78" : [ 0.098694, -0.127981, 0.142090, 0.046760 ],
			"coeffs_79" : [ 0.083326, -0.210711, -0.165272, 0.031248 ],
			"coeffs_80" : [ -0.058399, 0.238643, -0.196851, -0.164604 ],
			"coeffs_81" : [ 0.125903, 0.046367, 0.195467, 0.169848 ],
			"coeffs_82" : [ -0.210841, -0.110131, -0.191862, 0.061846 ],
			"coeffs_83" : [ 0.010094, 0.167837, -0.049787, -0.009504 ],
			"coeffs_84" : [ 0.333128, -0.268852, -0.010123, -0.133946 ],
			"coeffs_85" : [ -0.007847, 0.005912, -0.050291, -0.079454 ],
			"coeffs_86" : [ -0.059059, 0.053741, 0.117477, -0.189213 ],
			"coeffs_87" : [ -0.037844, -0.064580, 0.292585, 0.018855 ],
			"coeffs_88" : [ -0.213049, -0.220569, -0.145965, 0.088511 ],
			"coeffs_89" : [ -0.035499, -0.136580, -0.129602, -0.024174 ],
			"coeffs_90" : [ -0.286534, -0.041185, 0.097069, -0.121450 ],
			"coeffs_91" : [ 0.315372, -0.111008, 0.043620, -0.182070 ],
			"coeffs_92" : [ 0.162238, -0.150160, -0.078061, -0.179276 ],
			"coeffs_93" : [ -0.230829, 0.144062, -0.180562, 0.219356 ],
			"coeffs_94" : [ -0.273999, 0.005930, 0.127382, 0.036143 ],
			"coeffs_95" : [ -0.145576, -0.000392, 0.100015, -0.068494 ],
			"coeffs_96" : [ -0.252850, -0.104429, 0.039624, 0.242551 ],
			"coeffs_97" : [ -0.081516, 0.044762, 0.131536, 0.118360 ],
			"coeffs_98" : [ -0.186390, 0.112045, -0.164326, 0.119009 ],
			"coeffs_99" : [ -0.218543, -0.014716, 0.231745, -0.173017 ],
			"intercepts" : [ -0.017874, 0.090907, 0.105334, 0.193145 ]
		},
		"Layer_2" : {
			"name" : "Hidden_Layer_2",
			"NbInputs" : 4,
			"NbOutputs" : 8 ,
			"coeffs_0" : [ -0.331280, 0.046097, 0.811087, 0.703846, 0.512195, -0.201322, -0.336610, -0.265199 ],
			"coeffs_1" : [ 0.153989, -0.604201, -0.319473, -0.552422, -0.212213, -0.300222, 0.087404, -0.152977 ],
			"coeffs_2" : [ 0.728207, 0.509332, -0.265833, 0.120303, 0.425989, -0.632764, -0.314250, 0.090953 ],
			"coeffs_3" : [ -0.362170, -0.013230, -0.689326, -0.062566, -0.214084, 0.280723, -0.074905, 0.504726 ],
			"intercepts" : [ -0.048210, -0.621718, 0.603805, -0.525778, -0.011712, -0.058220, 0.375335, -0.689780 ]
		},
		"Layer_3" : {
			"name" : "Hidden_Layer_3",
			"NbInputs" : 8,
			"NbOutputs" : 6 ,
			"coeffs_0" : [ -0.141912, 0.515112, 0.426891, -0.361549, 0.575399, 0.049486 ],
			"coeffs_1" : [ 0.663677, 0.506011, -0.332814, 0.461518, 0.617647, 0.455842 ],
			"coeffs_2" : [ 0.537087, 0.085966, 0.151818, -0.143469, 0.038973, -0.054153 ],
			"coeffs_3" : [ 0.196481, -0.215494, -0.413525, -0.004945, 0.751044, -0.290411 ],
			"coeffs_4" : [ -0.193794, -0.485722, -0.295285, -0.387067, -0.004740, -0.275788 ],
			"coeffs_5" : [ 0.505710, 0.037936, 0.208026, 0.068700, 0.047888, 0.022317 ],
			"coeffs_6" : [ -0.364613, 0.559322, 0.740924, 0.357655, 0.111032, 0.276608 ],
			"coeffs_7" : [ -0.073557, 0.461395, 0.266721, -0.096627, -0.147877, -0.631406 ],
			"intercepts" : [ 0.652651, -0.575520, -0.232738, 0.661436, -0.315827, 0.607363 ]
		},
		"Layer_4" : {
			"name" : "Output_Layer",
			"NbInputs" : 6,
			"NbOutputs" : 1 ,
			"coeffs_0" : [ -0.570000 ],
			"coeffs_1" : [ 0.840942 ],
			"coeffs_2" : [ 0.453743 ],
			"coeffs_3" : [ 0.861368 ],
			"coeffs_4" : [ -0.717354 ],
			"coeffs_5" : [ 0.592693 ],
			"intercepts" : [ -0.723294 ]
		}
	}
}
BEAUTIFIED_JSON_START
{
	"dataset" : 	{
		"dataset_features" : 100,
		"dataset_rows" : 512
	},
	"layers" : 	{
		"Layer_0" : 	{
			"NbInputs" : 0,
			"NbOutputs" : 100,
			"name" : "Input_Layer"
		},
		"Layer_1" : 	{
			"NbInputs" : 100,
			"NbOutputs" : 4,
			"coeffs_00" : [ 0.074652, 0.064476, -0.092252, -0.036509 ],
			"coeffs_01" : [ 0.10561, -0.216481, 0.123136, 0.013785 ],
			"coeffs_02" : [ 0.102108, 0.021546, 0.106328, 0.050539 ],
			"coeffs_03" : [ 0.311378, -0.267084, -0.190324, 0.029166 ],
			"coeffs_04" : [ 0.138274, -0.115451, -0.140077, -0.181674 ],
			"coeffs_05" : [ 0.087345, -0.183635, -0.080666, -0.014058 ],
			"coeffs_06" : [ -0.143272, -0.239325, -0.040754, 0.250623 ],
			"coeffs_07" : [ -0.022732, 0.066196, -0.316204, -0.010908 ],
			"coeffs_08" : [ -0.074583, -0.000356, -0.059482, 0.103736 ],
			"coeffs_09" : [ -0.236491, 0.149116, -0.22856, 0.132025 ],
			"coeffs_10" : [ -0.142815, 0.195092, 0.118665, 0.155456 ],
			"coeffs_11" : [ -0.186328, -0.116449, 0.101921, 0.060023 ],
			"coeffs_12" : [ 0.23007, 0.175697, 0.110402, -0.091816 ],
			"coeffs_13" : [ 0.2071, 0.203715, -0.191845, 0.044751 ],
			"coeffs_14" : [ 0.154776, 0.001515, 0.082485, 0.069023 ],
			"coeffs_15" : [ -0.295819, 0.079204, 0.040724, 0.163873 ],
			"coeffs_16" : [ -0.20403, 0.231835, -0.126212, -0.171418 ],
			"coeffs_17" : [ 0.195807, 0.193229, -0.010255, -0.201466 ],
			"coeffs_18" : [ -0.075384, 0.2673, -0.116736, 0.128434 ],
			"coeffs_19" : [ 0.039131, 0.166118, 0.205066, -0.164048 ],
			"coeffs_20" : [ 0.068914, 0.184791, 0.140904, 0.047009 ],
			"coeffs_21" : [ -0.256738, 0.14508, 0.035879, -0.037218 ],
			"coeffs_22" : [ -0.284093, -0.123145, -0.150655, 0.257037 ],
			"coeffs_23" : [ -0.061516, -0.134511, -0.076451, -0.214398 ],
			"coeffs_24" : [ 0.130088, -0.078163, -0.049463, -0.094016 ],
			"coeffs_25" : [ 0.01545, -0.123933, -0.064898, 0.12192 ],
			"coeffs_26" : [ 0.050657, 0.287736, 0.165309, 0.165223 ],
			"coeffs_27" : [ -0.092509, 0.078041, 0.150565, 0.115261 ],
			"coeffs_28" : [ 0.120853, -0.018335, 0.139181, 0.15354 ],
			"coeffs_29" : [ 0.131829, -0.065462, -0.095617, -0.103307 ],
			"coeffs_30" : [ 0.019922, 0.237758, -0.256723, 0.151194 ],
			"coeffs_31" : [ -0.104386, 0.069819, -0.00884, -0.15015 ],
			"coeffs_32" : [ 0.001018, -0.142761, -0.268709, 0.240441 ],
			"coeffs_33" : [ 0.113236, -0.104585, -0.141993, -0.107 ],
			"coeffs_34" : [ -0.258801, -0.091881, -0.059979, -0.023522 ],
			"coeffs_35" : [ -0.148439, -0.175015, -0.1635, 0.017022 ],
			"coeffs_36" : [ -0.265726, -0.097001, 0.016083, 0.066348 ],
			"coeffs_37" : [ -0.029682, -0.040742, 0.094178, -0.168623 ],
			"coeffs_38" : [ -0.025222, 0.138239, 0.231274, -0.005562 ],
			"coeffs_39" : [ -0.270399, -0.031887, 0.028386, -0.10751 ],
			"coeffs_40" : [ 0.051034, -0.154586, 0.22177, 0.184844 ],
			"coeffs_41" : [ -0.074065, 0.15755, -0.13225, 0.242348 ],
			"coeffs_42" : [ -0.283891, 0.192961, 0.110498, -0.13892 ],
			"coeffs_43" : [ 0.017549, 0.066535, 0.057403, -0.108456 ],
			"coeffs_44" : [ 0.149341, 0.15651, -0.131272, -0.036975 ],
			"coeffs_45" : [ 0.080375, 0.002422, 0.000873, 0.289339 ],
			"coeffs_46" : [ -0.021062, 0.206412, -0.261533, -0.027849 ],
			"coeffs_47" : [ 0.120735, 0.111307, -0.098424, -0.027622 ],
			"coeffs_48" : [ 0.22469, -0.118949, -0.009679, 0.202087 ],
			"coeffs_49" : [ -0.307597, -0.096818, -0.145052, -0.054237 ],
			"coeffs_50" : [ -0.058708, 0.009019, -0.214735, 0.062373 ],
			"coeffs_51" : [ 0.289716, -0.038393, -0.038633, 0.035707 ],
			"coeffs_52" : [ 0.133781, 0.074152, 0.009447, 0.146943 ],
			"coeffs_53" : [ 0.044896, -0.039947, -0.004951, -0.188238 ],
			"coeffs_54" : [ -0.22299, -0.055603, 0.086181, -0.013246 ],
			"coeffs_55" : [ -0.044483, -0.211837, 0.180523, -0.139798 ],
			"coeffs_56" : [ 0.03005, 0.052518, 0.061671, 0.048869 ],
			"coeffs_57" : [ -0.117082, 0.112437, 0.167637, 0.095974 ],
			"coeffs_58" : [ -0.047453, 0.302001, 0.185343, 0.131241 ],
			"coeffs_59" : [ -0.17885, 0.292974, -0.341014, -0.205932 ],
			"coeffs_60" : [ 0.048958, -0.04024, 0.103842, 0.105429 ],
			"coeffs_61" : [ 0.134756, -0.066965, -0.149757, -0.123341 ],
			"coeffs_62" : [ 0.135374, 0.073523, -0.175887, -0.053189 ],
			"coeffs_63" : [ 0.197467, -0.147256, 0.000396, -0.031357 ],
			"coeffs_64" : [ 0.100503, -0.043224, 0.063438, 0.158291 ],
			"coeffs_65" : [ -0.015256, 0.071973, 0.234728, 0.186406 ],
			"coeffs_66" : [ 0.146557, -0.215573, -0.032971, -0.200127 ],
			"coeffs_67" : [ 0.07789, -0.010084, 0.145922, 0.187177 ],
			"coeffs_68" : [ 0.17802, -0.088086, 0.100934, 0.09376 ],
			"coeffs_69" : [ 0.134266, 0.165798, -0.127456, 0.21431 ],
			"coeffs_70" : [ 0.150786, 0.235712, -0.06225, 0.060202 ],
			"coeffs_71" : [ 0.146165, 0.030796, 0.132675, -0.252044 ],
			"coeffs_72" : [ 0.063341, 0.198162, 0.108862, 0.233114 ],
			"coeffs_73" : [ -0.189081, 0.173726, -0.118848, 0.08163 ],
			"coeffs_74" : [ -0.244161, 0.119648, 0.062326, -0.019331 ],
			"coeffs_75" : [ 0.120268, -0.188416, -0.201815, -0.068148 ],
			"coeffs_76" : [ -0.21406, 0.101883, -0.016226, 0.019865 ],
			"coeffs_77" : [ -0.163112, 0.255112, 0.040114, -0.045976 ],
			"coeffs_78" : [ 0.098694, -0.127981, 0.14209, 0.04676 ],
			"coeffs_79" : [ 0.083326, -0.210711, -0.165272, 0.031248 ],
			"coeffs_80" : [ -0.058399, 0.238643, -0.196851, -0.164604 ],
			"coeffs_81" : [ 0.125903, 0.046367, 0.195467, 0.169848 ],
			"coeffs_82" : [ -0.210841, -0.110131, -0.191862, 0.061846 ],
			"coeffs_83" : [ 0.010094, 0.167837, -0.049787, -0.009504 ],
			"coeffs_84" : [ 0.333128, -0.268852, -0.010123, -0.133946 ],
			"coeffs_85" : [ -0.007847, 0.005912, -0.050291, -0.079454 ],
			"coeffs_86" : [ -0.059059, 0.053741, 0.117477, -0.189213 ],
			"coeffs_87" : [ -0.037844, -0.06458, 0.292585, 0.018855 ],
			"coeffs_88" : [ -0.213049, -0.220569, -0.145965, 0.088511 ],
			"coeffs_89" : [ -0.035499, -0.13658, -0.129602, -0.024174 ],
			"coeffs_90" : [ -0.286534, -0.041185, 0.097069, -0.12145 ],
			"coeffs_91" : [ 0.315372, -0.111008, 0.04362, -0.18207 ],
			"coeffs_92" : [ 0.162238, -0.15016, -0.078061, -0.179276 ],
			"coeffs_93" : [ -0.230829, 0.144062, -0.180562, 0.219356 ],
			"coeffs_94" : [ -0.273999, 0.00593, 0.127382, 0.036143 ],
			"coeffs_95" : [ -0.145576, -0.000392, 0.100015, -0.068494 ],
			"coeffs_96" : [ -0.25285, -0.104429, 0.039624, 0.242551 ],
			"coeffs_97" : [ -0.081516, 0.044762, 0.131536, 0.11836 ],
			"coeffs_98" : [ -0.18639, 0.112045, -0.164326, 0.119009 ],
			"coeffs_99" : [ -0.218543, -0.014716, 0.231745, -0.173017 ],
			"intercepts" : [ -0.017874, 0.090907, 0.105334, 0.193145 ],
			"name" : "Hidden_Layer_1"
		},
		"Layer_2" : 	{
			"NbInputs" : 4,
			"NbOutputs" : 8,
			"coeffs_0" : [ -0.33128, 0.046097, 0.811087, 0.703846, 0.512195, -0.201322, -0.33661, -0.265199 ],
			"coeffs_1" : [ 0.153989, -0.604201, -0.319473, -0.552422, -0.212213, -0.300222, 0.087404, -0.152977 ],
			"coeffs_2" : [ 0.728207, 0.509332, -0.265833, 0.120303, 0.425989, -0.632764, -0.31425, 0.090953 ],
			"coeffs_3" : [ -0.36217, -0.01323, -0.689326, -0.062566, -0.214084, 0.280723, -0.074905, 0.504726 ],
			"intercepts" : [ -0.04821, -0.621718, 0.603805, -0.525778, -0.011712, -0.05822, 0.375335, -0.68978 ],
			"name" : "Hidden_Layer_2"
		},
		"Layer_3" : 	{
			"NbInputs" : 8,
			"NbOutputs" : 6,
			"coeffs_0" : [ -0.141912, 0.515112, 0.426891, -0.361549, 0.575399, 0.049486 ],
			"coeffs_1" : [ 0.663677, 0.506011, -0.332814, 0.461518, 0.617647, 0.455842 ],
			"coeffs_2" : [ 0.537087, 0.085966, 0.151818, -0.143469, 0.038973, -0.054153 ],
			"coeffs_3" : [ 0.196481, -0.215494, -0.413525, -0.004945, 0.751044, -0.290411 ],
			"coeffs_4" : [ -0.193794, -0.485722, -0.295285, -0.387067, -0.00474, -0.275788 ],
			"coeffs_5" : [ 0.50571, 0.037936, 0.208026, 0.0687, 0.047888, 0.022317 ],
			"coeffs_6" : [ -0.364613, 0.559322, 0.740924, 0.357655, 0.111032, 0.276608 ],
			"coeffs_7" : [ -0.073557, 0.461395, 0.266721, -0.096627, -0.147877, -0.631406 ],
			"intercepts" : [ 0.652651, -0.57552, -0.232738, 0.661436, -0.315827, 0.607363 ],
			"name" : "Hidden_Layer_3"
		},
		"Layer_4" : 	{
			"NbInputs" : 6,
			"NbOutputs" : 1,
			"coeffs_0" : [ -0.57 ],
			"coeffs_1" : [ 0.840942 ],
			"coeffs_2" : [ 0.453743 ],
			"coeffs_3" : [ 0.861368 ],
			"coeffs_4" : [ -0.717354 ],
			"coeffs_5" : [ 0.592693 ],
			"intercepts" : [ -0.723294 ],
			"name" : "Output_Layer"
		},
		"sizes" : [ 100, 4, 8, 6, 1 ]
	},
	"metadata" :  { "model" : "Rosenblatt_MLP", "signature" : "ff4_ff4", "version" : "2024-W13" },
	"options" :  { "activation" : "relu", "alpha" : 0.0001, "batch_size" : null, "beta_1" : 0.9, "beta_2" : 0.999, "early_stopping" : 0, "epsilon" : 0.0, "hidden_layer_sizes" : [ 4, 8, 6 ], "learning_rate" : "constant", "learning_rate_init" : 0.001, "max_fun" : 15000, "max_iter" : 32, "momentum" : 0.9, "n_iter_no_change" : 10, "nesterovs_momentum" : 1, "power_t" : 0.5, "random_state" : 1789, "shuffle" : 1, "solver" : "adam", "tol" : 0.0001, "validation_fraction" : 0.1, "verbose" : 0, "warm_start" : 0 }
}
BEAUTIFIED_JSON_END
('OPERATION_START', 'PREDICT')
[-1.48878300e+00 -2.04749489e+00 -7.93683469e-01 -1.27087831e-01
 -6.12230897e-02 -1.12974155e+00 -2.03221321e+00 -1.51116300e+00
 -8.58669817e-01  1.51668370e-01  3.81690264e-02 -3.90521199e-01
 -5.96045792e-01 -2.41773397e-01 -7.25311339e-02 -1.27087831e-01
 -2.50853992e+00 -9.53141451e-01 -8.65541220e-01  1.52893662e-02
  3.42461497e-01 -1.05343199e+00 -1.30447447e+00 -1.71842337e+00
 -2.97553015e+00 -1.27087831e-01 -7.60779858e-01 -1.86052352e-01
 -1.44400549e+00 -2.99564314e+00 -3.41636240e-02  2.41135478e-01
 -2.64137954e-01 -1.65586084e-01 -1.50168449e-01 -3.88848782e-01
  1.07389718e-01 -1.78637266e+00  3.06837767e-01 -1.74785423e+00
 -9.72235680e-01 -1.36472285e-02 -2.09335834e-01  5.05873561e-02
 -9.41468060e-01 -2.88471103e-01 -1.27087831e-01 -2.01502705e+00
 -6.72836185e-01 -4.94176596e-01 -7.38049328e-01  2.94384331e-01
 -1.16750687e-01 -4.86701071e-01 -6.76512361e-01 -9.59724545e-01
 -1.16061330e-01 -1.51365995e-03 -2.25615668e+00 -8.84389758e-01
 -4.85068768e-01 -1.89017922e-01 -1.56315088e+00  3.50928009e-01
  6.76038861e-03 -4.09223139e-02 -2.34310895e-01 -1.60954690e+00
 -2.36975384e+00 -2.62963247e+00 -4.65128183e-01 -7.29894996e-01
 -2.42490321e-01 -3.79742026e-01 -4.66364801e-01 -1.79061502e-01
 -3.17655325e+00 -4.12756860e-01 -3.42869473e+00 -1.09070313e+00
 -2.88661170e+00  1.18699759e-01 -3.82440448e+00 -2.54114807e-01
  2.60588259e-01 -3.49085093e+00 -1.51425272e-01 -1.03950953e+00
 -1.27087831e-01 -6.92932010e-01 -2.66275972e-01  2.25839317e-02
 -1.22446167e+00 -8.34712625e-01  3.06106061e-01 -1.25401878e+00
 -2.46833026e-01  1.90377235e-03 -1.88326955e-01 -1.21470630e+00
 -8.21205080e-01 -3.80385369e-01 -2.60117912e+00 -1.27087831e-01
 -4.61330116e-01 -8.26154709e-01 -2.10623646e+00 -4.02325392e-01
  6.51091933e-02 -9.74617302e-02  3.16970587e-01 -2.19514370e-01
 -5.86082637e-02 -8.74784231e-01 -8.35951030e-01 -4.27950412e-01
 -1.00647092e+00 -1.27087831e-01  1.37386620e-02 -7.22473860e-01
 -1.21780634e+00 -1.61319315e-01 -8.53149414e-01 -1.51611805e+00
 -8.93946409e-01 -2.04210758e-01 -2.00886786e-01 -2.54879087e-01
 -2.20530033e+00  8.06045532e-03 -1.56734884e-01 -9.03521478e-02
 -3.09276849e-01 -5.12599707e-01  1.31458402e-01 -1.56090260e+00
 -1.32982028e+00 -1.40131688e+00 -4.72213566e-01 -8.91358554e-02
 -1.39799976e+00 -2.03637552e+00  1.29608423e-01 -1.73301494e+00
 -1.31123602e-01  2.82345653e-01  1.03639752e-01 -2.51869380e-01
  3.66421759e-01 -1.04276609e+00 -1.14675617e+00 -8.76798034e-02
 -3.56310189e-01 -8.99993718e-01 -7.42325068e-01 -7.77252316e-01
  3.39559197e-01 -1.27087831e-01 -1.84996641e+00  1.21886551e-01
 -1.84197605e-01 -3.59211743e-01 -1.93678212e+00 -1.71428323e+00
 -9.14503872e-01 -8.28123569e-01 -3.77903461e-01 -1.37403756e-01
 -2.72851896e+00 -2.69514054e-01 -2.71537364e-01 -7.25068331e-01
 -9.48803127e-02 -2.10905641e-01 -1.13805103e+00 -8.88874531e-01
 -9.38628793e-01 -2.01142639e-01 -2.36242604e+00 -3.41776788e-01
 -1.02180862e+00 -1.09918725e+00  6.96950555e-02 -7.18092561e-01
 -8.79848361e-01 -4.83368039e-02 -2.15089393e+00 -2.26105189e+00
 -6.45853579e-02 -5.40513039e-01  2.91915208e-01  1.29015744e-01
 -6.47792161e-01  1.60870314e-01 -1.35000855e-01  1.31324232e-01
 -7.51440108e-01 -1.27087831e-01  1.63102746e-01 -3.98466676e-01
 -1.27087831e-01 -4.85607743e-01 -6.25653565e-01 -8.88756931e-01
 -6.14115298e-02 -6.53296471e-01 -1.64133763e+00 -7.28993714e-02
  1.45183623e-01 -2.75982469e-01 -1.16234541e-01 -1.50623417e+00
 -2.15358019e+00 -9.10506785e-01 -3.31477940e-01 -8.51119518e-01
  2.61276662e-02 -1.24933362e-01 -4.62116241e-01 -2.35985279e-01
 -4.71760273e-01 -1.87017953e+00 -1.86510897e+00  1.47125721e-02
 -3.41112643e-01 -3.91429567e+00  1.61619037e-01 -1.57460952e+00
 -9.43468809e-02 -8.30311477e-02 -2.28856832e-01 -1.03495705e+00
 -4.06462878e-01 -5.41811049e-01 -1.95911109e-01 -6.43981814e-01
 -1.77852666e+00 -8.90977085e-02 -2.14850640e+00 -1.34286761e+00
 -2.23632455e-02 -5.54893255e-01 -1.51506305e+00 -9.59739923e-01
 -1.27087831e-01 -5.39216220e-01 -9.30820704e-02 -6.17407680e-01
 -7.93043971e-01  2.16113657e-01 -1.18650830e+00  1.29838169e-01
 -1.04786605e-01 -4.12053376e-01 -1.27087831e-01 -2.33741611e-01
 -2.71600693e-01  7.28782713e-02  3.23049426e-02 -2.74759352e-01
 -3.99606556e-01 -5.86486459e-02  1.69603854e-01 -1.27087831e-01
 -1.27572143e+00 -1.27087831e-01 -7.70181656e-01 -7.78081715e-02
  2.86282301e-01 -6.06331587e-01 -4.46441054e-01 -1.91341281e-01
 -7.70142078e-02 -8.60230565e-01  2.20715106e-01 -1.40738845e+00
 -1.27087831e-01 -2.09659398e-01 -6.25331700e-02 -2.26327389e-01
 -7.80440927e-01 -5.28020263e-02 -2.16251493e-01 -5.32030821e-01
 -5.67231894e-01 -3.29477167e+00  2.14438498e-01 -2.97054201e-01
 -3.86745882e+00 -1.86512709e-01 -3.16561759e-01 -1.27087831e-01
  1.12229735e-01 -8.20097029e-02 -4.62020040e-02 -9.71042514e-02
 -2.44993353e+00 -1.89455360e-01 -1.27087831e-01 -3.36666644e-01
 -9.55054402e-01  9.68100429e-02  3.74055952e-01 -3.39243948e-01
 -1.29404819e+00 -9.56769764e-01 -1.36711597e+00  2.56969631e-02
 -3.93755436e+00 -8.52744520e-01 -6.10869825e-02 -2.55299300e-01
 -9.79224920e-01  1.45413488e-01  3.68893445e-01 -9.75576043e-02
 -1.57472801e+00 -5.71605563e-03 -1.26032948e-01 -4.30099392e+00
 -4.26307917e-01  1.56722993e-01 -1.34376788e+00 -3.29209018e+00
 -2.15389204e+00 -2.33561516e+00 -1.41698122e-02 -1.27087831e-01
 -1.81195295e+00 -3.13849473e+00 -1.74745917e-01 -5.78318357e-01
 -3.68900836e-01  1.04714900e-01  3.66133779e-01 -2.24838018e-01
 -5.32558858e-02 -5.20175636e-01 -3.76384854e-02 -3.63613844e+00
 -7.34890997e-02 -6.68064713e-01 -5.13321042e-01 -1.27306700e+00
  3.65868211e-03 -3.23926836e-01 -7.08991826e-01 -2.85445309e+00
 -1.41266614e-01 -8.05218220e-01  7.73672163e-02  1.29968762e-01
 -1.00664151e+00  2.80832648e-01  3.83241028e-01 -3.21633399e-01
  7.44873285e-03 -1.13229680e+00 -1.15155458e+00 -1.92275763e-01
 -1.67061722e+00 -5.90671837e-01  8.70543122e-02  2.41518974e-01
 -2.66003585e+00  1.65570676e-02  2.05061227e-01 -2.26871657e+00
 -1.19924939e+00 -4.68730390e-01 -2.13989258e+00 -2.27495575e+00
 -1.27087831e-01 -3.18259150e-01 -1.65586084e-01 -3.62334162e-01
  3.15472335e-01 -1.35328317e+00 -9.91910517e-01 -1.27087831e-01
 -5.74080825e-01 -1.28329098e-01 -1.63710654e+00 -8.61336112e-01
 -1.38407350e+00 -3.57132316e-01 -2.47325587e+00 -1.16348219e+00
  3.05154413e-01 -1.27735138e-02 -4.63576227e-01 -2.77785003e-01
 -1.37227118e-01 -1.16325080e+00 -2.77922839e-01 -2.20735162e-01
 -2.26452637e+00 -2.68224418e-01 -9.43577528e-01 -1.27087831e-01
 -1.61975473e-01 -8.32873940e-01  1.15455240e-01 -1.23354828e+00
 -2.25568366e+00 -1.41003919e+00 -6.19450927e-01  2.86599249e-01
 -3.11059141e+00  2.96065778e-01 -5.21987557e-01 -1.27087831e-01
 -2.18478346e+00 -1.73059851e-01 -3.15160108e+00 -5.15595436e-01
 -7.84408927e-01 -1.80826259e+00 -9.07793045e-01 -1.45662463e+00
  3.28230917e-01 -7.12611318e-01 -1.27087831e-01  3.50856751e-01
  9.91676748e-02 -1.87299693e+00 -1.02931350e-01 -2.20101428e+00
 -1.88728952e+00 -3.32580662e+00 -1.27087831e-01 -3.36981267e-01
  1.27685487e-01 -4.90327060e-01 -1.69163227e+00  2.06544995e-02
 -4.91105175e+00 -3.09937668e+00 -7.57133007e-01 -6.23170733e-01
 -3.31631255e+00  1.98078096e-01 -2.06900764e+00 -3.42092335e-01
  1.03999138e-01 -2.23409939e+00 -4.01943117e-01  1.26145840e-01
 -8.92733812e-01  2.58361042e-01 -4.59197044e-01 -3.87014198e+00
  8.26837718e-02 -2.75388777e-01 -9.19640005e-01 -6.81132376e-02
 -3.97888124e-01 -2.88639522e+00 -2.46992558e-01 -2.12615824e+00
 -1.13937950e+00 -8.79515469e-01  1.46018147e-01 -2.30018067e+00
 -3.01968169e+00 -2.11713862e+00 -2.85879827e+00 -8.69217515e-01
  1.08088046e-01 -1.99887538e+00 -2.45319366e+00 -3.25561464e-02
  2.06922114e-01 -6.97214484e-01 -3.88357937e-01  1.45836651e-01
 -7.82063961e-01 -1.56117678e-02 -5.13105035e-01 -5.24213195e-01
 -1.56610346e+00 -2.19942284e+00 -1.16877794e+00  4.51545715e-02
  1.24523640e-02 -1.01670265e+00 -3.15543604e+00 -1.33041131e+00
 -4.15466368e-01 -1.30101299e+00 -2.75822544e+00 -1.01723254e-01
 -9.09145772e-02  2.15479016e-01 -5.62286139e-01 -4.41343725e-01
  2.85248637e-01  2.07441717e-01 -3.11325645e+00 -6.07897758e-01
 -1.24954760e-01 -1.55546516e-01 -1.01753819e+00  4.64007944e-01
 -5.04568219e-01 -5.48075736e-02 -6.40271485e-01 -5.86423516e-01
 -5.67849874e-02 -2.01476192e+00 -4.63295668e-01 -3.51626825e+00]
('OPERATION_END_ELAPSED', 0.011, 'PREDICT')
('OPERATION_START', 'PREDICT')
[-1.48878086e+00 -2.04749584e+00 -7.93683171e-01 -1.27087831e-01
 -6.12217784e-02 -1.12974036e+00 -2.03221226e+00 -1.51116216e+00
 -8.58671427e-01  1.51668310e-01  3.81695032e-02 -3.90520811e-01
 -5.96044779e-01 -2.41770595e-01 -7.25299418e-02 -1.27087831e-01
 -2.50853729e+00 -9.53138351e-01 -8.65541697e-01  1.52908862e-02
  3.42461556e-01 -1.05343091e+00 -1.30447292e+00 -1.71842384e+00
 -2.97553158e+00 -1.27087831e-01 -7.60777831e-01 -1.86051011e-01
 -1.44400263e+00 -2.99564075e+00 -3.41638625e-02  2.41135716e-01
 -2.64137477e-01 -1.65585488e-01 -1.50166482e-01 -3.88848543e-01
  1.07389450e-01 -1.78637135e+00  3.06837976e-01 -1.74785459e+00
 -9.72232938e-01 -1.36475265e-02 -2.09335148e-01  5.05881310e-02
 -9.41470742e-01 -2.88468122e-01 -1.27087831e-01 -2.01502657e+00
 -6.72839046e-01 -4.94175166e-01 -7.38046825e-01  2.94384181e-01
 -1.16750509e-01 -4.86699373e-01 -6.76510692e-01 -9.59722340e-01
 -1.16063446e-01 -1.51231885e-03 -2.25615525e+00 -8.84385347e-01
 -4.85071093e-01 -1.89015180e-01 -1.56314993e+00  3.50927413e-01
  6.76023960e-03 -4.09227610e-02 -2.34310746e-01 -1.60954630e+00
 -2.36975145e+00 -2.62962842e+00 -4.65131998e-01 -7.29893267e-01
 -2.42489874e-01 -3.79742175e-01 -4.66368794e-01 -1.79059863e-01
 -3.17655277e+00 -4.12757128e-01 -3.42869759e+00 -1.09070420e+00
 -2.88660955e+00  1.18697792e-01 -3.82440400e+00 -2.54112810e-01
  2.60588288e-01 -3.49084544e+00 -1.51424140e-01 -1.03951275e+00
 -1.27087831e-01 -6.92931771e-01 -2.66273946e-01  2.25832760e-02
 -1.22446072e+00 -8.34709406e-01  3.06106031e-01 -1.25401878e+00
 -2.46831834e-01  1.90371275e-03 -1.88327253e-01 -1.21470654e+00
 -8.21204245e-01 -3.80384386e-01 -2.60118246e+00 -1.27087831e-01
 -4.61331159e-01 -8.26155066e-01 -2.10623503e+00 -4.02324289e-01
  6.51087463e-02 -9.74594951e-02  3.16970438e-01 -2.19516009e-01
 -5.86082339e-02 -8.74783874e-01 -8.35952401e-01 -4.27950829e-01
 -1.00647140e+00 -1.27087831e-01  1.37383342e-02 -7.22471297e-01
 -1.21780360e+00 -1.61319405e-01 -8.53146076e-01 -1.51612258e+00
 -8.93946648e-01 -2.04209208e-01 -2.00887084e-01 -2.54880577e-01
 -2.20529771e+00  8.06018710e-03 -1.56735182e-01 -9.03526843e-02
 -3.09275746e-01 -5.12599707e-01  1.31459385e-01 -1.56090188e+00
 -1.32981944e+00 -1.40131557e+00 -4.72215354e-01 -8.91361535e-02
 -1.39799976e+00 -2.03637362e+00  1.29608184e-01 -1.73301256e+00
 -1.31125629e-01  2.82345682e-01  1.03639334e-01 -2.51867682e-01
  3.66422087e-01 -1.04276538e+00 -1.14675260e+00 -8.76799822e-02
 -3.56309563e-01 -8.99990559e-01 -7.42323399e-01 -7.77252555e-01
  3.39559019e-01 -1.27087831e-01 -1.84997427e+00  1.21886551e-01
 -1.84194297e-01 -3.59213382e-01 -1.93678427e+00 -1.71427917e+00
 -9.14500713e-01 -8.28120530e-01 -3.77902746e-01 -1.37403816e-01
 -2.72851419e+00 -2.69513696e-01 -2.71532506e-01 -7.25062966e-01
 -9.48801935e-02 -2.10901499e-01 -1.13805187e+00 -8.88870716e-01
 -9.38628972e-01 -2.01140493e-01 -2.36242962e+00 -3.41776967e-01
 -1.02180541e+00 -1.09918666e+00  6.96960688e-02 -7.18093514e-01
 -8.79847765e-01 -4.83369827e-02 -2.15089846e+00 -2.26104927e+00
 -6.45842552e-02 -5.40510595e-01  2.91914582e-01  1.29015803e-01
 -6.47790432e-01  1.60869837e-01 -1.34999245e-01  1.31324291e-01
 -7.51441360e-01 -1.27087831e-01  1.63103014e-01 -3.98463249e-01
 -1.27087831e-01 -4.85606790e-01 -6.25656247e-01 -8.88756871e-01
 -6.14108145e-02 -6.53298259e-01 -1.64133751e+00 -7.28981197e-02
  1.45182937e-01 -2.75981665e-01 -1.16234750e-01 -1.50623202e+00
 -2.15358329e+00 -9.10503447e-01 -3.31478536e-01 -8.51113379e-01
  2.61270404e-02 -1.24931544e-01 -4.62115139e-01 -2.35985726e-01
 -4.71759439e-01 -1.87017655e+00 -1.86511266e+00  1.47124827e-02
 -3.41112942e-01 -3.91429305e+00  1.61622256e-01 -1.57460809e+00
 -9.43470001e-02 -8.30308795e-02 -2.28857130e-01 -1.03495681e+00
 -4.06463593e-01 -5.41808367e-01 -1.95910692e-01 -6.43983126e-01
 -1.77852380e+00 -8.90983641e-02 -2.14850712e+00 -1.34286606e+00
 -2.23635435e-02 -5.54892838e-01 -1.51505947e+00 -9.59740400e-01
 -1.27087831e-01 -5.39219975e-01 -9.30827260e-02 -6.17407382e-01
 -7.93044567e-01  2.16113597e-01 -1.18650675e+00  1.29838079e-01
 -1.04785651e-01 -4.12053585e-01 -1.27087831e-01 -2.33741790e-01
 -2.71599948e-01  7.28774071e-02  3.23052406e-02 -2.74759263e-01
 -3.99604321e-01 -5.86488247e-02  1.69604063e-01 -1.27087831e-01
 -1.27572060e+00 -1.27087831e-01 -7.70179272e-01 -7.78073370e-02
  2.86282390e-01 -6.06331110e-01 -4.46440876e-01 -1.91340417e-01
 -7.70149827e-02 -8.60230803e-01  2.20715433e-01 -1.40738738e+00
 -1.27087831e-01 -2.09660411e-01 -6.25332594e-02 -2.26327628e-01
 -7.80440629e-01 -5.28007448e-02 -2.16251731e-01 -5.32030404e-01
 -5.67233443e-01 -3.29476738e+00  2.14438647e-01 -2.97053605e-01
 -3.86745787e+00 -1.86511964e-01 -3.16559613e-01 -1.27087831e-01
  1.12229496e-01 -8.20089877e-02 -4.62027788e-02 -9.71027017e-02
 -2.44993162e+00 -1.89452231e-01 -1.27087831e-01 -3.36664051e-01
 -9.55053687e-01  9.68088508e-02  3.74056250e-01 -3.39243442e-01
 -1.29404509e+00 -9.56769705e-01 -1.36711478e+00  2.56970227e-02
 -3.93755007e+00 -8.52744102e-01 -6.10859692e-02 -2.55300790e-01
 -9.79224145e-01  1.45414084e-01  3.68893683e-01 -9.75571871e-02
 -1.57472742e+00 -5.71605563e-03 -1.26030594e-01 -4.30099010e+00
 -4.26304936e-01  1.56723261e-01 -1.34376705e+00 -3.29208446e+00
 -2.15389037e+00 -2.33561110e+00 -1.41714215e-02 -1.27087831e-01
 -1.81194437e+00 -3.13849592e+00 -1.74745679e-01 -5.78317702e-01
 -3.68899465e-01  1.04714662e-01  3.66134256e-01 -2.24838048e-01
 -5.32545149e-02 -5.20175517e-01 -3.76396477e-02 -3.63613510e+00
 -7.34881759e-02 -6.68063700e-01 -5.13322234e-01 -1.27306616e+00
  3.65808606e-03 -3.23926151e-01 -7.08992839e-01 -2.85445046e+00
 -1.41265750e-01 -8.05217445e-01  7.73665309e-02  1.29969031e-01
 -1.00664175e+00  2.80832708e-01  3.83241653e-01 -3.21632653e-01
  7.44891167e-03 -1.13229370e+00 -1.15155256e+00 -1.92275792e-01
 -1.67061949e+00 -5.90674818e-01  8.70542824e-02  2.41517514e-01
 -2.66003752e+00  1.65569484e-02  2.05060512e-01 -2.26872206e+00
 -1.19924772e+00 -4.68726844e-01 -2.13988829e+00 -2.27495193e+00
 -1.27087831e-01 -3.18256944e-01 -1.65585488e-01 -3.62332076e-01
  3.15473169e-01 -1.35328186e+00 -9.91906047e-01 -1.27087831e-01
 -5.74078679e-01 -1.28327847e-01 -1.63710320e+00 -8.61335278e-01
 -1.38407075e+00 -3.57131511e-01 -2.47324991e+00 -1.16348445e+00
  3.05154920e-01 -1.27733350e-02 -4.63573247e-01 -2.77783215e-01
 -1.37227446e-01 -1.16324794e+00 -2.77925223e-01 -2.20732778e-01
 -2.26452470e+00 -2.68225640e-01 -9.43577409e-01 -1.27087831e-01
 -1.61970794e-01 -8.32870722e-01  1.15456104e-01 -1.23354757e+00
 -2.25568342e+00 -1.41003764e+00 -6.19449198e-01  2.86598980e-01
 -3.11058736e+00  2.96065092e-01 -5.21985948e-01 -1.27087831e-01
 -2.18478179e+00 -1.73055977e-01 -3.15160155e+00 -5.15594423e-01
 -7.84414291e-01 -1.80825806e+00 -9.07792449e-01 -1.45662451e+00
  3.28230292e-01 -7.12609351e-01 -1.27087831e-01  3.50856811e-01
  9.91661847e-02 -1.87299061e+00 -1.02933496e-01 -2.20101452e+00
 -1.88728690e+00 -3.32580376e+00 -1.27087831e-01 -3.36981058e-01
  1.27685457e-01 -4.90326703e-01 -1.69162643e+00  2.06542313e-02
 -4.91105127e+00 -3.09937739e+00 -7.57132709e-01 -6.23168707e-01
 -3.31631184e+00  1.98078156e-01 -2.06900620e+00 -3.42090696e-01
  1.03998840e-01 -2.23410034e+00 -4.01942253e-01  1.26145899e-01
 -8.92733335e-01  2.58361101e-01 -4.59196091e-01 -3.87014532e+00
  8.26840699e-02 -2.75389254e-01 -9.19639826e-01 -6.81129992e-02
 -3.97888362e-01 -2.88639164e+00 -2.46987879e-01 -2.12615943e+00
 -1.13937783e+00 -8.79513562e-01  1.46018386e-01 -2.30017710e+00
 -3.01967454e+00 -2.11713815e+00 -2.85880232e+00 -8.69216144e-01
  1.08089507e-01 -1.99887323e+00 -2.45319462e+00 -3.25537920e-02
  2.06921607e-01 -6.97216034e-01 -3.88358474e-01  1.45836502e-01
 -7.82060742e-01 -1.56121850e-02 -5.13105333e-01 -5.24209261e-01
 -1.56610310e+00 -2.19942236e+00 -1.16877735e+00  4.51541543e-02
  1.24551654e-02 -1.01670396e+00 -3.15543032e+00 -1.33041012e+00
 -4.15464312e-01 -1.30101132e+00 -2.75822425e+00 -1.01724118e-01
 -9.09150541e-02  2.15478629e-01 -5.62286556e-01 -4.41341698e-01
  2.85248160e-01  2.07440615e-01 -3.11325574e+00 -6.07898951e-01
 -1.24955267e-01 -1.55542374e-01 -1.01753438e+00  4.64009047e-01
 -5.04566789e-01 -5.48086464e-02 -6.40270948e-01 -5.86425304e-01
 -5.67843914e-02 -2.01476288e+00 -4.63294953e-01 -3.51626587e+00]
('OPERATION_END_ELAPSED', 0.01, 'PREDICT')
MODEL_PERFS {'class_name': 'mllite.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'RandomReg_100_medium', 'size': 512, 'mse': 26171.693, 'mae': 126.53566, 'mape': 1.0060833, 'r2': 0.0057279925081518135}
WRITING_PERF_CODE 'logs/auto_tests/regression/MLPRegressor/mllite.MLPRegressor_RandomReg_100_medium_option_1.perf'

MODEL_PERFS_TIMINGS {'class_name': 'mllite.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'RandomReg_100_medium', 'training_time_in_sec': 0.104, 'prediction_time_in_sec': 0.01}
WRITING_SQL_CODE 'logs/auto_tests/regression/MLPRegressor/mllite.MLPRegressor_RandomReg_100_medium_option_1_duckdb.sql'



SQL_OUT_PUT_FIRST_LINES_START ('RandomReg_100_medium', 'MLPRegressor', 'duckdb')
WITH model_input AS 
 (SELECT "ADS"."index" AS "index",
    CAST("ADS"."X_0" AS FLOAT) AS "X_0", CAST("ADS"."X_1" AS FLOAT) AS "X_1", CAST("ADS"."X_2" AS FLOAT) AS "X_2", CAST("ADS"."X_3" AS FLOAT) AS "X_3", CAST("ADS"."X_4" AS FLOAT) AS "X_4", CAST("ADS"."X_5" AS FLOAT) AS "X_5", CAST("ADS"."X_6" AS FLOAT) AS "X_6", CAST("ADS"."X_7" AS FLOAT) AS "X_7", CAST("ADS"."X_8" AS FLOAT) AS "X_8", CAST("ADS"."X_9" AS FLOAT) AS "X_9", CAST("ADS"."X_10" AS FLOAT) AS "X_10", CAST("ADS"."X_11" AS FLOAT) AS "X_11", CAST("ADS"."X_12" AS FLOAT) AS "X_12", CAST("ADS"."X_13" AS FLOAT) AS "X_13", CAST("ADS"."X_14" AS FLOAT) AS "X_14", CAST("ADS"."X_15" AS FLOAT) AS "X_15", CAST("ADS"."X_16" AS FLOAT) AS "X_16", CAST("ADS"."X_17" AS FLOAT) AS "X_17", CAST("ADS"."X_18" AS FLOAT) AS "X_18", CAST("ADS"."X_19" AS FLOAT) AS "X_19", CAST("ADS"."X_20" AS FLOAT) AS "X_20", CAST("ADS"."X_21" AS FLOAT) AS "X_21", CAST("ADS"."X_22" AS FLOAT) AS "X_22", CAST("ADS"."X_23" AS FLOAT) AS "X_23", CAST("ADS"."X_24" AS FLOAT) AS "X_24", CAST("ADS
SQL_OUT_PUT_FIRST_LINES_END ('RandomReg_100_medium', 'MLPRegressor', 'duckdb')
SQL_OUT_PUT_LAST_LINES_START ('RandomReg_100_medium', 'MLPRegressor', 'duckdb')
-0.275788 * t."OUT_4"  + 0.022317 * t."OUT_5"  + 0.276608 * t."OUT_6"  + -0.631406 * t."OUT_7" AS "OUT_5"
   FROM "Hidden_Layer_2_Activation" AS t
 ),
"Hidden_Layer_3_Activation" AS
 ( SELECT
    t."index" as "index",
    CASE WHEN (t."OUT_0" > 0) THEN t."OUT_0" ELSE 0 END AS "OUT_0",
    CASE WHEN (t."OUT_1" > 0) THEN t."OUT_1" ELSE 0 END AS "OUT_1",
    CASE WHEN (t."OUT_2" > 0) THEN t."OUT_2" ELSE 0 END AS "OUT_2",
    CASE WHEN (t."OUT_3" > 0) THEN t."OUT_3" ELSE 0 END AS "OUT_3",
    CASE WHEN (t."OUT_4" > 0) THEN t."OUT_4" ELSE 0 END AS "OUT_4",
    CASE WHEN (t."OUT_5" > 0) THEN t."OUT_5" ELSE 0 END AS "OUT_5"
   FROM "Hidden_Layer_3_BA" AS t
 ),
"Output_Layer_BA" AS
 ( SELECT
    t."index" as "index",
    -0.723294 + -0.570000 * t."OUT_0"  + 0.840942 * t."OUT_1"  + 0.453743 * t."OUT_2"  + 0.861368 * t."OUT_3"  + -0.717354 * t."OUT_4"  + 0.592693 * t."OUT_5" AS "OUT_0"
   FROM "Hidden_Layer_3_Activation" AS t
 )
 SELECT
    t."index" AS "index",
    t.OUT_0 AS "Estimator"
  FROM "Output_Layer_BA" AS t

SQL_OUT_PUT_LAST_LINES_END ('RandomReg_100_medium', 'MLPRegressor', 'duckdb') 




COPY_TRAINING_DATA_TO_SQLITE_START
<class 'pandas.core.frame.DataFrame'>
Index: 512 entries, 0 to 511
Data columns (total 100 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   X_0     512 non-null    float32
 1   X_1     512 non-null    float32
 2   X_2     512 non-null    float32
 3   X_3     512 non-null    float32
 4   X_4     512 non-null    float32
 5   X_5     512 non-null    float32
 6   X_6     512 non-null    float32
 7   X_7     512 non-null    float32
 8   X_8     512 non-null    float32
 9   X_9     512 non-null    float32
 10  X_10    512 non-null    float32
 11  X_11    512 non-null    float32
 12  X_12    512 non-null    float32
 13  X_13    512 non-null    float32
 14  X_14    512 non-null    float32
 15  X_15    512 non-null    float32
 16  X_16    512 non-null    float32
 17  X_17    512 non-null    float32
 18  X_18    512 non-null    float32
 19  X_19    512 non-null    float32
 20  X_20    512 non-null    float32
 21  X_21    512 non-null    float32
 22  X_22    512 non-null    float32
 23  X_23    512 non-null    float32
 24  X_24    512 non-null    float32
 25  X_25    512 non-null    float32
 26  X_26    512 non-null    float32
 27  X_27    512 non-null    float32
 28  X_28    512 non-null    float32
 29  X_29    512 non-null    float32
 30  X_30    512 non-null    float32
 31  X_31    512 non-null    float32
 32  X_32    512 non-null    float32
 33  X_33    512 non-null    float32
 34  X_34    512 non-null    float32
 35  X_35    512 non-null    float32
 36  X_36    512 non-null    float32
 37  X_37    512 non-null    float32
 38  X_38    512 non-null    float32
 39  X_39    512 non-null    float32
 40  X_40    512 non-null    float32
 41  X_41    512 non-null    float32
 42  X_42    512 non-null    float32
 43  X_43    512 non-null    float32
 44  X_44    512 non-null    float32
 45  X_45    512 non-null    float32
 46  X_46    512 non-null    float32
 47  X_47    512 non-null    float32
 48  X_48    512 non-null    float32
 49  X_49    512 non-null    float32
 50  X_50    512 non-null    float32
 51  X_51    512 non-null    float32
 52  X_52    512 non-null    float32
 53  X_53    512 non-null    float32
 54  X_54    512 non-null    float32
 55  X_55    512 non-null    float32
 56  X_56    512 non-null    float32
 57  X_57    512 non-null    float32
 58  X_58    512 non-null    float32
 59  X_59    512 non-null    float32
 60  X_60    512 non-null    float32
 61  X_61    512 non-null    float32
 62  X_62    512 non-null    float32
 63  X_63    512 non-null    float32
 64  X_64    512 non-null    float32
 65  X_65    512 non-null    float32
 66  X_66    512 non-null    float32
 67  X_67    512 non-null    float32
 68  X_68    512 non-null    float32
 69  X_69    512 non-null    float32
 70  X_70    512 non-null    float32
 71  X_71    512 non-null    float32
 72  X_72    512 non-null    float32
 73  X_73    512 non-null    float32
 74  X_74    512 non-null    float32
 75  X_75    512 non-null    float32
 76  X_76    512 non-null    float32
 77  X_77    512 non-null    float32
 78  X_78    512 non-null    float32
 79  X_79    512 non-null    float32
 80  X_80    512 non-null    float32
 81  X_81    512 non-null    float32
 82  X_82    512 non-null    float32
 83  X_83    512 non-null    float32
 84  X_84    512 non-null    float32
 85  X_85    512 non-null    float32
 86  X_86    512 non-null    float32
 87  X_87    512 non-null    float32
 88  X_88    512 non-null    float32
 89  X_89    512 non-null    float32
 90  X_90    512 non-null    float32
 91  X_91    512 non-null    float32
 92  X_92    512 non-null    float32
 93  X_93    512 non-null    float32
 94  X_94    512 non-null    float32
 95  X_95    512 non-null    float32
 96  X_96    512 non-null    float32
 97  X_97    512 non-null    float32
 98  X_98    512 non-null    float32
 99  X_99    512 non-null    float32
dtypes: float32(100)
memory usage: 204.0 KB
            X_0       X_1       X_2  ...      X_97      X_98      X_99
index                                ...                              
0      1.331256 -0.890219  0.415263  ...  0.066743 -1.017662  0.850023
1      0.101475  1.673349  0.646130  ... -0.967804 -0.082562 -0.235254
2      1.143388 -0.121749  1.763748  ... -0.006825 -1.210605  0.186911
3     -0.664377  0.702461 -2.030148  ...  1.036630 -1.509694  1.665004
4     -0.183233  0.167641 -1.018050  ...  0.131021 -0.518277 -0.467736
...         ...       ...       ...  ...       ...       ...       ...
507   -0.085407  0.635354 -0.165095  ...  0.566512 -0.632373 -2.073853
508    0.689048 -0.025340 -0.142676  ... -0.129965 -0.697890  0.794571
509   -0.929089 -0.892634  1.709303  ...  1.169236 -2.478970 -0.503891
510    1.078776 -0.630185  0.944048  ... -0.346390 -0.233485 -0.721341
511    0.057106  0.428869 -0.742754  ... -0.777635 -0.139843 -0.534295

[512 rows x 100 columns]
COPY_TRAINING_DATA_TO_SQLITE_END


PREDICT_MODEL_ON_TRAINING_DATA_INSIDE_SQLITE_START
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 512 entries, 0 to 511
Data columns (total 2 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   index      512 non-null    int64  
 1   Estimator  512 non-null    float64
dtypes: float64(1), int64(1)
memory usage: 8.1 KB
     index  Estimator
0        0  -1.488781
1        1  -2.047496
2        2  -0.793683
3        3  -0.127088
4        4  -0.061222
..     ...        ...
507    507  -0.586425
508    508  -0.056784
509    509  -2.014763
510    510  -0.463295
511    511  -3.516266

[512 rows x 2 columns]
PREDICT_MODEL_ON_TRAINING_DATA_INSIDE_SQLITE_END


COMPARE_SQL_OUTPUT_AND_PYTHON_OUTPUT_START
Index(['index', 'Estimator'], dtype='object')
MLLITE_REG_SQL_L1_ERROR ('RandomReg_100_medium', 'MLPRegressor') Estimator 1.3801036402583122e-06
     index  SQL_Estimator  Py_Estimator     SQL_Error
496    496       0.285248      0.285249 -4.768372e-07
497    497       0.207441      0.207442 -1.102686e-06
498    498      -3.113256     -3.113256  7.152557e-07
499    499      -0.607899     -0.607898 -1.192093e-06
500    500      -0.124955     -0.124955 -5.066395e-07
501    501      -0.155542     -0.155547  4.142523e-06
502    502      -1.017534     -1.017538  3.814697e-06
503    503       0.464009      0.464008  1.102686e-06
504    504      -0.504567     -0.504568  1.430511e-06
505    505      -0.054809     -0.054808 -1.072884e-06
506    506      -0.640271     -0.640271  5.364418e-07
507    507      -0.586425     -0.586424 -1.788139e-06
508    508      -0.056784     -0.056785  5.960464e-07
509    509      -2.014763     -2.014762 -9.536743e-07
510    510      -0.463295     -0.463296  7.152557e-07
511    511      -3.516266     -3.516268  2.384186e-06
MLLITE_REG_SQL_EXECUTION_STATUS ('RandomReg_100_medium', 'MLPRegressor', 'Success')
COMPARE_SQL_OUTPUT_AND_PYTHON_OUTPUT_END
WRITING_SQL_CODE 'logs/auto_tests/regression/MLPRegressor/mllite.MLPRegressor_RandomReg_100_medium_option_1_sqlite.sql'



SQL_OUT_PUT_FIRST_LINES_START ('RandomReg_100_medium', 'MLPRegressor', 'sqlite')
WITH model_input AS 
 (SELECT "ADS"."index" AS "index",
    CAST("ADS"."X_0" AS FLOAT) AS "X_0", CAST("ADS"."X_1" AS FLOAT) AS "X_1", CAST("ADS"."X_2" AS FLOAT) AS "X_2", CAST("ADS"."X_3" AS FLOAT) AS "X_3", CAST("ADS"."X_4" AS FLOAT) AS "X_4", CAST("ADS"."X_5" AS FLOAT) AS "X_5", CAST("ADS"."X_6" AS FLOAT) AS "X_6", CAST("ADS"."X_7" AS FLOAT) AS "X_7", CAST("ADS"."X_8" AS FLOAT) AS "X_8", CAST("ADS"."X_9" AS FLOAT) AS "X_9", CAST("ADS"."X_10" AS FLOAT) AS "X_10", CAST("ADS"."X_11" AS FLOAT) AS "X_11", CAST("ADS"."X_12" AS FLOAT) AS "X_12", CAST("ADS"."X_13" AS FLOAT) AS "X_13", CAST("ADS"."X_14" AS FLOAT) AS "X_14", CAST("ADS"."X_15" AS FLOAT) AS "X_15", CAST("ADS"."X_16" AS FLOAT) AS "X_16", CAST("ADS"."X_17" AS FLOAT) AS "X_17", CAST("ADS"."X_18" AS FLOAT) AS "X_18", CAST("ADS"."X_19" AS FLOAT) AS "X_19", CAST("ADS"."X_20" AS FLOAT) AS "X_20", CAST("ADS"."X_21" AS FLOAT) AS "X_21", CAST("ADS"."X_22" AS FLOAT) AS "X_22", CAST("ADS"."X_23" AS FLOAT) AS "X_23", CAST("ADS"."X_24" AS FLOAT) AS "X_24", CAST("ADS
SQL_OUT_PUT_FIRST_LINES_END ('RandomReg_100_medium', 'MLPRegressor', 'sqlite')
SQL_OUT_PUT_LAST_LINES_START ('RandomReg_100_medium', 'MLPRegressor', 'sqlite')
-0.275788 * t."OUT_4"  + 0.022317 * t."OUT_5"  + 0.276608 * t."OUT_6"  + -0.631406 * t."OUT_7" AS "OUT_5"
   FROM "Hidden_Layer_2_Activation" AS t
 ),
"Hidden_Layer_3_Activation" AS
 ( SELECT
    t."index" as "index",
    CASE WHEN (t."OUT_0" > 0) THEN t."OUT_0" ELSE 0 END AS "OUT_0",
    CASE WHEN (t."OUT_1" > 0) THEN t."OUT_1" ELSE 0 END AS "OUT_1",
    CASE WHEN (t."OUT_2" > 0) THEN t."OUT_2" ELSE 0 END AS "OUT_2",
    CASE WHEN (t."OUT_3" > 0) THEN t."OUT_3" ELSE 0 END AS "OUT_3",
    CASE WHEN (t."OUT_4" > 0) THEN t."OUT_4" ELSE 0 END AS "OUT_4",
    CASE WHEN (t."OUT_5" > 0) THEN t."OUT_5" ELSE 0 END AS "OUT_5"
   FROM "Hidden_Layer_3_BA" AS t
 ),
"Output_Layer_BA" AS
 ( SELECT
    t."index" as "index",
    -0.723294 + -0.570000 * t."OUT_0"  + 0.840942 * t."OUT_1"  + 0.453743 * t."OUT_2"  + 0.861368 * t."OUT_3"  + -0.717354 * t."OUT_4"  + 0.592693 * t."OUT_5" AS "OUT_0"
   FROM "Hidden_Layer_3_Activation" AS t
 )
 SELECT
    t."index" AS "index",
    t.OUT_0 AS "Estimator"
  FROM "Output_Layer_BA" AS t

SQL_OUT_PUT_LAST_LINES_END ('RandomReg_100_medium', 'MLPRegressor', 'sqlite') 




COPY_TRAINING_DATA_TO_SQLITE_START
<class 'pandas.core.frame.DataFrame'>
Index: 512 entries, 0 to 511
Data columns (total 100 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   X_0     512 non-null    float32
 1   X_1     512 non-null    float32
 2   X_2     512 non-null    float32
 3   X_3     512 non-null    float32
 4   X_4     512 non-null    float32
 5   X_5     512 non-null    float32
 6   X_6     512 non-null    float32
 7   X_7     512 non-null    float32
 8   X_8     512 non-null    float32
 9   X_9     512 non-null    float32
 10  X_10    512 non-null    float32
 11  X_11    512 non-null    float32
 12  X_12    512 non-null    float32
 13  X_13    512 non-null    float32
 14  X_14    512 non-null    float32
 15  X_15    512 non-null    float32
 16  X_16    512 non-null    float32
 17  X_17    512 non-null    float32
 18  X_18    512 non-null    float32
 19  X_19    512 non-null    float32
 20  X_20    512 non-null    float32
 21  X_21    512 non-null    float32
 22  X_22    512 non-null    float32
 23  X_23    512 non-null    float32
 24  X_24    512 non-null    float32
 25  X_25    512 non-null    float32
 26  X_26    512 non-null    float32
 27  X_27    512 non-null    float32
 28  X_28    512 non-null    float32
 29  X_29    512 non-null    float32
 30  X_30    512 non-null    float32
 31  X_31    512 non-null    float32
 32  X_32    512 non-null    float32
 33  X_33    512 non-null    float32
 34  X_34    512 non-null    float32
 35  X_35    512 non-null    float32
 36  X_36    512 non-null    float32
 37  X_37    512 non-null    float32
 38  X_38    512 non-null    float32
 39  X_39    512 non-null    float32
 40  X_40    512 non-null    float32
 41  X_41    512 non-null    float32
 42  X_42    512 non-null    float32
 43  X_43    512 non-null    float32
 44  X_44    512 non-null    float32
 45  X_45    512 non-null    float32
 46  X_46    512 non-null    float32
 47  X_47    512 non-null    float32
 48  X_48    512 non-null    float32
 49  X_49    512 non-null    float32
 50  X_50    512 non-null    float32
 51  X_51    512 non-null    float32
 52  X_52    512 non-null    float32
 53  X_53    512 non-null    float32
 54  X_54    512 non-null    float32
 55  X_55    512 non-null    float32
 56  X_56    512 non-null    float32
 57  X_57    512 non-null    float32
 58  X_58    512 non-null    float32
 59  X_59    512 non-null    float32
 60  X_60    512 non-null    float32
 61  X_61    512 non-null    float32
 62  X_62    512 non-null    float32
 63  X_63    512 non-null    float32
 64  X_64    512 non-null    float32
 65  X_65    512 non-null    float32
 66  X_66    512 non-null    float32
 67  X_67    512 non-null    float32
 68  X_68    512 non-null    float32
 69  X_69    512 non-null    float32
 70  X_70    512 non-null    float32
 71  X_71    512 non-null    float32
 72  X_72    512 non-null    float32
 73  X_73    512 non-null    float32
 74  X_74    512 non-null    float32
 75  X_75    512 non-null    float32
 76  X_76    512 non-null    float32
 77  X_77    512 non-null    float32
 78  X_78    512 non-null    float32
 79  X_79    512 non-null    float32
 80  X_80    512 non-null    float32
 81  X_81    512 non-null    float32
 82  X_82    512 non-null    float32
 83  X_83    512 non-null    float32
 84  X_84    512 non-null    float32
 85  X_85    512 non-null    float32
 86  X_86    512 non-null    float32
 87  X_87    512 non-null    float32
 88  X_88    512 non-null    float32
 89  X_89    512 non-null    float32
 90  X_90    512 non-null    float32
 91  X_91    512 non-null    float32
 92  X_92    512 non-null    float32
 93  X_93    512 non-null    float32
 94  X_94    512 non-null    float32
 95  X_95    512 non-null    float32
 96  X_96    512 non-null    float32
 97  X_97    512 non-null    float32
 98  X_98    512 non-null    float32
 99  X_99    512 non-null    float32
dtypes: float32(100)
memory usage: 204.0 KB
            X_0       X_1       X_2  ...      X_97      X_98      X_99
index                                ...                              
0      1.331256 -0.890219  0.415263  ...  0.066743 -1.017662  0.850023
1      0.101475  1.673349  0.646130  ... -0.967804 -0.082562 -0.235254
2      1.143388 -0.121749  1.763748  ... -0.006825 -1.210605  0.186911
3     -0.664377  0.702461 -2.030148  ...  1.036630 -1.509694  1.665004
4     -0.183233  0.167641 -1.018050  ...  0.131021 -0.518277 -0.467736
...         ...       ...       ...  ...       ...       ...       ...
507   -0.085407  0.635354 -0.165095  ...  0.566512 -0.632373 -2.073853
508    0.689048 -0.025340 -0.142676  ... -0.129965 -0.697890  0.794571
509   -0.929089 -0.892634  1.709303  ...  1.169236 -2.478970 -0.503891
510    1.078776 -0.630185  0.944048  ... -0.346390 -0.233485 -0.721341
511    0.057106  0.428869 -0.742754  ... -0.777635 -0.139843 -0.534295

[512 rows x 100 columns]
COPY_TRAINING_DATA_TO_SQLITE_END


PREDICT_MODEL_ON_TRAINING_DATA_INSIDE_SQLITE_START
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 512 entries, 0 to 511
Data columns (total 2 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   index      512 non-null    int64  
 1   Estimator  512 non-null    float64
dtypes: float64(1), int64(1)
memory usage: 8.1 KB
     index  Estimator
0        0  -1.488781
1        1  -2.047496
2        2  -0.793683
3        3  -0.127088
4        4  -0.061222
..     ...        ...
507    507  -0.586425
508    508  -0.056784
509    509  -2.014762
510    510  -0.463295
511    511  -3.516266

[512 rows x 2 columns]
PREDICT_MODEL_ON_TRAINING_DATA_INSIDE_SQLITE_END


COMPARE_SQL_OUTPUT_AND_PYTHON_OUTPUT_START
Index(['index', 'Estimator'], dtype='object')
MLLITE_REG_SQL_L1_ERROR ('RandomReg_100_medium', 'MLPRegressor') Estimator 1.3762950478804232e-06
     index  SQL_Estimator  Py_Estimator     SQL_Error
496    496       0.285248      0.285249 -4.342418e-07
497    497       0.207440      0.207442 -1.254466e-06
498    498      -3.113256     -3.113256  2.492677e-07
499    499      -0.607899     -0.607898 -1.177640e-06
500    500      -0.124955     -0.124955 -6.038141e-07
501    501      -0.155543     -0.155547  3.833782e-06
502    502      -1.017535     -1.017538  3.340820e-06
503    503       0.464009      0.464008  1.088287e-06
504    504      -0.504567     -0.504568  1.309000e-06
505    505      -0.054809     -0.054808 -1.018025e-06
506    506      -0.640271     -0.640271  3.784913e-07
507    507      -0.586425     -0.586424 -1.731672e-06
508    508      -0.056784     -0.056785  6.248342e-07
509    509      -2.014762     -2.014762 -3.625328e-07
510    510      -0.463295     -0.463296  8.882809e-07
511    511      -3.516266     -3.516268  2.070020e-06
MLLITE_REG_SQL_EXECUTION_STATUS ('RandomReg_100_medium', 'MLPRegressor', 'Success')
COMPARE_SQL_OUTPUT_AND_PYTHON_OUTPUT_END
WRITING_SQL_CODE 'logs/auto_tests/regression/MLPRegressor/mllite.MLPRegressor_RandomReg_100_medium_option_1_pgsql.sql'



SQL_OUT_PUT_FIRST_LINES_START ('RandomReg_100_medium', 'MLPRegressor', 'pgsql')
WITH model_input AS 
 (SELECT "ADS"."index" AS "index",
    CAST("ADS"."X_0" AS FLOAT) AS "X_0", CAST("ADS"."X_1" AS FLOAT) AS "X_1", CAST("ADS"."X_2" AS FLOAT) AS "X_2", CAST("ADS"."X_3" AS FLOAT) AS "X_3", CAST("ADS"."X_4" AS FLOAT) AS "X_4", CAST("ADS"."X_5" AS FLOAT) AS "X_5", CAST("ADS"."X_6" AS FLOAT) AS "X_6", CAST("ADS"."X_7" AS FLOAT) AS "X_7", CAST("ADS"."X_8" AS FLOAT) AS "X_8", CAST("ADS"."X_9" AS FLOAT) AS "X_9", CAST("ADS"."X_10" AS FLOAT) AS "X_10", CAST("ADS"."X_11" AS FLOAT) AS "X_11", CAST("ADS"."X_12" AS FLOAT) AS "X_12", CAST("ADS"."X_13" AS FLOAT) AS "X_13", CAST("ADS"."X_14" AS FLOAT) AS "X_14", CAST("ADS"."X_15" AS FLOAT) AS "X_15", CAST("ADS"."X_16" AS FLOAT) AS "X_16", CAST("ADS"."X_17" AS FLOAT) AS "X_17", CAST("ADS"."X_18" AS FLOAT) AS "X_18", CAST("ADS"."X_19" AS FLOAT) AS "X_19", CAST("ADS"."X_20" AS FLOAT) AS "X_20", CAST("ADS"."X_21" AS FLOAT) AS "X_21", CAST("ADS"."X_22" AS FLOAT) AS "X_22", CAST("ADS"."X_23" AS FLOAT) AS "X_23", CAST("ADS"."X_24" AS FLOAT) AS "X_24", CAST("ADS
SQL_OUT_PUT_FIRST_LINES_END ('RandomReg_100_medium', 'MLPRegressor', 'pgsql')
SQL_OUT_PUT_LAST_LINES_START ('RandomReg_100_medium', 'MLPRegressor', 'pgsql')
-0.275788 * t."OUT_4"  + 0.022317 * t."OUT_5"  + 0.276608 * t."OUT_6"  + -0.631406 * t."OUT_7" AS "OUT_5"
   FROM "Hidden_Layer_2_Activation" AS t
 ),
"Hidden_Layer_3_Activation" AS
 ( SELECT
    t."index" as "index",
    CASE WHEN (t."OUT_0" > 0) THEN t."OUT_0" ELSE 0 END AS "OUT_0",
    CASE WHEN (t."OUT_1" > 0) THEN t."OUT_1" ELSE 0 END AS "OUT_1",
    CASE WHEN (t."OUT_2" > 0) THEN t."OUT_2" ELSE 0 END AS "OUT_2",
    CASE WHEN (t."OUT_3" > 0) THEN t."OUT_3" ELSE 0 END AS "OUT_3",
    CASE WHEN (t."OUT_4" > 0) THEN t."OUT_4" ELSE 0 END AS "OUT_4",
    CASE WHEN (t."OUT_5" > 0) THEN t."OUT_5" ELSE 0 END AS "OUT_5"
   FROM "Hidden_Layer_3_BA" AS t
 ),
"Output_Layer_BA" AS
 ( SELECT
    t."index" as "index",
    -0.723294 + -0.570000 * t."OUT_0"  + 0.840942 * t."OUT_1"  + 0.453743 * t."OUT_2"  + 0.861368 * t."OUT_3"  + -0.717354 * t."OUT_4"  + 0.592693 * t."OUT_5" AS "OUT_0"
   FROM "Hidden_Layer_3_Activation" AS t
 )
 SELECT
    t."index" AS "index",
    t.OUT_0 AS "Estimator"
  FROM "Output_Layer_BA" AS t

SQL_OUT_PUT_LAST_LINES_END ('RandomReg_100_medium', 'MLPRegressor', 'pgsql') 




COPY_TRAINING_DATA_TO_SQLITE_START
