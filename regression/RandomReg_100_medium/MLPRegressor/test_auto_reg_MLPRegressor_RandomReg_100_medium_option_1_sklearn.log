          X_0       X_1       X_2  ...      X_98      X_99      target
0    1.331256 -0.890219  0.415263  ... -1.017662  0.850023   46.659290
1    0.101475  1.673349  0.646130  ... -0.082562 -0.235254   17.096290
2    1.143388 -0.121749  1.763748  ... -1.210605  0.186911 -214.157384
3   -0.664377  0.702461 -2.030148  ... -1.509694  1.665004  -45.132339
4   -0.183233  0.167641 -1.018050  ... -0.518277 -0.467736 -472.368048
..        ...       ...       ...  ...       ...       ...         ...
507 -0.085407  0.635354 -0.165095  ... -0.632373 -2.073853   44.160388
508  0.689048 -0.025340 -0.142676  ... -0.697890  0.794571   89.670862
509 -0.929090 -0.892634  1.709303  ... -2.478970 -0.503891    7.769367
510  1.078776 -0.630185  0.944048  ... -0.233485 -0.721341  -26.929418
511  0.057106  0.428869 -0.742754  ... -0.139843 -0.534295 -333.558668

[512 rows x 101 columns]
SKLEARN_MODEL_SET_OPTIONS MLPRegressor {"hidden_layer_sizes" : [4, 8, 6]}
('OPERATION_START', 'TRAINING')
[[ 1.3312556  -0.8902193   0.41526318  0.788659   -0.7130887   1.4502782
  -0.90123427  0.7631844   0.13805741  1.6139201  -0.38690123  0.03777119
  -1.1557323  -0.5858095   1.6626205   0.68594664  0.39176562 -0.2828408
   0.76795787  0.3757667  -2.7420852  -0.01939904  0.40063587  0.17057581
  -0.44771507 -1.2317778  -0.8443254   0.7879428  -0.58345294  1.3596091
   0.6738356   1.2624362  -0.65831184 -0.88525283  0.04143903 -1.1881486
   0.83240134  0.49906567  0.11895992 -0.8198342   0.57035816 -1.4580479
  -0.7348064  -0.4322117   1.670222    0.49406192  0.47490185  0.86712915
   0.06314228 -2.2807853   0.7397412   0.7191424  -0.25634202 -0.75440276
   0.6664157   1.004504   -0.7802194   1.2289945  -0.3151011  -0.52080745
   2.0017505  -0.65958494 -0.06995643  1.3200244  -2.0877314  -2.0024903
  -0.02926418  1.5691092   0.85446763  0.5201753  -0.8084577   1.7062956
   0.8540257  -0.4603799  -0.06352598 -0.8714725  -0.5609252  -0.3273548
   0.74484634 -1.6906458  -0.33099878  0.06832191  1.3731502   0.84443164
   0.45396343  0.7861103   0.26812124 -0.06078732 -1.0477233   1.4219131
   1.2072196   0.7279877  -0.68208903 -1.8471589   0.77888066 -0.9317782
   0.2455851   0.06674263 -1.017662    0.8500229 ]
 [ 0.10147522  1.6733488   0.6461298  -1.2563368   1.1040853  -1.4227151
  -0.6017069  -0.9161108   0.16968192 -1.2134422   0.2153484  -0.08051679
   1.2387321  -0.48785964  0.27010813  1.5061171  -1.2551115  -1.1685889
   0.85681057 -0.3032395   0.34517288 -2.404879   -0.5924876  -0.11890611
  -1.1995318  -2.3414571   0.2762635  -4.5431676  -0.53269297  0.8078485
  -1.5695755  -1.10812     0.8487407  -0.2972983   0.58910424  0.82803386
   1.6629032  -0.71414495  1.0407996   0.00796571 -0.5988979  -1.67421
  -1.17802    -0.22742864  2.2339568   0.3043066   0.37049994 -1.5326284
  -0.64369285  0.96310157 -0.85642195  1.1327137  -1.4508079  -0.13960138
   1.4753928  -0.45546088  0.90462786  1.6403875   0.37488905  1.2000089
   1.3891938  -0.12287517 -0.7553856   0.8262001  -0.0514452   1.0170072
  -0.67795897  0.09011087  0.2844104   0.6818215  -0.8403749  -0.68216765
  -0.05586801  0.47741953 -0.56251854 -0.308212    0.13523772 -0.60912365
   0.6620477   0.10418119 -0.9861369   0.09003809 -2.009931   -0.989154
   0.5694446  -0.7199399  -1.0182965  -0.67762125  1.3876666  -1.4770343
   1.3187295   0.10140626 -0.21362238 -2.1321723  -0.9681868  -1.3000042
  -0.23745911 -0.96780366 -0.08256175 -0.2352544 ]
 [ 1.1433884  -0.12174941  1.763748    0.1567726  -1.074581    0.3090569
  -0.59582543  0.75060105  1.2144792   1.2156659   0.5765553  -0.4355099
   0.2547612  -0.9373168   1.2934183  -0.13537018  0.74200237  0.43883675
  -0.6069965   0.1554797   0.5401852   1.1975131  -1.1406852  -0.20755301
  -0.31834334 -0.6807571   0.24679533  1.5937762   0.1117221   0.80259067
  -1.2313136   0.917576    0.50131375 -0.5743615  -0.17214447 -2.0382028
   0.39552817  2.1280468   1.0293733  -0.02239049 -0.67557096 -1.5386295
   1.4090686  -0.59869367  0.6779711   0.37315732 -0.3413207  -0.21079636
   0.34230128  1.0776794   0.30119348 -0.24192244  0.23708598 -2.070677
  -0.48128286  0.3670155   1.0218917  -0.82419974  0.02379109  0.3317229
   1.5780679  -1.4464464  -1.6030818  -1.4349504  -0.11372316 -0.05236021
   0.37578964  1.7915944  -0.87417036  0.19483742 -0.11213797  0.07997598
  -0.89980906 -0.26016855  0.50538015 -1.2370168   1.1031808  -0.8198793
  -1.211      -0.5554547  -0.3449861   0.52403086 -0.2534899   1.0931107
   0.6548259   0.05409036 -0.2681419  -0.06177664  0.09527746  0.8096554
  -0.2925497   0.3124124  -0.37333974  1.0691838  -1.405716   -2.0550418
  -0.42661822 -0.00682518 -1.2106051   0.18691105]
 [-0.6643772   0.70246065 -2.0301476  -2.0342605   0.87254155  0.8668411
  -0.26858473 -0.73312426  1.0275607  -0.7275404   2.1678174  -1.1313968
   0.40187567 -0.71089333  0.2914241  -0.7301117  -1.095154   -2.268191
  -0.4189619  -0.03287303 -1.6552861   0.06898359 -1.912857   -0.55441886
   0.5538053   1.8425944   0.4861901   0.15487543 -0.10841709 -1.1439068
   0.09484072  0.54181457  0.55301255 -0.16916783 -1.6026491   0.4181876
   1.2507617   0.34678614 -1.1298385   2.87664     0.13642831 -0.22074103
  -1.260425    0.59103775  0.71680933 -1.1945326   0.18375565  0.6792565
  -1.1802012   1.2635881   0.08254713 -1.2404718   0.6709144  -0.07065742
  -0.2673846   0.35829198 -0.9844122  -0.8540105   0.54115516  0.78829515
   0.6353548  -0.6730554  -0.280788   -1.2170262   0.28105965 -1.9465499
   0.2613469  -0.85210556 -0.43554437 -0.4999505   1.1504256  -0.28339145
   0.96043223  0.3302896   0.04703231  0.7185348  -0.7703047  -1.8150443
  -0.29500493  0.47207424  1.062915   -0.8516409   0.34324142 -0.0487177
   0.5544652   0.7159857   0.26749176 -0.28686863 -0.10642244  2.2316606
  -1.3776829  -0.18294522 -1.7351075  -0.15755087  1.2045766   0.8150365
  -0.62745374  1.0366299  -1.5096936   1.665004  ]
 [-0.18323281  0.16764143 -1.0180501  -1.1114113  -0.6096793  -1.203273
  -0.91351366 -1.728868    1.45258    -0.7503452  -0.9816971   2.4864185
   0.2919757   0.7608395   0.02673802  0.01965859  1.7714319   0.10731484
   1.5014844   0.04669442  0.01499307  2.2903414  -1.5327556   0.2251275
   0.13013346 -1.3958306   0.477327   -2.448051   -1.034129   -0.36448112
  -0.27540234 -0.0670136  -0.41763452  1.1593167   0.36466834  0.51342684
  -1.9805648  -1.26423     0.1809458   0.09143253 -0.7548743   0.2509495
  -0.38412192 -1.5486463  -0.01736598  1.3488103  -2.0970352   0.540929
  -0.69701564  0.10016023 -0.34506392  0.5776158   2.3233109  -1.0692806
  -0.67232996 -0.8802096   0.67767256 -0.98172724  1.5671804  -1.077151
   0.6932878  -0.29901332  0.7109348  -0.6178621   0.13290192 -1.3579625
   0.8258206   0.2063345   0.6937211  -0.23165497  1.6014408  -0.33389077
  -0.52463293  1.2932607   0.24274994 -0.84197    -1.8947318  -0.31777015
   0.011854    1.0398142   1.1872678   0.5696282   0.77042353  0.04953871
  -1.2782155  -0.57868254  0.8079945   1.9365364   1.0325052   0.6288556
  -1.8749372   0.49370095  0.64584726  0.73780465  0.23192818 -1.2330554
   0.8333075   0.13102144 -0.5182774  -0.46773615]] [  46.65929   17.09629 -214.15738  -45.13234 -472.36804]
('OPERATION_END_ELAPSED', 0.125, 'TRAINING')
CONVERT_MODEL  <class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
BEAUTIFIED_JSON_START
{
	"layers" : 	{
		"Layer_0" : 	{
			"NbInputs" : 0,
			"NbOutputs" : 100,
			"intercepts" : [  ],
			"name" : "Input_Layer"
		},
		"Layer_1" : 	{
			"NbInputs" : 100,
			"NbOutputs" : 4,
			"coeffs_00" : [ -0.052528634667396545, -0.0850432738661766, 0.07569553703069687, 0.12196899950504303 ],
			"coeffs_01" : [ 0.14628568291664124, 0.2449178695678711, 0.26786354184150696, -0.023167923092842102 ],
			"coeffs_02" : [ 0.11676356196403503, -0.06161104515194893, 0.1274397075176239, -0.1063641756772995 ],
			"coeffs_03" : [ -0.06415395438671112, -0.1758146733045578, -0.11067946255207062, -0.2266978919506073 ],
			"coeffs_04" : [ -0.06413788348436356, -0.005313058849424124, -0.19138479232788086, -0.14229276776313782 ],
			"coeffs_05" : [ -0.16620637476444244, 0.0005372129380702972, -0.1403304487466812, 0.10977690666913986 ],
			"coeffs_06" : [ 0.1146828904747963, 0.1340261995792389, 0.06277057528495789, -0.1095542162656784 ],
			"coeffs_07" : [ 0.1456642895936966, 0.05151360481977463, -0.2829274833202362, 0.07738430052995682 ],
			"coeffs_08" : [ -0.25644826889038086, -0.13816224038600922, 0.27969953417778015, -0.008043099194765091 ],
			"coeffs_09" : [ 0.07302253693342209, -0.1168932244181633, 0.023080961778759956, 0.20222525298595428 ],
			"coeffs_10" : [ 0.11674058437347412, 0.2295214831829071, -0.13072654604911804, 0.027725324034690857 ],
			"coeffs_11" : [ -0.18678168952465057, -0.11052925884723663, -0.05685828626155853, -0.10021531581878662 ],
			"coeffs_12" : [ 0.0753558799624443, -0.08509927242994308, 0.14944985508918762, -0.10616464167833328 ],
			"coeffs_13" : [ 0.22465714812278748, 0.18629029393196106, -0.08734734356403351, 0.05071470886468887 ],
			"coeffs_14" : [ 0.1363210380077362, 0.12011133879423141, 0.12117864936590195, -0.007777308113873005 ],
			"coeffs_15" : [ 0.20096778869628906, -0.1838388890028, -0.2445458471775055, 0.0014772071735933423 ],
			"coeffs_16" : [ 0.16271047294139862, -0.161344975233078, 0.29618215560913086, -0.16940680146217346 ],
			"coeffs_17" : [ -0.1453600972890854, -0.03477736562490463, -0.1353902965784073, -0.21927496790885925 ],
			"coeffs_18" : [ -0.10226044803857803, 0.004177783150225878, -0.0641438290476799, -0.026822738349437714 ],
			"coeffs_19" : [ -0.10730228573083878, 0.2061166614294052, -0.2554955780506134, 0.05943679064512253 ],
			"coeffs_20" : [ 0.16719521582126617, 0.22448520362377167, -0.09345301240682602, -0.18914806842803955 ],
			"coeffs_21" : [ -0.10295917838811874, 0.11208739876747131, 0.09010791033506393, -0.01002331543713808 ],
			"coeffs_22" : [ 0.3010958433151245, -0.023744257166981697, 0.012981140054762363, -0.096686452627182 ],
			"coeffs_23" : [ 0.11595327407121658, -0.12618134915828705, 0.17983272671699524, -0.055062200874090195 ],
			"coeffs_24" : [ 0.1515246033668518, 0.09017370641231537, -0.1890573799610138, -0.09768375754356384 ],
			"coeffs_25" : [ -0.06786248087882996, -0.12387368083000183, 0.14031480252742767, -0.05786799639463425 ],
			"coeffs_26" : [ 0.118235282599926, 0.03646020218729973, 0.021745603531599045, -0.15564848482608795 ],
			"coeffs_27" : [ -0.16813895106315613, 0.11225848644971848, -0.10689080506563187, 0.20438331365585327 ],
			"coeffs_28" : [ -0.12504282593727112, 0.1299649477005005, -0.22581729292869568, 0.23929788172245026 ],
			"coeffs_29" : [ 0.006463969126343727, 0.061573147773742676, -0.036124471575021744, -0.1809922754764557 ],
			"coeffs_30" : [ 0.0696907639503479, 0.19361191987991333, -0.0632455125451088, -0.24952980875968933 ],
			"coeffs_31" : [ 0.12150994688272476, -0.24345999956130981, 0.1105380728840828, -0.058313384652137756 ],
			"coeffs_32" : [ 0.1832417994737625, 0.09671247750520706, -0.0380387082695961, 0.07523048669099808 ],
			"coeffs_33" : [ 0.13916797935962677, 0.049090299755334854, -0.00812884233891964, 0.17731526494026184 ],
			"coeffs_34" : [ 0.2688056528568268, 0.05811497941613197, -0.02042536251246929, -0.10833101719617844 ],
			"coeffs_35" : [ 0.06668232381343842, 0.05275406315922737, 0.11435626447200775, 0.174345925450325 ],
			"coeffs_36" : [ 0.17344720661640167, 0.11841889470815659, -0.18418245017528534, -0.2555970251560211 ],
			"coeffs_37" : [ -0.3161231279373169, -0.08111230283975601, 0.13570085167884827, -0.07753732800483704 ],
			"coeffs_38" : [ -0.16839414834976196, -0.006705132313072681, -0.1330806314945221, 0.1594470739364624 ],
			"coeffs_39" : [ 0.20505701005458832, 0.19950099289417267, 0.013865943998098373, -0.2056691199541092 ],
			"coeffs_40" : [ -0.10391737520694733, -0.19814257323741913, 0.08126682043075562, 0.12167039513587952 ],
			"coeffs_41" : [ -0.051868196576833725, -0.21698778867721558, 0.1230454370379448, -0.12373009324073792 ],
			"coeffs_42" : [ 0.2173445224761963, -0.06833932548761368, 0.01738469861447811, -0.08555042743682861 ],
			"coeffs_43" : [ -0.14552487432956696, -0.012151266448199749, -0.10849310457706451, 0.2675988972187042 ],
			"coeffs_44" : [ -0.15503361821174622, -0.2068978250026703, -0.038306038826704025, -0.21845030784606934 ],
			"coeffs_45" : [ -0.21538829803466797, 0.1645907312631607, 0.2412591278553009, 0.013988182879984379 ],
			"coeffs_46" : [ 0.22411483526229858, -0.05239051580429077, -0.3357603847980499, -0.32237327098846436 ],
			"coeffs_47" : [ -0.19464196264743805, 0.18419486284255981, -0.08785336464643478, 0.11098077893257141 ],
			"coeffs_48" : [ -0.14451824128627777, 0.015622096136212349, 0.056448690593242645, 0.11372993886470795 ],
			"coeffs_49" : [ -0.09605057537555695, -0.1403815895318985, -0.24591396749019623, 0.10754454880952835 ],
			"coeffs_50" : [ -0.11026361584663391, 0.12342271208763123, -0.20295488834381104, 0.19558513164520264 ],
			"coeffs_51" : [ 0.11860036849975586, -0.07888253033161163, 0.05721821263432503, -0.01566954329609871 ],
			"coeffs_52" : [ -0.12460651993751526, -0.036877088248729706, 0.27615129947662354, -0.1303265392780304 ],
			"coeffs_53" : [ 0.17121756076812744, -0.10282856971025467, -0.12568509578704834, -0.1821531355381012 ],
			"coeffs_54" : [ 0.008629927411675453, -0.005647873505949974, -0.10777152329683304, 0.1004132404923439 ],
			"coeffs_55" : [ 0.07161622494459152, 0.13386376202106476, -0.09972243756055832, 0.18287290632724762 ],
			"coeffs_56" : [ 0.07752839475870132, 0.19463931024074554, -0.03403686732053757, 0.24671342968940735 ],
			"coeffs_57" : [ 0.06909891217947006, 0.03515117987990379, -0.1118479073047638, 0.05351442098617554 ],
			"coeffs_58" : [ -0.09365115314722061, 0.23890624940395355, -0.08866801857948303, -0.06244801729917526 ],
			"coeffs_59" : [ 0.053486168384552, 0.276721715927124, -0.05459034815430641, -0.07775028049945831 ],
			"coeffs_60" : [ -0.11275980621576309, 0.19307762384414673, 0.10727626085281372, -0.03451491892337799 ],
			"coeffs_61" : [ -0.0031371128279715776, -0.014189883135259151, 0.2902587056159973, -0.043484609574079514 ],
			"coeffs_62" : [ -0.15344734489917755, -0.1785857230424881, 0.13908278942108154, -0.16994167864322662 ],
			"coeffs_63" : [ -0.25739189982414246, -0.08788235485553741, -0.03924648091197014, 0.14962807297706604 ],
			"coeffs_64" : [ -0.004310299642384052, -0.04270007088780403, -0.11784739047288895, 0.046268247067928314 ],
			"coeffs_65" : [ 0.17355160415172577, -0.0940755307674408, 0.19879239797592163, 0.13452091813087463 ],
			"coeffs_66" : [ -0.049976199865341187, -0.18776310980319977, 0.023266207426786423, -0.020352931693196297 ],
			"coeffs_67" : [ 0.11443526297807693, -0.09692258387804031, 0.07344646006822586, 0.21060208976268768 ],
			"coeffs_68" : [ -0.11028031259775162, 0.21028414368629456, -0.16306918859481812, -0.1925193965435028 ],
			"coeffs_69" : [ -0.07879337668418884, -0.12000542134046555, 0.049656350165605545, -0.1545628309249878 ],
			"coeffs_70" : [ 0.06929559260606766, -0.06565442681312561, 0.16240927577018738, -0.08291992545127869 ],
			"coeffs_71" : [ 0.07373684644699097, 0.017172396183013916, -0.09290921688079834, 0.120767742395401 ],
			"coeffs_72" : [ 0.03897177428007126, -0.10948055237531662, -0.10171480476856232, -0.03844509646296501 ],
			"coeffs_73" : [ 0.12075489014387131, -0.23879115283489227, -0.027682369574904442, -0.06599721312522888 ],
			"coeffs_74" : [ -0.1706240028142929, 0.2348800003528595, 0.03386542573571205, -0.08730052411556244 ],
			"coeffs_75" : [ 0.23343202471733093, -0.12951023876667023, 0.15168894827365875, -0.27832964062690735 ],
			"coeffs_76" : [ 0.1055896058678627, 0.14378449320793152, -0.03582917898893356, 0.042878612875938416 ],
			"coeffs_77" : [ 0.04760972037911415, -0.06339976191520691, -0.23603619635105133, -0.2678486704826355 ],
			"coeffs_78" : [ -0.038841888308525085, 0.09387370944023132, 0.1025751605629921, -0.13896550238132477 ],
			"coeffs_79" : [ 0.055401552468538284, -0.2672037184238434, 0.04729895666241646, -0.10154683142900467 ],
			"coeffs_80" : [ -0.21732348203659058, 0.25163182616233826, 0.07168172299861908, 0.21492613852024078 ],
			"coeffs_81" : [ 0.07885070145130157, -0.09680479019880295, -0.10038170963525772, 0.2275116741657257 ],
			"coeffs_82" : [ -0.07754460722208023, -0.10177087038755417, 0.04689249023795128, 0.2761487364768982 ],
			"coeffs_83" : [ -0.010226818732917309, 0.13572965562343597, 0.16734068095684052, -0.1624622642993927 ],
			"coeffs_84" : [ -0.12416722625494003, -0.10547063499689102, -0.047592781484127045, -0.0430322103202343 ],
			"coeffs_85" : [ 0.24649789929389954, -0.25099095702171326, -0.018573807552456856, 0.022043319419026375 ],
			"coeffs_86" : [ 0.1714203953742981, -0.10264859348535538, 0.20923832058906555, 0.04333138093352318 ],
			"coeffs_87" : [ -0.2632574737071991, -0.12456393986940384, -0.17857015132904053, -0.10387246310710907 ],
			"coeffs_88" : [ -0.08802391588687897, -0.04182933643460274, 0.10868008434772491, 0.198713019490242 ],
			"coeffs_89" : [ -0.11000438034534454, -0.22436507046222687, 0.00020698830485343933, -0.10798782855272293 ],
			"coeffs_90" : [ 0.042756587266922, -0.05490658059716225, -0.10676486045122147, -0.2569858431816101 ],
			"coeffs_91" : [ 0.19375614821910858, -0.016312995925545692, -0.19718214869499207, -0.12059193849563599 ],
			"coeffs_92" : [ 0.03818901255726814, -0.11454253643751144, -0.08748950064182281, 0.017864922061562538 ],
			"coeffs_93" : [ -0.08561526983976364, -0.1651686131954193, -0.05811342969536781, -0.00818518828600645 ],
			"coeffs_94" : [ -0.13653422892093658, -0.03917611390352249, 0.13503021001815796, -0.19964979588985443 ],
			"coeffs_95" : [ -0.07389094680547714, 0.12877999246120453, -0.023223338648676872, -0.11864402890205383 ],
			"coeffs_96" : [ 0.10254458338022232, 0.12010897696018219, 0.1310059279203415, -0.14621849358081818 ],
			"coeffs_97" : [ 0.04481751471757889, 0.22979900240898132, -0.008331132121384144, -0.1719883233308792 ],
			"coeffs_98" : [ 0.0751049816608429, -0.13020475208759308, -0.22681604325771332, 0.21099531650543213 ],
			"coeffs_99" : [ -0.19038020074367523, -0.18585753440856934, 0.15190668404102325, 0.1461295336484909 ],
			"intercepts" : [ 0.1967272013425827, -0.15004751086235046, 0.26956096291542053, -0.058009982109069824 ],
			"name" : "Hidden_Layer_1"
		},
		"Layer_2" : 	{
			"NbInputs" : 4,
			"NbOutputs" : 8,
			"coeffs_0" : [ -0.12500208616256714, -0.6450313329696655, -0.4151840806007385, 0.6335968375205994, 0.3694603443145752, -0.1498764306306839, 0.5264642834663391, -0.5587512254714966 ],
			"coeffs_1" : [ -0.5950615406036377, 0.07777189463376999, 0.43271568417549133, 0.1582520604133606, 0.5407078266143799, -0.09059547632932663, -0.0009149236138910055, -0.3552625775337219 ],
			"coeffs_2" : [ 0.4796757102012634, 0.2942834794521332, 0.7265302538871765, -0.3304626941680908, -0.2906523644924164, 0.020452719181776047, 0.20549656450748444, -0.4997192621231079 ],
			"coeffs_3" : [ -0.004458058625459671, 0.36066901683807373, -0.5045595765113831, -0.3308335840702057, -0.32480981945991516, 0.743134617805481, -0.07522054761648178, -0.4596202075481415 ],
			"intercepts" : [ 0.585776150226593, -0.5317262411117554, 0.20239950716495514, 0.7562242746353149, -0.6250683665275574, 0.0508248507976532, 0.4500223398208618, 0.6929497718811035 ],
			"name" : "Hidden_Layer_2"
		},
		"Layer_3" : 	{
			"NbInputs" : 8,
			"NbOutputs" : 6,
			"coeffs_0" : [ -0.11426517367362976, -0.397664874792099, -0.488812655210495, -0.3075244724750519, 0.3031080365180969, 0.644354522228241 ],
			"coeffs_1" : [ 0.260299950838089, -0.43314558267593384, -0.5100297331809998, 0.3855851888656616, -0.357540488243103, 0.353014200925827 ],
			"coeffs_2" : [ 0.11794352531433105, -0.01600092276930809, -0.252936989068985, 0.29641610383987427, -0.6368688941001892, 0.32822656631469727 ],
			"coeffs_3" : [ 0.6540389657020569, 0.20932979881763458, 0.18016833066940308, 0.31027233600616455, 0.3615705668926239, -0.22797466814517975 ],
			"coeffs_4" : [ 0.14385166764259338, 0.1609519124031067, 0.33908724784851074, -0.22264155745506287, 0.24227167665958405, -0.5413968563079834 ],
			"coeffs_5" : [ -0.4079541265964508, 0.11439269036054611, 0.5125563740730286, -0.10755717754364014, -0.2905485928058624, -0.3866749703884125 ],
			"coeffs_6" : [ -0.5074973106384277, 0.17161737382411957, -0.704680860042572, 0.4699276089668274, 0.42135196924209595, 0.39993131160736084 ],
			"coeffs_7" : [ 0.07344090938568115, -0.4021202027797699, 0.5553035140037537, 0.6970899105072021, 0.1564135104417801, 0.02261708304286003 ],
			"intercepts" : [ 0.6703680157661438, -0.38172540068626404, -0.5247790217399597, -0.19506680965423584, 0.4797281324863434, -0.5216377973556519 ],
			"name" : "Hidden_Layer_3"
		},
		"Layer_4" : 	{
			"NbInputs" : 6,
			"NbOutputs" : 1,
			"coeffs_0" : [ 0.8166514039039612 ],
			"coeffs_1" : [ -0.15600642561912537 ],
			"coeffs_2" : [ 0.6165885925292969 ],
			"coeffs_3" : [ 0.8129311800003052 ],
			"coeffs_4" : [ 0.7390931248664856 ],
			"coeffs_5" : [ -0.5927435159683228 ],
			"intercepts" : [ -0.30965784192085266 ],
			"name" : "Output_Layer"
		},
		"sizes" : [ 100, 4, 8, 6, 1 ]
	},
	"metadata" :  { "model" : "sklearn.neural_network._multilayer_perceptron.MLPRegressor", "version" : "1.4.1.post1" },
	"options" :  { "activation" : "relu", "alpha" : 0.0001, "batch_size" : "auto", "beta_1" : 0.9, "beta_2" : 0.999, "early_stopping" : false, "epsilon" : 1e-08, "hidden_layer_sizes" : [ 4, 8, 6 ], "learning_rate" : "constant", "learning_rate_init" : 0.001, "max_fun" : 15000, "max_iter" : 32, "momentum" : 0.9, "n_iter_no_change" : 10, "nesterovs_momentum" : true, "power_t" : 0.5, "random_state" : 1789, "shuffle" : false, "solver" : "adam", "tol" : 0.0001, "validation_fraction" : 0.1, "verbose" : false, "warm_start" : false }
}
BEAUTIFIED_JSON_END
('OPERATION_START', 'PREDICT')
[ 1.39226055e+00  3.95348001e+00 -1.53328136e-01  4.30833310e-01
 -1.36599743e+00  2.36271620e-02  1.34263587e+00 -1.04679930e+00
  9.02163982e-03  1.56951988e+00  8.91638756e-01  1.57423210e+00
  2.13129139e+00  7.73960948e-01  3.89145207e+00  1.74126840e+00
 -2.84723997e-01 -3.09657842e-01  1.14871204e+00  2.63174891e+00
  3.42234111e+00  3.13710898e-01  1.19889200e+00  3.72387171e-02
  8.00442457e-01  3.36628437e+00  3.55902642e-01  1.74126840e+00
 -6.09094620e-01 -6.67294979e-01 -1.00522327e+00  4.38625431e+00
 -4.98982698e-01  2.33612704e+00  9.48467016e-01  2.03129840e+00
  4.40278673e+00  3.86474997e-01  3.52088523e+00 -3.09657842e-01
 -7.19286919e-01 -8.64269495e-01  1.13171422e+00 -7.60943174e-01
 -8.72459292e-01 -2.41801813e-01  5.49066186e-01  2.56575495e-01
  1.64664865e+00 -3.09893310e-01  1.73153138e+00  3.91544461e+00
  1.73601079e+00  1.23741221e+00  4.98991013e+00 -5.94269753e-01
 -1.29804403e-01  1.47160876e+00 -7.93387115e-01 -1.22543365e-01
  1.98206520e+00  5.14427233e+00 -1.22780085e-01  7.50889778e-01
  1.13293386e+00  1.90817738e+00  2.04996085e+00  1.74126840e+00
  1.08908343e+00  1.74126840e+00  1.52243888e+00 -1.07259226e+00
  2.63969684e+00  1.96472597e+00  3.05048156e+00  4.56364542e-01
 -7.75597095e-01  1.02830887e+00  1.12497807e+00  6.55041695e-01
 -9.57242846e-02  2.79005694e+00  1.62417781e+00  1.56712115e-01
  3.29434037e+00 -3.09657842e-01  1.60272288e+00  1.57953119e+00
  1.55731833e+00 -2.85663813e-01  1.74126840e+00  2.22386718e+00
  2.77676553e-01  9.41192627e-01  1.74126840e+00  3.02902251e-01
  2.68795991e+00  1.35339844e+00  1.14809155e+00  5.96510530e-01
  1.74126840e+00 -1.17421657e-01 -3.09657842e-01  3.62254691e+00
 -2.99658358e-01  1.74126840e+00  5.24385810e-01 -4.62082028e-03
  3.68806154e-01  1.38554311e+00  2.19047904e+00 -8.72774601e-01
  3.44323540e+00  1.19076741e+00  2.36360955e+00  1.36505485e+00
  2.37935400e+00  1.43756700e+00  2.40817714e+00  1.79573202e+00
 -2.45251507e-01 -2.52700329e-01  6.17497325e-01  3.64784926e-01
 -6.56749785e-01  1.98919368e+00  1.27368784e+00 -3.21482301e-01
  8.44410181e-01  5.80333233e-01 -2.16611817e-01  1.90188456e+00
  4.49077755e-01  2.88517785e+00  3.00115395e+00 -4.19961929e-01
 -1.63946539e-01 -6.48513436e-02  1.29268372e+00  3.45912367e-01
  1.46350294e-01  5.99022508e-01  2.98554635e+00  1.72696471e+00
  1.73104358e+00  8.12460423e-01 -1.27760440e-01  1.73317409e+00
  1.99752402e+00  7.65455961e-01 -9.72400546e-01  1.74126840e+00
  3.56991321e-01  6.11425996e-01  5.60534596e-01  2.46991801e+00
  3.30222154e+00 -9.79358435e-01  3.04798245e+00  2.49226356e+00
  6.32810593e-01  2.54940486e+00  4.62413728e-02  1.85178494e+00
  2.14349222e+00  1.38572240e+00  2.01178461e-01  2.22604203e+00
  9.59214091e-01  8.37450027e-01  2.36325026e+00 -9.00490046e-01
  3.46697301e-01  4.59941775e-01  2.00201917e+00 -1.51894048e-01
 -4.13111180e-01  1.38602114e+00  3.36267054e-02  2.43385100e+00
  3.77523637e+00  2.09046125e+00  1.86419821e+00  1.86723852e+00
  1.34253573e+00  2.80452657e+00 -6.67578578e-01  3.88499469e-01
  1.10409379e+00  1.67789257e+00  4.26073933e+00  1.11845040e+00
  6.48427963e-01  2.61351919e+00 -9.98391062e-02  1.31371307e+00
 -8.96230221e-01 -5.80868959e-01  3.55641580e+00  2.64519364e-01
  1.74126840e+00  5.04168153e-01  5.38275337e+00  3.63592356e-01
  7.25516796e-01  4.32846934e-01  1.86222291e+00  3.49330992e-01
  1.50601184e+00  3.01433063e+00 -1.82691574e-01 -1.82771742e-01
  1.61087441e+00 -6.26424551e-01  1.15476727e+00 -3.09657842e-01
  2.81417847e+00  1.04332936e+00  2.10103846e+00  1.04177308e+00
  3.51628631e-01 -7.78847933e-01 -5.65591335e-01  3.75362277e+00
  1.87587428e+00 -6.30916417e-01  4.66767341e-01  2.01590747e-01
  1.54987824e+00  1.93660855e+00  2.97474217e+00  1.18502885e-01
  1.85920501e+00 -7.52944648e-02 -3.09657842e-01  1.03086185e+00
 -4.77839023e-01  2.24918818e+00  1.74126840e+00  1.66822791e+00
 -7.27685809e-01  3.48931938e-01  7.12995887e-01  1.65114820e-01
  1.68492436e+00  2.51783222e-01 -3.79807621e-01 -2.03395814e-01
  6.97911501e-01  2.36339760e+00 -5.85358858e-01  3.52966118e+00
  3.40598297e+00  2.29491878e+00 -3.09657842e-01  8.23096037e-01
  2.51937604e+00  2.14735675e+00  2.21589208e+00 -8.19540501e-01
  3.89590770e-01  7.15556383e-01  1.66126955e+00  1.52268136e+00
  9.53022480e-01 -8.37543249e-01  6.35592103e-01 -1.89528167e-01
  2.65278363e+00  1.63972998e+00  1.69574451e+00 -3.06934357e-01
  2.78258681e+00 -1.12086332e+00  1.74126840e+00  1.73739552e+00
  1.62839770e+00  7.26477861e-01  2.97693419e+00  1.73989797e+00
  2.58273768e+00  1.88570231e-01  1.74102950e+00  4.77360249e+00
 -4.23942626e-01 -1.13334000e+00 -5.29469848e-01 -9.07732725e-01
  1.22754359e+00  3.11658764e+00  2.62026572e+00  7.34727144e-01
  1.79637051e+00  3.43617940e+00  3.12264943e+00  1.13922989e+00
  8.07083249e-02  5.82848668e-01  2.66759372e+00  1.52849984e+00
  1.15154529e+00  1.73045802e+00  1.74520707e+00 -7.38608837e-01
  4.37083572e-01  1.98092437e+00  1.66625583e+00  1.94960237e+00
  8.34397674e-01  2.12657356e+00  5.42018318e+00  1.99074960e+00
  1.08741736e+00  2.45217395e+00  3.22905445e+00  2.37962890e+00
  3.46437263e+00  1.15570486e+00  1.36202645e+00  5.09796143e-02
  1.52018309e+00  2.39913416e+00  1.16200686e+00 -1.54231206e-01
 -5.14832079e-01  1.61284363e+00  1.23141527e+00  3.61943030e+00
 -8.78938556e-01  2.49260592e+00  3.45635700e+00  7.77215242e-01
  3.32157397e+00 -1.97001368e-01  3.89050317e+00  8.16766977e-01
  3.00657034e+00 -9.05943990e-01  1.23613524e+00  2.68590760e+00
  1.92073154e+00  1.07251674e-01  1.74126840e+00 -1.70957550e-01
  1.49753422e-01  1.33734655e+00  7.84000158e-01  3.62201631e-02
  1.74126840e+00  1.13358831e+00  1.40931344e+00  1.27770340e+00
 -4.34491038e-01  2.03335786e+00  3.25654626e+00  1.76074433e+00
  1.04754686e+00 -1.93934083e-01  2.25933409e+00  1.65144014e+00
  1.15948224e+00  3.40995222e-01  3.03732109e+00  2.77258539e+00
 -4.93588775e-01  5.05862474e-01  2.80551028e+00  1.37921822e+00
 -2.94002652e-01  1.61935377e+00  6.01988792e-01  1.74126840e+00
  3.09239888e+00  3.57250780e-01  2.19255179e-01 -6.08051062e-01
  5.09344959e+00 -8.44439626e-01  1.57287264e+00  1.74126840e+00
  1.80359674e+00 -9.36255693e-01  1.68245107e-01  2.53248262e+00
 -9.65980768e-01  9.70359564e-01  1.74126840e+00 -1.88858658e-01
  2.17050028e+00  9.27204847e-01  2.23557854e+00  2.79656553e+00
  1.63273084e+00  2.18131948e+00  3.15514207e+00  1.98391652e+00
  6.89270020e-01  9.55170393e-02  2.03387642e+00 -8.37428927e-01
  1.61763990e+00  1.74126840e+00  1.72728348e+00  2.11303520e+00
  8.68430138e-01  1.70394683e+00  9.83499050e-01  1.74126840e+00
 -2.50016600e-01  3.13950968e+00  1.74126840e+00  1.74126840e+00
 -6.67969465e-01 -1.31594032e-01 -1.32803249e+00 -9.27021652e-02
  4.33563262e-01 -4.51259494e-01  9.40239429e-01 -3.09657842e-01
  1.45353270e+00 -6.04381800e-01  2.31253219e+00  1.68773508e+00
  1.25964916e+00  9.78741884e-01  3.43351269e+00  1.31341887e+00
 -3.40785742e-01  1.42669940e+00  2.25578141e+00  1.77263570e+00
  1.96753788e+00  1.67291117e+00  3.04496551e+00  8.98797512e-01
  1.71629500e+00  6.12289071e-01 -7.85307050e-01 -5.94691157e-01
  1.74126840e+00  2.11355424e+00  1.18303740e+00  5.40594220e-01
  1.74126840e+00  1.97171283e+00 -8.37322950e-01  1.79274058e+00
  1.22184622e+00  1.73862576e+00  1.30944312e+00  1.98025274e+00
  6.38031721e-01 -9.28996563e-01  1.49687541e+00  2.17100054e-01
  2.84217811e+00  3.23283672e-02 -1.76072404e-01  1.49787295e+00
  1.74126840e+00  4.50349480e-01  1.70376039e+00  1.47719717e+00
  3.70008200e-01  7.79153407e-02 -9.23231125e-01  1.09503257e+00
  1.43515444e+00  7.83160329e-01 -8.85730386e-01  1.41831267e+00
  2.63450313e+00 -1.11959368e-01 -6.96467459e-01  1.74126840e+00
  1.80766988e+00  2.17982745e+00  1.76682448e+00  1.73931050e+00
 -3.09657842e-01  1.69811440e+00  4.98735309e-02  9.91510868e-01
 -4.90564108e-03 -2.99084514e-01 -2.20718861e-01 -1.04432821e-01
 -5.09655178e-02  1.77679873e+00  8.96533251e-01  1.51788282e+00
  1.74126840e+00  1.30845523e+00  5.48098326e-01  2.12775922e+00
  3.09054637e+00  2.21665645e+00  2.72345871e-01 -3.09657842e-01
  2.46889973e+00  2.11930871e+00 -1.98190808e-02  3.97832036e+00
 -6.97941780e-01 -5.02905428e-01  4.72064018e+00  2.99588895e+00
  1.32328176e+00  1.44765759e+00 -2.69892544e-01 -3.09657842e-01]
('OPERATION_END_ELAPSED', 0.007, 'PREDICT')
MODEL_PERFS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'RandomReg_100_medium', 'size': 512, 'mse': 26065.797, 'mae': 126.16521, 'mape': 0.9905047, 'r2': 0.009751150592219648}
WRITING_PERF_CODE 'logs/auto_tests/regression/MLPRegressor/sklearn.neural_network._multilayer_perceptron.MLPRegressor_RandomReg_100_medium_option_1.perf'

MODEL_PERFS_TIMINGS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'RandomReg_100_medium', 'training_time_in_sec': 0.125, 'prediction_time_in_sec': 0.007}
