         X_0       X_1       X_2  ...      X_98      X_99      target
0  -0.301125  1.387725  1.263659  ... -0.345798  0.839573  -54.486094
1  -0.155583 -1.181091  2.934304  ... -1.884572 -0.938959 -244.069750
2   1.079390 -0.496549 -1.240677  ... -0.569426  0.970424   50.603288
3  -1.027789 -0.022726  0.110301  ...  2.721471 -0.697773  200.907470
4   0.155319  0.126152  0.905780  ...  1.218228  0.672223 -138.968166
5  -1.133144 -1.651195 -0.342055  ... -2.102750  0.912526  150.012595
6   0.019270 -1.217387 -0.758529  ...  2.781903  1.876952    6.485013
7   0.158980 -1.225813 -1.678237  ...  0.246019  0.614807  128.961327
8   0.064271 -2.410516 -0.576757  ... -0.795053 -0.008973  -41.105676
9   0.652203  0.436045 -0.138883  ... -0.172713  0.919309  410.248003
10  1.188539 -1.250136 -1.027288  ...  0.531235 -1.526836  309.975525
11 -0.757753 -0.757876 -1.128427  ... -1.704143 -1.010355   92.184336
12  1.469914  1.519842  0.957997  ... -1.215914 -0.053610    2.776306
13  0.414327 -1.399507 -0.741872  ...  0.767883 -0.178207  -19.506459
14  0.348487 -1.384055  1.225923  ...  0.239461 -0.572437   64.722984
15  0.227229  0.619283 -1.530483  ... -1.305061  0.904966  -55.250985

[16 rows x 101 columns]
SKLEARN_MODEL_SET_OPTIONS MLPRegressor {"hidden_layer_sizes" : [4, 8, 6]}
('OPERATION_START', 'TRAINING')
[[-3.01124662e-01  1.38772476e+00  1.26365924e+00 -6.82732940e-01
  -1.34683120e+00  5.14777362e-01  1.62561178e-01 -1.08774674e+00
   2.71217942e-01  1.01093900e+00 -2.49382392e-01 -1.65061939e+00
  -9.93423760e-01  1.09921061e-01 -5.43385804e-01  1.32529700e+00
   1.88495958e+00  7.49242187e-01  1.60594404e+00  1.60841656e+00
   9.63199854e-01 -3.32925580e-02  1.82730234e+00 -8.89043733e-02
   1.03326552e-01  1.23202717e+00 -1.62939775e+00  8.68012488e-01
   4.37752008e-01  5.41352630e-01  4.58516777e-01 -1.81859002e-01
  -1.24955714e-01 -4.90596294e-02  1.27540767e+00  3.60171229e-01
   9.01617110e-01 -2.70372570e-01  5.28253436e-01  1.62672818e-01
  -7.59159401e-02 -1.01459682e+00 -1.15195643e-02  1.09213448e+00
   1.62921584e+00 -4.38633710e-01 -1.36951172e+00 -3.69843543e-01
  -8.60606253e-01 -5.75633466e-01  1.27469802e+00  4.84667391e-01
  -1.51542473e+00  1.16384223e-01 -8.54173601e-01 -7.89357781e-01
  -3.20154727e-01  6.83034718e-01 -4.90488261e-01 -1.72152758e+00
  -5.88555992e-01  2.26808488e-01  1.35770679e+00 -7.94542491e-01
  -2.14967608e+00 -2.36072704e-01 -8.44686925e-01 -1.26111269e+00
  -8.60300481e-01  1.04535091e+00  1.55022159e-01  3.35658103e-01
  -1.93393087e+00 -3.61082911e-01  1.09404229e-01 -1.99066043e-01
   4.23870265e-01  1.72599167e-01  8.43244314e-01  3.28669012e-01
   5.97395658e-01 -3.46147346e+00 -1.11853528e+00  1.95144802e-01
   1.27475369e+00 -6.44126832e-01  1.99323013e-01 -3.13881099e-01
   8.67388427e-01  1.60537791e+00 -4.69321199e-02  2.43432593e+00
   1.97670710e+00 -2.16416430e+00 -3.69843096e-01 -3.87633115e-01
  -1.22007036e+00 -1.98171616e+00 -3.45797569e-01  8.39573085e-01]
 [-1.55582651e-01 -1.18109095e+00  2.93430400e+00 -6.94847047e-01
   7.65346646e-01 -1.38265729e-01 -1.87197840e+00 -5.47255754e-01
  -1.02196300e+00 -2.40618020e-01  1.53626862e-03 -1.30697155e+00
   1.12571657e+00 -6.09562814e-01  1.84250534e+00  5.26184618e-01
  -1.34199113e-01 -4.64435697e-01 -2.08943471e-01 -7.25073040e-01
  -3.43274266e-01 -4.16672796e-01 -1.43184513e-01 -1.58227885e+00
   5.83837748e-01 -6.71794951e-01  1.48207486e+00  8.79976228e-02
  -1.44496679e+00 -1.04719102e+00 -7.33625665e-02  7.97347367e-01
  -1.49433851e+00 -4.77036446e-01  1.96962878e-01  9.63585079e-01
  -1.06893802e+00  6.35676011e-02 -2.22821608e-01  2.68229580e+00
  -1.22432196e+00  2.33570918e-01  3.37046921e-01 -8.80938113e-01
   3.74312043e-01  4.75478441e-01 -4.31405723e-01 -1.49277854e+00
  -7.05678761e-01  8.41879070e-01 -9.39537287e-01 -1.32261544e-01
   1.27473974e+00 -2.45601356e-01 -1.09500504e+00 -1.12451386e+00
  -1.68831348e+00 -1.66038191e+00  8.83513212e-01 -2.12799621e+00
   1.85301650e+00  1.06351781e+00 -1.55932271e+00 -3.31427574e-01
  -4.71284389e-01 -8.42893183e-01  2.66877890e-01 -2.19565415e+00
  -1.28610337e+00  2.07863837e-01  2.12718658e-02 -1.87917411e-01
   2.03328684e-01 -1.13658619e+00  7.13958859e-01  3.48399356e-02
   8.75229090e-02 -1.98537886e-01  8.98726761e-01 -1.00354958e+00
   3.02103549e-01  5.83664238e-01 -1.10631919e+00 -7.53618121e-01
   2.54740193e-02 -5.73981524e-01  6.41987205e-01  1.07664418e+00
   8.75026762e-01 -4.93790269e-01  7.25373566e-01  8.76517355e-01
   1.12196028e+00  2.01063013e+00  7.10269690e-01  3.98254842e-01
  -1.09351709e-01 -6.48999751e-01 -1.88457203e+00 -9.38958764e-01]
 [ 1.07939029e+00 -4.96548653e-01 -1.24067736e+00  2.76984543e-01
  -8.75897765e-01  3.70610416e-01  2.94699460e-01  2.08256388e+00
   1.13252175e+00  1.22751558e+00 -1.09523833e+00  5.95500886e-01
   1.12185657e+00  7.54166365e-01 -8.05529773e-01 -1.03034163e+00
   1.09279633e+00  1.50435269e+00  1.11762017e-01  1.07449919e-01
  -1.02401459e+00 -1.84641510e-01  7.72709399e-02 -4.99782264e-01
  -2.27677077e-01  6.49656951e-02  1.01747751e-01  1.98065881e-02
   5.62927604e-01  1.23536706e+00 -1.95974544e-01 -2.40673900e-01
   4.45345312e-01  5.92050970e-01  1.02269816e+00 -1.74345989e-02
  -1.73933551e-01 -6.29255697e-02 -2.20695183e-01  1.29462636e+00
   2.52112865e-01 -2.17039728e+00  9.85382497e-01 -7.44955420e-01
   1.57546902e+00  5.03206611e-01 -9.72602367e-01 -6.84496820e-01
   5.72718233e-02 -4.53426331e-01  5.94903529e-01  1.22255921e+00
   7.85897672e-01  6.81237221e-01  1.01942651e-01  4.92852837e-01
   5.66514671e-01 -1.17636359e+00  5.35346568e-01  1.32921374e+00
   1.24366212e+00 -5.73699355e-01  1.01102638e+00  8.36099803e-01
  -2.34281445e+00 -9.67368782e-01  2.25748569e-01 -9.44747999e-02
  -3.09640646e-01 -1.30452168e+00  2.80700207e-01 -1.33084834e+00
  -9.54967201e-01 -1.04418528e+00 -2.22839022e+00 -1.09560812e+00
   1.24782884e+00  1.80284277e-01 -1.25238645e+00  7.14607239e-01
   1.95957351e+00  2.53216553e+00 -8.34777504e-02  1.11373827e-01
  -8.84591460e-01  1.04834628e+00 -5.54076493e-01  8.74013126e-01
   5.28820276e-01 -4.34727043e-01  8.74114633e-01  5.12598276e-01
   1.89701116e+00 -2.48368159e-01  1.54528201e+00  9.61921632e-01
   3.33936542e-01  6.79747701e-01 -5.69426119e-01  9.70423698e-01]
 [-1.02778935e+00 -2.27263141e-02  1.10301085e-01  1.79546729e-01
  -4.90769893e-02  5.96660793e-01  1.15810835e+00  6.27954483e-01
   1.70029372e-01 -1.79182720e+00  1.67022240e+00 -6.81339622e-01
  -1.44740534e+00  1.85646936e-01  3.88607174e-01 -1.27854240e+00
  -1.10837603e+00 -7.75742769e-01  2.63235778e-01  1.52836013e+00
  -3.64444673e-01 -7.31038809e-01  1.15823066e+00  4.90437865e-01
  -5.65739751e-01 -9.17869389e-01  4.85316277e-01 -5.30028522e-01
  -3.88954520e-01 -9.46969926e-01  2.02791953e+00 -1.98417687e+00
   2.03697324e-01  2.72669315e-01  8.71521354e-01  1.61161527e-01
  -5.35239339e-01  7.95005918e-01 -1.75809467e+00 -1.27538812e+00
   6.49030805e-01  7.76762664e-01 -1.38365650e+00  2.20103472e-01
  -6.78639174e-01  5.56474961e-02 -2.88346857e-01 -9.60100591e-01
   6.68293357e-01  3.41510355e-01  1.58940768e+00  8.42409909e-01
  -8.10967982e-01  7.20736444e-01  1.03318654e-01 -1.99774706e+00
  -5.00028312e-01 -8.31591904e-01  5.86176157e-01  1.26994348e+00
   1.11285400e+00 -2.34507990e+00  1.84525335e+00 -2.89478928e-01
   2.72452831e+00 -1.85729042e-01 -5.73727824e-02  3.77985537e-01
   7.77927995e-01  1.63456053e-01 -8.48517776e-01 -1.21194494e+00
   9.50055778e-01  6.42497420e-01  5.55373728e-01 -8.59387755e-01
   1.78057873e+00  3.59947473e-01  1.22501051e+00  4.28557783e-01
  -9.43535328e-01  1.48158407e+00  2.29413652e+00 -1.71246231e+00
  -7.68941283e-01 -1.05595326e+00 -8.25688243e-01  4.38418001e-01
  -1.34553814e+00 -3.07022452e-01 -2.85520107e-01  1.29907846e+00
   2.93988436e-01  8.37067842e-01  3.93039435e-01 -3.51285487e-01
   8.80326271e-01  7.99354970e-01  2.72147131e+00 -6.97772563e-01]
 [ 1.55318558e-01  1.26152441e-01  9.05780315e-01  8.18991438e-02
  -1.29863977e-01  2.24427879e-01 -1.33973733e-01  4.19006675e-01
   1.31296360e+00 -1.32973254e-01 -4.47266810e-02 -1.72328055e-01
  -7.81214893e-01  1.23739958e+00 -1.91514134e-01 -1.37162611e-01
  -3.27861980e-02  7.96586096e-01 -1.01332128e+00  9.38049018e-01
  -9.92036402e-01 -1.93410134e+00  5.83719552e-01 -1.21997699e-01
  -7.61240780e-01  7.48465629e-03 -1.03534722e+00 -3.08909953e-01
   5.18409371e-01 -3.92305017e-01 -2.04970264e+00  1.69121325e+00
  -9.20890093e-01 -3.47137898e-01 -1.20613670e+00  3.00528228e-01
   9.43905175e-01 -1.59808314e+00  1.40788376e+00  1.41724432e+00
   8.80906701e-01 -5.68516105e-02 -1.04051435e+00  9.45277989e-01
   7.76422098e-02  6.56548738e-01 -6.62871718e-01 -6.30390286e-01
  -8.90351236e-01 -3.24517757e-01 -8.22051048e-01 -7.51260757e-01
   1.96293175e-01  7.01368928e-01 -1.39238060e+00 -6.68631792e-01
   1.55344999e+00  4.97449428e-01  9.80282068e-01  5.71124673e-01
  -2.32236490e-01  7.84225240e-02 -8.50019217e-01  5.46521485e-01
  -3.93892229e-01  1.12404597e+00 -4.27607358e-01 -2.33180244e-02
  -9.29678082e-01 -1.25660062e+00 -7.16505647e-01 -1.50908187e-01
  -1.59349561e+00 -1.10606849e+00 -2.74965316e-01 -1.46609589e-01
  -2.04783964e+00  5.78249320e-02  1.45206213e+00 -9.06186819e-01
  -4.90492165e-01  5.79007506e-01  2.22233844e+00  2.66923189e-01
  -1.62423000e-01 -2.91489512e-01 -5.97359419e-01  3.06709170e-01
  -1.82558179e+00  1.38704881e-01 -1.57723725e+00 -5.51332712e-01
  -2.92212635e-01 -8.04641563e-03 -7.65457824e-02  8.14916015e-01
  -1.61039865e+00 -7.35562503e-01  1.21822762e+00  6.72223270e-01]] [ -54.486095 -244.06975    50.603287  200.90747  -138.96817 ]
('OPERATION_END_ELAPSED', 0.034, 'TRAINING')
CONVERT_MODEL  <class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
BEAUTIFIED_JSON_START
{
	"layers" : 	{
		"Layer_0" : 	{
			"NbInputs" : 0,
			"NbOutputs" : 100,
			"intercepts" : [  ],
			"name" : "Input_Layer"
		},
		"Layer_1" : 	{
			"NbInputs" : 100,
			"NbOutputs" : 4,
			"coeffs_00" : [ 0.012403558939695358, -0.06901320815086365, 0.017665384337306023, 0.0448085181415081 ],
			"coeffs_01" : [ 0.1747935712337494, 0.22442668676376343, 0.21231116354465485, -0.07288465648889542 ],
			"coeffs_02" : [ 0.0960393026471138, -0.11955799162387848, 0.1322208046913147, -0.0689084604382515 ],
			"coeffs_03" : [ -0.01977667771279812, -0.13605083525180817, -0.04037285968661308, -0.1869259625673294 ],
			"coeffs_04" : [ -0.09183124452829361, -0.037211425602436066, -0.13414019346237183, -0.20332404971122742 ],
			"coeffs_05" : [ -0.15701042115688324, 0.028098415583372116, -0.177112877368927, 0.0933876559138298 ],
			"coeffs_06" : [ 0.14688844978809357, 0.19256989657878876, 0.130193829536438, -0.16761435568332672 ],
			"coeffs_07" : [ 0.10409754514694214, 0.015773365274071693, -0.23332318663597107, 0.1203448623418808 ],
			"coeffs_08" : [ -0.17491841316223145, -0.13990437984466553, 0.20974200963974, -0.008408886380493641 ],
			"coeffs_09" : [ -0.04202679917216301, -0.14259956777095795, 0.11368343979120255, 0.19837497174739838 ],
			"coeffs_10" : [ 0.06825025379657745, 0.14895491302013397, -0.18713398277759552, 0.03181521221995354 ],
			"coeffs_11" : [ -0.14538337290287018, -0.093665711581707, -0.0180469062179327, -0.08196959644556046 ],
			"coeffs_12" : [ 0.1087552085518837, -0.07066655158996582, 0.08636723458766937, -0.08089881390333176 ],
			"coeffs_13" : [ 0.1463073492050171, 0.16228000819683075, -0.08057986199855804, 0.08276636898517609 ],
			"coeffs_14" : [ 0.12181901186704636, 0.11800719052553177, 0.05975488945841789, -0.04885784909129143 ],
			"coeffs_15" : [ 0.14763757586479187, -0.15026195347309113, -0.16552014648914337, 0.07487726956605911 ],
			"coeffs_16" : [ 0.12628111243247986, -0.184019535779953, 0.19704072177410126, -0.11973025649785995 ],
			"coeffs_17" : [ -0.15080809593200684, -0.06442554295063019, -0.1834854632616043, -0.23619210720062256 ],
			"coeffs_18" : [ -0.19123810529708862, 0.07570705562829971, -0.011769316159188747, 0.0013707356993108988 ],
			"coeffs_19" : [ -0.059172775596380234, 0.2138274610042572, -0.18137803673744202, 0.12540031969547272 ],
			"coeffs_20" : [ 0.04958048835396767, 0.23217333853244781, -0.0849241390824318, -0.13941441476345062 ],
			"coeffs_21" : [ -0.1702551692724228, 0.035141751170158386, 0.08345193415880203, -0.01635894924402237 ],
			"coeffs_22" : [ 0.2333442121744156, -0.12383043020963669, 0.10607381910085678, 0.008960272185504436 ],
			"coeffs_23" : [ 0.10047955811023712, -0.1833183616399765, 0.17845575511455536, -0.04011757671833038 ],
			"coeffs_24" : [ 0.15958338975906372, 0.02266470529139042, -0.19405117630958557, -0.15181368589401245 ],
			"coeffs_25" : [ -0.08584551513195038, -0.17502762377262115, 0.20035070180892944, 0.03878796845674515 ],
			"coeffs_26" : [ 0.04764965549111366, -0.005432936362922192, 0.1124945729970932, -0.09210162609815598 ],
			"coeffs_27" : [ -0.15042908489704132, 0.05738988146185875, -0.07500560581684113, 0.21540160477161407 ],
			"coeffs_28" : [ -0.028741391375660896, 0.13121995329856873, -0.24236536026000977, 0.2126375138759613 ],
			"coeffs_29" : [ 0.036541763693094254, 0.17960911989212036, -0.10749134421348572, -0.2667110860347748 ],
			"coeffs_30" : [ 0.008207288570702076, 0.06986436992883682, 0.020145662128925323, -0.16826064884662628 ],
			"coeffs_31" : [ 0.04284442961215973, -0.19636747241020203, 0.13683995604515076, -0.0007038154872134328 ],
			"coeffs_32" : [ 0.11742604523897171, -0.011212301440536976, 0.045127809047698975, 0.14444352686405182 ],
			"coeffs_33" : [ 0.04397866129875183, 0.0011563089210540056, 0.0632595643401146, 0.21645499765872955 ],
			"coeffs_34" : [ 0.2169715017080307, 0.05386907607316971, 0.06651416420936584, -0.13627022504806519 ],
			"coeffs_35" : [ 0.08005089312791824, -0.010165272280573845, 0.16752906143665314, 0.12246782332658768 ],
			"coeffs_36" : [ 0.09739112854003906, 0.0266505666077137, -0.10350800305604935, -0.17217490077018738 ],
			"coeffs_37" : [ -0.25389981269836426, -0.026472361758351326, 0.13902410864830017, -0.19014059007167816 ],
			"coeffs_38" : [ -0.14913129806518555, 5.319545743986964e-05, -0.08124925196170807, 0.13569492101669312 ],
			"coeffs_39" : [ 0.09775814414024353, 0.2135622650384903, 0.08540153503417969, -0.14110754430294037 ],
			"coeffs_40" : [ -0.06890074908733368, -0.22626954317092896, 0.0781623050570488, 0.15700452029705048 ],
			"coeffs_41" : [ -0.17757850885391235, -0.21731534600257874, 0.09099670499563217, -0.04019896686077118 ],
			"coeffs_42" : [ 0.20685163140296936, -0.03944166749715805, 0.031643643975257874, -0.09262420982122421 ],
			"coeffs_43" : [ -0.08379879593849182, 0.04836081713438034, -0.09236007928848267, 0.2336694747209549 ],
			"coeffs_44" : [ -0.22153019905090332, -0.17778940498828888, -0.08452340960502625, -0.15211990475654602 ],
			"coeffs_45" : [ -0.2071416676044464, 0.21207289397716522, 0.25507229566574097, 0.10899457335472107 ],
			"coeffs_46" : [ 0.15633690357208252, -0.17136652767658234, -0.2507281005382538, -0.2419169545173645 ],
			"coeffs_47" : [ -0.2034403532743454, 0.2170906513929367, -0.07492602616548538, 0.17772544920444489 ],
			"coeffs_48" : [ -0.1811746507883072, 0.03096342645585537, -0.05554144084453583, 0.1320684254169464 ],
			"coeffs_49" : [ -0.18678060173988342, -0.1819189190864563, -0.14776423573493958, 0.17254024744033813 ],
			"coeffs_50" : [ -0.1545199304819107, 0.117843396961689, -0.10032352805137634, 0.21633848547935486 ],
			"coeffs_51" : [ 0.14920000731945038, -0.13249674439430237, 0.06894567608833313, -0.11544439941644669 ],
			"coeffs_52" : [ -0.06071080267429352, 0.0002137838746421039, 0.26261958479881287, -0.1581019163131714 ],
			"coeffs_53" : [ 0.16908326745033264, -0.1500394344329834, -0.1453106701374054, -0.1938108503818512 ],
			"coeffs_54" : [ -0.051932599395513535, -0.03952100872993469, -0.03011459670960903, 0.17288215458393097 ],
			"coeffs_55" : [ -0.03204664960503578, 0.11819962412118912, -0.07990243285894394, 0.17459537088871002 ],
			"coeffs_56" : [ 0.18980823457241058, 0.23170194029808044, -0.12751390039920807, 0.20427264273166656 ],
			"coeffs_57" : [ 0.1864398717880249, 0.04651285335421562, -0.058300308883190155, -0.003099567024037242 ],
			"coeffs_58" : [ -0.13521376252174377, 0.2185976803302765, -0.09715930372476578, -0.10424615442752838 ],
			"coeffs_59" : [ -0.01740291528403759, 0.18249937891960144, 0.025303125381469727, -0.0056112464517354965 ],
			"coeffs_60" : [ -0.08900197595357895, 0.24790212512016296, 0.05716254189610481, -0.023787837475538254 ],
			"coeffs_61" : [ 0.09260396659374237, -0.036906611174345016, 0.25449416041374207, -0.09271091222763062 ],
			"coeffs_62" : [ -0.12532100081443787, -0.15464258193969727, 0.06171400472521782, -0.14077657461166382 ],
			"coeffs_63" : [ -0.2085217386484146, -0.12139524519443512, -0.17075811326503754, 0.11348074674606323 ],
			"coeffs_64" : [ -0.02316090278327465, -0.12070930004119873, -0.13886752724647522, 0.025886835530400276 ],
			"coeffs_65" : [ 0.15777680277824402, -0.07259462028741837, 0.12753751873970032, 0.19891929626464844 ],
			"coeffs_66" : [ 0.016652369871735573, -0.2260221391916275, 0.019347677007317543, -0.06899602711200714 ],
			"coeffs_67" : [ 0.16728830337524414, -0.16524913907051086, 0.10396460443735123, 0.23367424309253693 ],
			"coeffs_68" : [ -0.1724829375743866, 0.11876452714204788, -0.0899331197142601, -0.1776057630777359 ],
			"coeffs_69" : [ -0.07205263525247574, -0.155573770403862, 0.037590909749269485, -0.1954060047864914 ],
			"coeffs_70" : [ 0.06655193120241165, 0.02020038105547428, 0.19964690506458282, -0.0529705174267292 ],
			"coeffs_71" : [ 0.11918972432613373, 0.1138937771320343, -0.12519095838069916, 0.13310280442237854 ],
			"coeffs_72" : [ 0.10613840073347092, -0.14082977175712585, -0.15655487775802612, -0.13670040667057037 ],
			"coeffs_73" : [ 0.10855502635240555, -0.19317105412483215, 0.0034217280335724354, -0.16386722028255463 ],
			"coeffs_74" : [ -0.20020952820777893, 0.15689785778522491, -0.0071449242532253265, -0.057571638375520706 ],
			"coeffs_75" : [ 0.15929313004016876, -0.1730562448501587, 0.0821470394730568, -0.23502081632614136 ],
			"coeffs_76" : [ 0.06662292033433914, 0.04504961892962456, -0.003847176907584071, 0.035599056631326675 ],
			"coeffs_77" : [ -0.01667395606637001, -0.12009412795305252, -0.15326836705207825, -0.15577925741672516 ],
			"coeffs_78" : [ -0.024129066616296768, 0.04899647831916809, 0.07638458162546158, -0.1674150675535202 ],
			"coeffs_79" : [ 0.06383340060710907, -0.19451656937599182, -0.008581612259149551, -0.1221301257610321 ],
			"coeffs_80" : [ -0.16483332216739655, 0.23030857741832733, 0.13916712999343872, 0.13859708607196808 ],
			"coeffs_81" : [ 0.12049396336078644, -0.043038539588451385, -0.13809512555599213, 0.15267877280712128 ],
			"coeffs_82" : [ -0.1202956885099411, -0.134861558675766, 0.07912153750658035, 0.19237861037254333 ],
			"coeffs_83" : [ -0.06045310199260712, 0.17448322474956512, 0.1845490038394928, -0.15598264336585999 ],
			"coeffs_84" : [ -0.06070435419678688, -0.05688299238681793, -0.10707772523164749, -0.11408568173646927 ],
			"coeffs_85" : [ 0.21804575622081757, -0.2101374864578247, -0.08053915202617645, -0.0076012504287064075 ],
			"coeffs_86" : [ 0.18438370525836945, -0.05990220606327057, 0.19757631421089172, 0.06075528264045715 ],
			"coeffs_87" : [ -0.20003855228424072, -0.12223401665687561, -0.1524152010679245, -0.14723767340183258 ],
			"coeffs_88" : [ -0.10199487954378128, 0.016849182546138763, 0.11155031621456146, 0.10951618105173111 ],
			"coeffs_89" : [ 0.009250202216207981, -0.21962806582450867, -0.09627658873796463, -0.14915664494037628 ],
			"coeffs_90" : [ -0.04581400379538536, -0.16137249767780304, 0.018428286537528038, -0.2101372480392456 ],
			"coeffs_91" : [ 0.16819509863853455, -0.04796140640974045, -0.2084398865699768, -0.23587022721767426 ],
			"coeffs_92" : [ -0.01110170315951109, -0.043859418481588364, -0.08667042851448059, -0.024040795862674713 ],
			"coeffs_93" : [ -0.11105194687843323, -0.1388287991285324, -0.14862939715385437, 0.07253940403461456 ],
			"coeffs_94" : [ -0.18043085932731628, -0.08556398004293442, 0.22433175146579742, -0.1899375021457672 ],
			"coeffs_95" : [ -0.16637247800827026, 0.07582242041826248, 0.11201246082782745, -0.028855368494987488 ],
			"coeffs_96" : [ 0.03985914587974548, 0.1574404090642929, 0.1391054093837738, -0.10516408085823059 ],
			"coeffs_97" : [ 0.15207690000534058, 0.25952258706092834, -0.09349650889635086, -0.22005067765712738 ],
			"coeffs_98" : [ 0.02481076680123806, -0.13416175544261932, -0.19314442574977875, 0.09241479635238647 ],
			"coeffs_99" : [ -0.22825977206230164, -0.2071705460548401, 0.17698968946933746, 0.24567507207393646 ],
			"intercepts" : [ 0.18985114991664886, -0.1299561858177185, 0.2166670858860016, -0.1303650140762329 ],
			"name" : "Hidden_Layer_1"
		},
		"Layer_2" : 	{
			"NbInputs" : 4,
			"NbOutputs" : 8,
			"coeffs_0" : [ -0.12344153970479965, -0.6079649329185486, -0.4100094139575958, 0.6019955277442932, 0.3661438226699829, -0.21004851162433624, 0.4943002164363861, -0.616344153881073 ],
			"coeffs_1" : [ -0.6174781322479248, 0.1140240803360939, 0.46821513772010803, 0.1477763056755066, 0.5257532000541687, -0.16097086668014526, -0.02185395359992981, -0.388573557138443 ],
			"coeffs_2" : [ 0.37774333357810974, 0.3829883337020874, 0.6422132849693298, -0.2657012343406677, -0.3103017210960388, -0.10510154068470001, 0.2889889180660248, -0.5216975808143616 ],
			"coeffs_3" : [ 0.01769745536148548, 0.32857567071914673, -0.6025924682617188, -0.2543850243091583, -0.3700908422470093, 0.6712557077407837, -0.01079703588038683, -0.4428301453590393 ],
			"intercepts" : [ 0.512536883354187, -0.4672949016094208, 0.14032794535160065, 0.7384389042854309, -0.6352207064628601, -0.007103016600012779, 0.43723973631858826, 0.6251798272132874 ],
			"name" : "Hidden_Layer_2"
		},
		"Layer_3" : 	{
			"NbInputs" : 8,
			"NbOutputs" : 6,
			"coeffs_0" : [ -0.0030542274471372366, -0.3680122494697571, -0.39480847120285034, -0.20625151693820953, 0.41290974617004395, 0.54756760597229 ],
			"coeffs_1" : [ 0.35179972648620605, -0.3591874837875366, -0.4575643241405487, 0.4596917927265167, -0.2776065170764923, 0.23336264491081238 ],
			"coeffs_2" : [ 0.2143430858850479, -0.018848974257707596, -0.29198119044303894, 0.393165647983551, -0.5400449633598328, 0.23197056353092194 ],
			"coeffs_3" : [ 0.6076111793518066, 0.24923154711723328, 0.22339461743831635, 0.26450061798095703, 0.3162299692630768, -0.24493373930454254 ],
			"coeffs_4" : [ 0.10179661214351654, 0.1979774534702301, 0.38181331753730774, -0.264075368642807, 0.19948753714561462, -0.5283652544021606 ],
			"coeffs_5" : [ -0.3448825478553772, 0.02528357319533825, 0.5613590478897095, -0.044022295624017715, -0.21004492044448853, -0.5088943243026733 ],
			"coeffs_6" : [ -0.5064402222633362, 0.2041219174861908, -0.6405479907989502, 0.4774836301803589, 0.4351446032524109, 0.32001975178718567 ],
			"coeffs_7" : [ 0.008996491320431232, -0.37896427512168884, 0.6033060550689697, 0.6327269673347473, 0.09222991019487381, 0.02421445958316326 ],
			"intercepts" : [ 0.6746387481689453, -0.3524274528026581, -0.46658188104629517, -0.18721391260623932, 0.5147522687911987, -0.6109247803688049 ],
			"name" : "Hidden_Layer_3"
		},
		"Layer_4" : 	{
			"NbInputs" : 6,
			"NbOutputs" : 1,
			"coeffs_0" : [ 0.7698447704315186 ],
			"coeffs_1" : [ -0.1931886076927185 ],
			"coeffs_2" : [ 0.6418218016624451 ],
			"coeffs_3" : [ 0.8033493161201477 ],
			"coeffs_4" : [ 0.7103408575057983 ],
			"coeffs_5" : [ -0.4940992593765259 ],
			"intercepts" : [ -0.26279324293136597 ],
			"name" : "Output_Layer"
		},
		"sizes" : [ 100, 4, 8, 6, 1 ]
	},
	"metadata" :  { "model" : "sklearn.neural_network._multilayer_perceptron.MLPRegressor", "version" : "1.4.1.post1" },
	"options" :  { "activation" : "relu", "alpha" : 0.0001, "batch_size" : "auto", "beta_1" : 0.9, "beta_2" : 0.999, "early_stopping" : false, "epsilon" : 1e-08, "hidden_layer_sizes" : [ 4, 8, 6 ], "learning_rate" : "constant", "learning_rate_init" : 0.001, "max_fun" : 15000, "max_iter" : 32, "momentum" : 0.9, "n_iter_no_change" : 10, "nesterovs_momentum" : true, "power_t" : 0.5, "random_state" : 1789, "shuffle" : false, "solver" : "adam", "tol" : 0.0001, "validation_fraction" : 0.1, "verbose" : false, "warm_start" : false }
}
BEAUTIFIED_JSON_END
('OPERATION_START', 'PREDICT')
[1.4413223  1.0209816  1.3600857  1.7024231  0.01326728 1.6757734
 0.07398167 3.0719502  1.7133417  2.3757708  2.0961797  2.4509451
 1.2739699  1.7863212  2.1127086  0.18238398]
('OPERATION_END_ELAPSED', 0.001, 'PREDICT')
MODEL_PERFS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'RandomReg_100_tiny', 'size': 16, 'mse': 27562.334, 'mae': 122.389244, 'mape': 0.97218025, 'r2': -0.10371514415299332}
WRITING_PERF_CODE 'logs/auto_tests/regression/MLPRegressor/sklearn.neural_network._multilayer_perceptron.MLPRegressor_RandomReg_100_tiny_option_1.perf'

MODEL_PERFS_TIMINGS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'RandomReg_100_tiny', 'training_time_in_sec': 0.034, 'prediction_time_in_sec': 0.001}
