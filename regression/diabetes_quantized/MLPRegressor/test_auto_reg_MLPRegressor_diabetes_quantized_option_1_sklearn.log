     age  sex  bmi  bp  s1  s2  s3  s4  s5  s6  target
0      7   10    8   6   1   2   1   5   6   3   151.0
1      4    0    1   3   4   3   9   2   0   0    75.0
2      9   10    8   5   1   2   2   5   5   2   141.0
3      0    0    4   2   6   7   2   7   6   4   206.0
4      5    0    2   6   5   6   6   5   2   1   135.0
..   ...  ...  ...  ..  ..  ..  ..  ..  ..  ..     ...
437    7   10    6   8   4   5   3   5   7   5   178.0
438    4   10    4   0   8   9   3   7   3   8   104.0
439    7   10    4   6   2   4   3   4   1   6   132.0
440    2    0    7   5   6   6   3   7   8   2   220.0
441    2    0    0   0   9   7   9   2   4   5    57.0

[442 rows x 11 columns]
SKLEARN_MODEL_SET_OPTIONS MLPRegressor {"hidden_layer_sizes" : [4, 8, 6]}
('OPERATION_START', 'TRAINING')
[[ 7. 10.  8.  6.  1.  2.  1.  5.  6.  3.]
 [ 4.  0.  1.  3.  4.  3.  9.  2.  0.  0.]
 [ 9. 10.  8.  5.  1.  2.  2.  5.  5.  2.]
 [ 0.  0.  4.  2.  6.  7.  2.  7.  6.  4.]
 [ 5.  0.  2.  6.  5.  6.  6.  5.  2.  1.]] [151.  75. 141. 206. 135.]
('OPERATION_END_ELAPSED', 0.143, 'TRAINING')
CONVERT_MODEL  <class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
BEAUTIFIED_JSON_START
{
	"layers" : 	{
		"Layer_0" : 	{
			"NbInputs" : 0,
			"NbOutputs" : 10,
			"intercepts" : [  ],
			"name" : "Input_Layer"
		},
		"Layer_1" : 	{
			"NbInputs" : 10,
			"NbOutputs" : 4,
			"coeffs_0" : [ -0.13115526735782623, -0.13106675446033478, 0.24442631006240845, 0.11816155910491943 ],
			"coeffs_1" : [ 0.4851565957069397, 0.6780734658241272, 0.7707459330558777, -0.32786184549331665 ],
			"coeffs_2" : [ 0.2682479918003082, -0.19440481066703796, 0.4087522327899933, -0.18775784969329834 ],
			"coeffs_3" : [ -0.20313294231891632, -0.19759665429592133, 0.07612740248441696, -0.661070704460144 ],
			"coeffs_4" : [ -0.2626160979270935, 0.003439430147409439, -0.3104405105113983, -0.5509638786315918 ],
			"coeffs_5" : [ -0.5352422595024109, 0.28976380825042725, -0.2852831482887268, 0.14573021233081818 ],
			"coeffs_6" : [ 0.3585190176963806, 0.6304006576538086, 0.5162354111671448, -0.45379701256752014 ],
			"coeffs_7" : [ 0.12537981569766998, 0.2203369289636612, -0.4365256726741791, 0.18870419263839722 ],
			"coeffs_8" : [ -0.6267110109329224, -0.22673848271369934, 0.7698878645896912, -0.16493107378482819 ],
			"coeffs_9" : [ -0.09555140882730484, -0.3341652452945709, 0.41942206025123596, 0.49568629264831543 ],
			"intercepts" : [ 0.19714170694351196, 0.6144610643386841, -0.4563685655593872, -0.07120221108198166 ],
			"name" : "Hidden_Layer_1"
		},
		"Layer_2" : 	{
			"NbInputs" : 4,
			"NbOutputs" : 8,
			"coeffs_0" : [ -0.4382188618183136, -0.15202195942401886, 0.006595049984753132, -0.07507390528917313, 0.1701606661081314, -0.33315208554267883, 0.39669761061668396, -0.17167365550994873 ],
			"coeffs_1" : [ 0.595277726650238, 0.6500227451324463, -0.23675726354122162, 0.3328021764755249, 0.22596678137779236, 0.29463860392570496, 0.25639256834983826, -0.1350577175617218 ],
			"coeffs_2" : [ 0.48815253376960754, -0.3515394330024719, -0.3831874132156372, 0.2558845281600952, 0.2209894061088562, -0.7274451851844788, 0.7718938589096069, -0.20327350497245789 ],
			"coeffs_3" : [ -0.4731690585613251, -0.1885852962732315, -0.395148903131485, -0.5393806099891663, -0.519603967666626, 0.09537482261657715, -0.024522244930267334, 0.0961354523897171 ],
			"intercepts" : [ -0.00833228137344122, 0.8065860271453857, -0.6014647483825684, 0.40690577030181885, 0.15457887947559357, 0.5826990008354187, -0.1972166895866394, -0.3275717794895172 ],
			"name" : "Hidden_Layer_2"
		},
		"Layer_3" : 	{
			"NbInputs" : 8,
			"NbOutputs" : 6,
			"coeffs_0" : [ -0.4104098081588745, 0.01649324968457222, 0.4296824336051941, -0.0038816677406430244, 0.6719794273376465, -0.20255978405475616 ],
			"coeffs_1" : [ 0.4739544093608856, -0.12072529643774033, 0.3465173840522766, -0.4705575108528137, 0.6470358967781067, -0.16947902739048004 ],
			"coeffs_2" : [ 0.3923056721687317, 0.04134216904640198, -0.5339289903640747, -0.3025997579097748, -0.2020338773727417, -0.4672466516494751 ],
			"coeffs_3" : [ 0.7513495683670044, -0.09085654467344284, 0.19085638225078583, 0.026801731437444687, 0.41978317499160767, -0.17728261649608612 ],
			"coeffs_4" : [ -0.22200588881969452, 0.08221413195133209, -0.17088903486728668, 0.5504018664360046, -0.050506118685007095, 0.3218083083629608 ],
			"coeffs_5" : [ -0.49438461661338806, 0.5086583495140076, 0.11460326611995697, 0.3920334279537201, -0.14573726058006287, -0.6332890391349792 ],
			"coeffs_6" : [ 0.06954726576805115, 0.14263057708740234, 0.2352590560913086, -0.4493575394153595, 0.2843431532382965, -0.560088038444519 ],
			"coeffs_7" : [ 0.3452427387237549, -0.024735914543271065, 0.24262244999408722, -0.013895221054553986, 0.21037845313549042, 0.3936721682548523 ],
			"intercepts" : [ 0.32212355732917786, -0.1461087018251419, 0.33940938115119934, 0.6179021000862122, 0.7005951404571533, 0.16120555996894836 ],
			"name" : "Hidden_Layer_3"
		},
		"Layer_4" : 	{
			"NbInputs" : 6,
			"NbOutputs" : 1,
			"coeffs_0" : [ 0.4569088816642761 ],
			"coeffs_1" : [ -0.3789703845977783 ],
			"coeffs_2" : [ 0.5625819563865662 ],
			"coeffs_3" : [ 0.019743580371141434 ],
			"coeffs_4" : [ 0.6575888991355896 ],
			"coeffs_5" : [ 0.5841704607009888 ],
			"intercepts" : [ 0.4305127263069153 ],
			"name" : "Output_Layer"
		},
		"sizes" : [ 10, 4, 8, 6, 1 ]
	},
	"metadata" :  { "model" : "sklearn.neural_network._multilayer_perceptron.MLPRegressor", "version" : "1.4.1.post1" },
	"options" :  { "activation" : "relu", "alpha" : 0.0001, "batch_size" : "auto", "beta_1" : 0.9, "beta_2" : 0.999, "early_stopping" : false, "epsilon" : 1e-08, "hidden_layer_sizes" : [ 4, 8, 6 ], "learning_rate" : "constant", "learning_rate_init" : 0.001, "max_fun" : 15000, "max_iter" : 32, "momentum" : 0.9, "n_iter_no_change" : 10, "nesterovs_momentum" : true, "power_t" : 0.5, "random_state" : 1789, "shuffle" : false, "solver" : "adam", "tol" : 0.0001, "validation_fraction" : 0.1, "verbose" : false, "warm_start" : false }
}
BEAUTIFIED_JSON_END
('OPERATION_START', 'PREDICT')
[13.128662   9.99658   13.40136    4.4955597  7.2180133  8.266496
 15.170488  16.891996  13.635001   6.048104   4.6876097 13.253716
  8.875584  15.443481   9.738713  14.322167   9.864787  12.684292
  4.721106   8.392844   6.3156843 14.647216   7.7665887 13.559669
  6.543268  13.845396   6.822414   8.611215   6.9893413 19.179163
  7.428079   9.147624  12.002294   9.05447    7.6548495 18.34448
  8.477223   8.735934  13.616254  15.237655  13.217908   6.6360736
 12.493184   8.199143  12.248678  15.242934   6.504645   5.320893
 17.53794   11.841787   6.6848807 13.762515   8.118967   5.1547856
  7.7709794  9.224212   5.987415   8.669797   8.926097  16.809805
  7.2938037 12.122067  17.361908   7.231792  14.784751  11.764281
 12.220807  14.5484915 14.436933   5.143302   7.305004   6.65117
 18.500381  16.43894   16.558084  13.770925  11.638906   6.7004924
  5.0598593  7.56273    8.1168785 14.693378   9.833302   6.083658
  9.026347   6.6768985 13.932549   9.469387  15.236607   8.011008
  7.927203   6.0792336  8.4111     9.835132   9.31201    2.8348997
 13.011539   4.7573013 16.500431   5.8104615  7.6648417 16.677156
  8.146135  13.670108   7.071432  14.246051   7.951431   3.1936886
 13.377179  11.515758  14.527859   8.896611   6.1688666 12.603467
  8.284351  12.743411  13.487422   5.7750607 12.510292   8.396488
  4.7884583  5.0907416 12.359687  11.036258   8.056975  14.198125
  8.487905  19.655375   8.612428   6.14092   15.057618   8.028701
 15.63061   13.9926195  9.101174   6.8423476  7.885635   7.065986
 15.086363   8.262071  11.492656   4.396828  15.034162   9.642801
 12.436467   7.181113  12.221471   5.772653  16.998613  12.649743
  5.0093436  8.055603  10.425278  14.091254  15.144956  14.517561
  6.4868407 15.353836   7.1799045  6.6284947  7.3267694 12.654574
  6.313645  14.245065   8.787624   8.579762  13.724786   8.1119375
 12.592132   5.1057343 17.194942   5.944426  15.439851  13.821546
 14.083873   6.984545  15.430234   5.774738   8.725087   3.2654908
 11.803677   8.837007  12.915515  14.118733   7.258855  11.741859
 13.029929   8.066598   8.0593     7.293849   4.287001  13.010772
 17.81586   14.231933   6.400986  12.357036   8.037031  12.111453
  4.525938   7.71987    7.705565  15.97082   16.824999  12.25677
 12.713448   7.507404  14.049281   9.602571   5.7638545 10.895706
  8.699885   6.666784   8.050819   7.5902157  8.559374  13.678687
 11.25214    7.8279634  3.6993208  7.04817   15.310277   4.4464087
 13.660811   7.518636   6.2702947 12.6780615 17.270737  16.976313
  8.146915  17.288689  15.578618   5.37435   13.041495   8.754408
  9.572537  11.5725565  6.402101   8.939544  11.284032   8.701731
 13.011498  15.569465  13.244611  14.939761  14.342951   7.8554544
  9.92983    9.72603    4.932055   6.158591   9.291341  12.181499
  3.6884563  6.5714345 13.40185    7.835522   5.990151  17.906317
 11.7312155  3.7717783  8.489537   8.128257  13.460456  15.004411
 11.579773  15.731582  16.686842   6.2852054 12.6127825  9.0355625
 15.80466   12.407159   5.187494  12.641626   5.652624  13.823037
  8.155519   8.082368  12.744468  15.351652  16.1525    15.863849
  6.1277175  7.1941576 11.488141   2.86138    8.823335   7.082564
 17.713202  13.878417  13.077775   7.0448093  8.168379   5.7253776
 13.797342  15.975058   8.177837   6.915898  11.822206  13.611161
  8.519172  16.421457   6.5013347  8.275306  13.652404  13.737639
 13.998731  12.928792   8.279757  16.26332    5.23701    5.1008816
  7.0526347 14.578071   6.2161546  8.524145  13.878758   4.9774666
  8.525775  10.190379   5.1932034  6.07564   12.463019  12.179583
  3.9521148  4.873059  13.443053   6.902662   6.424546  14.533368
 10.863871  11.869082   7.4178767 12.627186  14.716493   8.91269
  4.579527  12.091599   5.675998   7.8114204  5.8696923 12.307344
 12.2462845  6.1036983  6.1662073 15.29532   13.555888  16.72711
  8.810936  11.764013  15.899576  14.271489  17.587389   7.1282487
 15.138507  13.172089  17.79697   14.801333   8.811304  13.518729
  7.341697   7.7131515 14.058954  11.439326  13.012681   6.7271624
 12.730827  13.804168  14.06397    4.4029717 10.532136  14.075345
 18.106932   5.36952    7.8656096 16.106977   4.6513424 12.46465
 12.989602   7.1718473  5.1257234 14.7147455  8.915028  14.891887
 18.3995    13.9843645  7.530783   5.1761622 13.5420265 17.756165
 13.173743   7.4491663 16.019304   7.6478086  6.153333   7.3149576
 14.906949   8.863696   7.0959086 14.634213   7.7208033  8.415167
 11.494193   8.221716   7.091676  12.963574   8.856086   4.280438
  5.868711  13.622767  15.092421   6.8839192  8.147067   7.8745847
 13.161832   3.3839996  4.814675   6.118867   9.065173   6.457813
  2.4575145 12.07721   16.842546  12.988597  12.5379305  7.8747954
 12.040805  13.325889  14.249079   9.451606   5.513666  15.902642
  7.0055346  9.518537   6.496269   6.1243916  9.649989  14.0707445
 14.853333  12.356546   5.0345855 10.19476  ]
('OPERATION_END_ELAPSED', 0.009, 'PREDICT')
MODEL_PERFS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'diabetes_quantized', 'size': 442, 'mse': 26085.357, 'mae': 141.73976, 'mape': 0.90543556, 'r2': -3.3989653276616236}
WRITING_PERF_CODE 'logs/auto_tests/regression/MLPRegressor/sklearn.neural_network._multilayer_perceptron.MLPRegressor_diabetes_quantized_option_1.perf'

MODEL_PERFS_TIMINGS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'diabetes_quantized', 'training_time_in_sec': 0.143, 'prediction_time_in_sec': 0.009}
