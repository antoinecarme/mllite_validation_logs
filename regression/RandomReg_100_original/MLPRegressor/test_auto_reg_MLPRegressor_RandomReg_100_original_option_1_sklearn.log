           X_0       X_1       X_2  ...      X_98      X_99      target
0     1.331256 -0.890219  0.415263  ... -1.017662  0.850023   46.659290
1     0.101475  1.673349  0.646130  ... -0.082562 -0.235254   17.096290
2     1.143388 -0.121749  1.763748  ... -1.210605  0.186911 -214.157384
3    -0.664377  0.702461 -2.030148  ... -1.509694  1.665004  -45.132339
4    -0.183233  0.167641 -1.018050  ... -0.518277 -0.467736 -472.368048
...        ...       ...       ...  ...       ...       ...         ...
1019 -0.108644  0.248959  0.944445  ...  0.725405  0.269099 -131.431944
1020  0.822555  0.293314 -0.646792  ... -0.830353  1.159861  254.930037
1021  1.088703 -0.069279 -0.134730  ...  0.744206 -0.777811 -211.425817
1022 -1.594396 -2.513778 -0.362671  ...  0.569479  0.734742  199.635925
1023  0.335938  0.869565 -0.384574  ...  1.383897  1.366430  -64.848269

[1024 rows x 101 columns]
SKLEARN_MODEL_SET_OPTIONS MLPRegressor {"hidden_layer_sizes" : [4, 8, 6]}
('OPERATION_START', 'TRAINING')
[[ 1.3312556  -0.8902193   0.41526318  0.788659   -0.7130887   1.4502782
  -0.90123427  0.7631844   0.13805741  1.6139201  -0.38690123  0.03777119
  -1.1557323  -0.5858095   1.6626205   0.68594664  0.39176562 -0.2828408
   0.76795787  0.3757667  -2.7420852  -0.01939904  0.40063587  0.17057581
  -0.44771507 -1.2317778  -0.8443254   0.7879428  -0.58345294  1.3596091
   0.6738356   1.2624362  -0.65831184 -0.88525283  0.04143903 -1.1881486
   0.83240134  0.49906567  0.11895992 -0.8198342   0.57035816 -1.4580479
  -0.7348064  -0.4322117   1.670222    0.49406192  0.47490185  0.86712915
   0.06314228 -2.2807853   0.7397412   0.7191424  -0.25634202 -0.75440276
   0.6664157   1.004504   -0.7802194   1.2289945  -0.3151011  -0.52080745
   2.0017505  -0.65958494 -0.06995643  1.3200244  -2.0877314  -2.0024903
  -0.02926418  1.5691092   0.85446763  0.5201753  -0.8084577   1.7062956
   0.8540257  -0.4603799  -0.06352598 -0.8714725  -0.5609252  -0.3273548
   0.74484634 -1.6906458  -0.33099878  0.06832191  1.3731502   0.84443164
   0.45396343  0.7861103   0.26812124 -0.06078732 -1.0477233   1.4219131
   1.2072196   0.7279877  -0.68208903 -1.8471589   0.77888066 -0.9317782
   0.2455851   0.06674263 -1.017662    0.8500229 ]
 [ 0.10147522  1.6733488   0.6461298  -1.2563368   1.1040853  -1.4227151
  -0.6017069  -0.9161108   0.16968192 -1.2134422   0.2153484  -0.08051679
   1.2387321  -0.48785964  0.27010813  1.5061171  -1.2551115  -1.1685889
   0.85681057 -0.3032395   0.34517288 -2.404879   -0.5924876  -0.11890611
  -1.1995318  -2.3414571   0.2762635  -4.5431676  -0.53269297  0.8078485
  -1.5695755  -1.10812     0.8487407  -0.2972983   0.58910424  0.82803386
   1.6629032  -0.71414495  1.0407996   0.00796571 -0.5988979  -1.67421
  -1.17802    -0.22742864  2.2339568   0.3043066   0.37049994 -1.5326284
  -0.64369285  0.96310157 -0.85642195  1.1327137  -1.4508079  -0.13960138
   1.4753928  -0.45546088  0.90462786  1.6403875   0.37488905  1.2000089
   1.3891938  -0.12287517 -0.7553856   0.8262001  -0.0514452   1.0170072
  -0.67795897  0.09011087  0.2844104   0.6818215  -0.8403749  -0.68216765
  -0.05586801  0.47741953 -0.56251854 -0.308212    0.13523772 -0.60912365
   0.6620477   0.10418119 -0.9861369   0.09003809 -2.009931   -0.989154
   0.5694446  -0.7199399  -1.0182965  -0.67762125  1.3876666  -1.4770343
   1.3187295   0.10140626 -0.21362238 -2.1321723  -0.9681868  -1.3000042
  -0.23745911 -0.96780366 -0.08256175 -0.2352544 ]
 [ 1.1433884  -0.12174941  1.763748    0.1567726  -1.074581    0.3090569
  -0.59582543  0.75060105  1.2144792   1.2156659   0.5765553  -0.4355099
   0.2547612  -0.9373168   1.2934183  -0.13537018  0.74200237  0.43883675
  -0.6069965   0.1554797   0.5401852   1.1975131  -1.1406852  -0.20755301
  -0.31834334 -0.6807571   0.24679533  1.5937762   0.1117221   0.80259067
  -1.2313136   0.917576    0.50131375 -0.5743615  -0.17214447 -2.0382028
   0.39552817  2.1280468   1.0293733  -0.02239049 -0.67557096 -1.5386295
   1.4090686  -0.59869367  0.6779711   0.37315732 -0.3413207  -0.21079636
   0.34230128  1.0776794   0.30119348 -0.24192244  0.23708598 -2.070677
  -0.48128286  0.3670155   1.0218917  -0.82419974  0.02379109  0.3317229
   1.5780679  -1.4464464  -1.6030818  -1.4349504  -0.11372316 -0.05236021
   0.37578964  1.7915944  -0.87417036  0.19483742 -0.11213797  0.07997598
  -0.89980906 -0.26016855  0.50538015 -1.2370168   1.1031808  -0.8198793
  -1.211      -0.5554547  -0.3449861   0.52403086 -0.2534899   1.0931107
   0.6548259   0.05409036 -0.2681419  -0.06177664  0.09527746  0.8096554
  -0.2925497   0.3124124  -0.37333974  1.0691838  -1.405716   -2.0550418
  -0.42661822 -0.00682518 -1.2106051   0.18691105]
 [-0.6643772   0.70246065 -2.0301476  -2.0342605   0.87254155  0.8668411
  -0.26858473 -0.73312426  1.0275607  -0.7275404   2.1678174  -1.1313968
   0.40187567 -0.71089333  0.2914241  -0.7301117  -1.095154   -2.268191
  -0.4189619  -0.03287303 -1.6552861   0.06898359 -1.912857   -0.55441886
   0.5538053   1.8425944   0.4861901   0.15487543 -0.10841709 -1.1439068
   0.09484072  0.54181457  0.55301255 -0.16916783 -1.6026491   0.4181876
   1.2507617   0.34678614 -1.1298385   2.87664     0.13642831 -0.22074103
  -1.260425    0.59103775  0.71680933 -1.1945326   0.18375565  0.6792565
  -1.1802012   1.2635881   0.08254713 -1.2404718   0.6709144  -0.07065742
  -0.2673846   0.35829198 -0.9844122  -0.8540105   0.54115516  0.78829515
   0.6353548  -0.6730554  -0.280788   -1.2170262   0.28105965 -1.9465499
   0.2613469  -0.85210556 -0.43554437 -0.4999505   1.1504256  -0.28339145
   0.96043223  0.3302896   0.04703231  0.7185348  -0.7703047  -1.8150443
  -0.29500493  0.47207424  1.062915   -0.8516409   0.34324142 -0.0487177
   0.5544652   0.7159857   0.26749176 -0.28686863 -0.10642244  2.2316606
  -1.3776829  -0.18294522 -1.7351075  -0.15755087  1.2045766   0.8150365
  -0.62745374  1.0366299  -1.5096936   1.665004  ]
 [-0.18323281  0.16764143 -1.0180501  -1.1114113  -0.6096793  -1.203273
  -0.91351366 -1.728868    1.45258    -0.7503452  -0.9816971   2.4864185
   0.2919757   0.7608395   0.02673802  0.01965859  1.7714319   0.10731484
   1.5014844   0.04669442  0.01499307  2.2903414  -1.5327556   0.2251275
   0.13013346 -1.3958306   0.477327   -2.448051   -1.034129   -0.36448112
  -0.27540234 -0.0670136  -0.41763452  1.1593167   0.36466834  0.51342684
  -1.9805648  -1.26423     0.1809458   0.09143253 -0.7548743   0.2509495
  -0.38412192 -1.5486463  -0.01736598  1.3488103  -2.0970352   0.540929
  -0.69701564  0.10016023 -0.34506392  0.5776158   2.3233109  -1.0692806
  -0.67232996 -0.8802096   0.67767256 -0.98172724  1.5671804  -1.077151
   0.6932878  -0.29901332  0.7109348  -0.6178621   0.13290192 -1.3579625
   0.8258206   0.2063345   0.6937211  -0.23165497  1.6014408  -0.33389077
  -0.52463293  1.2932607   0.24274994 -0.84197    -1.8947318  -0.31777015
   0.011854    1.0398142   1.1872678   0.5696282   0.77042353  0.04953871
  -1.2782155  -0.57868254  0.8079945   1.9365364   1.0325052   0.6288556
  -1.8749372   0.49370095  0.64584726  0.73780465  0.23192818 -1.2330554
   0.8333075   0.13102144 -0.5182774  -0.46773615]] [  46.65929   17.09629 -214.15738  -45.13234 -472.36804]
('OPERATION_END_ELAPSED', 0.337, 'TRAINING')
CONVERT_MODEL  <class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
BEAUTIFIED_JSON_START
{
	"layers" : 	{
		"Layer_0" : 	{
			"NbInputs" : 0,
			"NbOutputs" : 100,
			"intercepts" : [  ],
			"name" : "Input_Layer"
		},
		"Layer_1" : 	{
			"NbInputs" : 100,
			"NbOutputs" : 4,
			"coeffs_00" : [ 0.008789870887994766, -0.062060385942459106, 0.021328546106815338, 0.17297878861427307 ],
			"coeffs_01" : [ 0.0495828315615654, 0.09955593943595886, 0.24191321432590485, -0.11763504147529602 ],
			"coeffs_02" : [ 0.25439950823783875, -0.013158618472516537, 0.1140042394399643, -0.12825123965740204 ],
			"coeffs_03" : [ -0.22043953835964203, -0.32122763991355896, -0.011391129344701767, -0.19689354300498962 ],
			"coeffs_04" : [ -0.20739296078681946, -0.12659025192260742, -0.1152658611536026, -0.06139499694108963 ],
			"coeffs_05" : [ -0.20707020163536072, -0.02864641137421131, -0.12631924450397491, 0.049766283482313156 ],
			"coeffs_06" : [ 0.3106197714805603, 0.333396852016449, 0.06918701529502869, -0.1359853446483612 ],
			"coeffs_07" : [ 0.17120841145515442, 0.11914987117052078, -0.28686943650245667, 0.11396342515945435 ],
			"coeffs_08" : [ -0.3355923295021057, -0.16944578289985657, 0.33949729800224304, 0.003746283706277609 ],
			"coeffs_09" : [ 0.14233551919460297, -0.021746667101979256, -0.04895370081067085, 0.10646002739667892 ],
			"coeffs_10" : [ -0.006528377998620272, 0.21741078794002533, -0.07974904775619507, 0.04409575089812279 ],
			"coeffs_11" : [ -0.16615360975265503, -0.014262980781495571, -0.11587950587272644, 0.04662524163722992 ],
			"coeffs_12" : [ -0.027576187625527382, -0.1641513556241989, 0.17393556237220764, -0.18689940869808197 ],
			"coeffs_13" : [ 0.2644137144088745, 0.28772273659706116, -0.09606979787349701, 0.019448185339570045 ],
			"coeffs_14" : [ 0.23072603344917297, 0.18140189349651337, 0.12845338881015778, 0.029538363218307495 ],
			"coeffs_15" : [ 0.18977828323841095, -0.14982423186302185, -0.150583878159523, 0.041523728519678116 ],
			"coeffs_16" : [ 0.07326219230890274, -0.2849689722061157, 0.26966896653175354, -0.22848030924797058 ],
			"coeffs_17" : [ -0.1715984046459198, -0.09969045221805573, -0.14584870636463165, -0.0922660231590271 ],
			"coeffs_18" : [ -0.04347794130444527, 0.048922985792160034, -0.06942969560623169, -0.0023276670835912228 ],
			"coeffs_19" : [ -0.03188077732920647, 0.24480129778385162, -0.2189720869064331, 0.06096307933330536 ],
			"coeffs_20" : [ 0.17684730887413025, 0.325273722410202, -0.08107784390449524, -0.1362055093050003 ],
			"coeffs_21" : [ -0.03093668259680271, 0.16053393483161926, 0.07250431925058365, -0.04557926207780838 ],
			"coeffs_22" : [ 0.3988155722618103, 0.09533965587615967, -0.12102599442005157, -0.17352156341075897 ],
			"coeffs_23" : [ 0.019999876618385315, -0.23129163682460785, 0.05260404199361801, -0.019597843289375305 ],
			"coeffs_24" : [ 0.011707109399139881, -0.15185070037841797, -0.13185365498065948, -0.11828944832086563 ],
			"coeffs_25" : [ -0.20059585571289062, -0.32016679644584656, 0.08874213695526123, -0.10301622003316879 ],
			"coeffs_26" : [ 0.17640744149684906, 0.09187638014554977, -0.025995511561632156, -0.0943940132856369 ],
			"coeffs_27" : [ -0.13496646285057068, 0.06785845756530762, -0.1255556046962738, 0.24493420124053955 ],
			"coeffs_28" : [ -0.17587867379188538, -0.01607774756848812, -0.2748253047466278, 0.1938449591398239 ],
			"coeffs_29" : [ -0.044735606759786606, 0.019912660121917725, 0.09205473959445953, -0.13894271850585938 ],
			"coeffs_30" : [ 0.214024618268013, 0.3775075078010559, -0.20656363666057587, -0.361625075340271 ],
			"coeffs_31" : [ 0.15094877779483795, -0.12250673770904541, 0.16500534117221832, 0.014810968190431595 ],
			"coeffs_32" : [ 0.3245113790035248, 0.2753230035305023, -0.1483675092458725, -0.04447230324149132 ],
			"coeffs_33" : [ 0.23411825299263, 0.19888851046562195, -0.03544158861041069, 0.10788259655237198 ],
			"coeffs_34" : [ 0.16506636142730713, -0.010404129512608051, 0.0030647292733192444, -0.0578453429043293 ],
			"coeffs_35" : [ 0.07342501729726791, 0.0524628646671772, 0.13681574165821075, 0.14859236776828766 ],
			"coeffs_36" : [ 0.30959635972976685, 0.28376325964927673, -0.3187814950942993, -0.3440604507923126 ],
			"coeffs_37" : [ -0.2837976813316345, -0.1374477595090866, 0.0888994112610817, -0.09724003076553345 ],
			"coeffs_38" : [ -0.132998988032341, 0.13539649546146393, -0.17300274968147278, 0.08668264001607895 ],
			"coeffs_39" : [ 0.24988162517547607, 0.3339078724384308, -0.05365186929702759, -0.1824968457221985 ],
			"coeffs_40" : [ -0.08877646178007126, -0.1856997013092041, 0.026714060455560684, 0.04764026030898094 ],
			"coeffs_41" : [ -0.0976855531334877, -0.27313339710235596, 0.1665433794260025, -0.1241748183965683 ],
			"coeffs_42" : [ 0.16218307614326477, -0.12716786563396454, 0.055250417441129684, -0.00821480993181467 ],
			"coeffs_43" : [ -0.26378512382507324, -0.09503954648971558, -0.1674407571554184, 0.2522512972354889 ],
			"coeffs_44" : [ -0.1200864240527153, -0.2743492126464844, -0.07238239049911499, -0.11940212547779083 ],
			"coeffs_45" : [ -0.14873073995113373, 0.23834501206874847, 0.16712920367717743, 0.014355291612446308 ],
			"coeffs_46" : [ 0.3667653203010559, 0.13726547360420227, -0.4659380614757538, -0.4230286777019501 ],
			"coeffs_47" : [ -0.26046448945999146, 0.18229153752326965, -0.11157560348510742, 0.1548883467912674 ],
			"coeffs_48" : [ -0.09085723757743835, -0.014171973802149296, 0.008623629808425903, 0.0589328408241272 ],
			"coeffs_49" : [ -0.0630209818482399, -0.11544769257307053, -0.30004382133483887, 0.19908346235752106 ],
			"coeffs_50" : [ 0.011650757864117622, 0.1448403000831604, -0.2163386195898056, 0.25782883167266846 ],
			"coeffs_51" : [ 0.016707440838217735, -0.15485361218452454, 0.08315667510032654, 0.027813991531729698 ],
			"coeffs_52" : [ -0.05418753996491432, 0.030780043452978134, 0.2571661174297333, -0.1289498507976532 ],
			"coeffs_53" : [ 0.20127856731414795, -0.07388660311698914, -0.003182691289111972, -0.19830451905727386 ],
			"coeffs_54" : [ -0.06448879837989807, -0.055059079080820084, -0.04936433210968971, 0.07885090261697769 ],
			"coeffs_55" : [ -0.05419725552201271, 0.06846287846565247, -0.17440907657146454, 0.14784342050552368 ],
			"coeffs_56" : [ 0.05932198092341423, 0.17152374982833862, -0.16809597611427307, 0.2398383915424347 ],
			"coeffs_57" : [ 0.26122748851776123, 0.12393781542778015, -0.21819062530994415, -0.01106190588325262 ],
			"coeffs_58" : [ -0.08709514141082764, 0.4048461318016052, -0.08980388194322586, -0.009146849624812603 ],
			"coeffs_59" : [ 0.19942526519298553, 0.45552510023117065, -0.13056392967700958, -0.14774854481220245 ],
			"coeffs_60" : [ -0.15754105150699615, 0.1536668837070465, 0.10444379597902298, -0.054208043962717056 ],
			"coeffs_61" : [ 0.1258620172739029, 0.025340352207422256, 0.28018710017204285, -0.11720890551805496 ],
			"coeffs_62" : [ -0.1562948375940323, -0.19203338027000427, 0.08101892471313477, -0.09987720102071762 ],
			"coeffs_63" : [ -0.3207927942276001, -0.16445280611515045, 0.05049993842840195, 0.1786310076713562 ],
			"coeffs_64" : [ -0.11250434815883636, -0.14502833783626556, -0.04231729358434677, -0.02251272089779377 ],
			"coeffs_65" : [ 0.1825350522994995, 0.017259731888771057, 0.22046658396720886, 0.21996839344501495 ],
			"coeffs_66" : [ 0.004917791113257408, -0.17171956598758698, 0.11401252448558807, -0.04811682179570198 ],
			"coeffs_67" : [ 0.020277459174394608, -0.15354877710342407, 0.14533652365207672, 0.19529369473457336 ],
			"coeffs_68" : [ -0.24783095717430115, 0.02724321186542511, -0.14680716395378113, -0.17174407839775085 ],
			"coeffs_69" : [ -0.15828946232795715, -0.22029602527618408, 0.08506117761135101, -0.21118710935115814 ],
			"coeffs_70" : [ 0.11287253350019455, 0.023980673402547836, 0.2141682505607605, -0.05634543299674988 ],
			"coeffs_71" : [ 0.038236696273088455, 0.027673538774251938, -0.05275809392333031, 0.15675611793994904 ],
			"coeffs_72" : [ 0.011153138242661953, -0.20560523867607117, -0.08060760051012039, -0.012962483800947666 ],
			"coeffs_73" : [ 0.23934493958950043, -0.1601455807685852, 0.025403723120689392, -0.08872436732053757 ],
			"coeffs_74" : [ -0.26928022503852844, 0.17437279224395752, -0.02803655155003071, -0.0809921994805336 ],
			"coeffs_75" : [ 0.17106474936008453, -0.0810050293803215, 0.11124668270349503, -0.34161078929901123 ],
			"coeffs_76" : [ 0.04372028633952141, 0.2040572166442871, -0.01349172554910183, 0.04310271516442299 ],
			"coeffs_77" : [ 0.19609983265399933, 0.1297309249639511, -0.36695924401283264, -0.3749937117099762 ],
			"coeffs_78" : [ 0.021541953086853027, 0.07713072001934052, -0.02681204117834568, -0.04696006327867508 ],
			"coeffs_79" : [ -0.04312966391444206, -0.33296120166778564, 0.036176931113004684, -0.123578742146492 ],
			"coeffs_80" : [ -0.2811634838581085, 0.016840973868966103, 0.010641735978424549, 0.2925926148891449 ],
			"coeffs_81" : [ 0.12353885173797607, -0.09986074268817902, -0.12819837033748627, 0.2060278356075287 ],
			"coeffs_82" : [ -0.14762526750564575, -0.11860450357198715, 0.030565520748496056, 0.1598730832338333 ],
			"coeffs_83" : [ 0.05973934754729271, 0.2395753562450409, 0.2599991261959076, -0.15224598348140717 ],
			"coeffs_84" : [ -0.01237304788082838, -0.08753488212823868, 0.0036330795846879482, -0.09814197570085526 ],
			"coeffs_85" : [ 0.2247355729341507, -0.31180334091186523, 0.07164347916841507, -0.0024081082083284855 ],
			"coeffs_86" : [ 0.15144853293895721, -0.14179310202598572, 0.11685134470462799, 0.0003098322486039251 ],
			"coeffs_87" : [ -0.2228437215089798, -0.19482454657554626, -0.14998795092105865, -0.03636748716235161 ],
			"coeffs_88" : [ -0.23474138975143433, -0.16069018840789795, 0.18028564751148224, 0.11159265786409378 ],
			"coeffs_89" : [ -0.12030552327632904, -0.32348915934562683, -0.018124910071492195, -0.09999512135982513 ],
			"coeffs_90" : [ 0.18777479231357574, 0.053373802453279495, -0.16145087778568268, -0.33042728900909424 ],
			"coeffs_91" : [ 0.1885136365890503, -0.09107732772827148, -0.0848349779844284, -0.12653730809688568 ],
			"coeffs_92" : [ -0.14673224091529846, -0.2579580545425415, -0.09266409277915955, 0.044666267931461334 ],
			"coeffs_93" : [ -0.18943633139133453, -0.15007731318473816, 0.00019108597189188004, 0.019903067499399185 ],
			"coeffs_94" : [ -0.06915096938610077, 0.012713102623820305, 0.05797713249921799, -0.2059662640094757 ],
			"coeffs_95" : [ 0.0027788167353719473, 0.20796753466129303, -0.059911321848630905, -0.1684558391571045 ],
			"coeffs_96" : [ 0.2050318568944931, 0.2540200352668762, 0.15417595207691193, -0.13570989668369293 ],
			"coeffs_97" : [ 0.15534299612045288, 0.32907307147979736, -0.027952874079346657, -0.11422142386436462 ],
			"coeffs_98" : [ 0.0822894275188446, -0.12824644148349762, -0.2268747240304947, 0.18889394402503967 ],
			"coeffs_99" : [ -0.31378263235092163, -0.2250804305076599, 0.13122087717056274, 0.0888107568025589 ],
			"intercepts" : [ 0.3949357271194458, 0.08927428722381592, 0.32938912510871887, 0.0529087595641613 ],
			"name" : "Hidden_Layer_1"
		},
		"Layer_2" : 	{
			"NbInputs" : 4,
			"NbOutputs" : 8,
			"coeffs_0" : [ -0.1685800999403, -0.5466641783714294, -0.4458053410053253, 0.8102748990058899, 0.6121424436569214, -0.12692326307296753, 0.720576822757721, -0.4000591039657593 ],
			"coeffs_1" : [ -0.5119562745094299, 0.023066803812980652, 0.33228379487991333, 0.374819278717041, 0.7577738165855408, -0.06341520696878433, 0.21985435485839844, -0.22880546748638153 ],
			"coeffs_2" : [ 0.6339100003242493, 0.3232060968875885, 0.860230565071106, -0.40856653451919556, -0.1750306934118271, 0.1661326289176941, 0.10473556816577911, -0.52614825963974 ],
			"coeffs_3" : [ 0.09573432803153992, 0.4767875075340271, -0.37023112177848816, -0.4286535382270813, -0.35854285955429077, 0.8369846343994141, -0.15308110415935516, -0.5638708472251892 ],
			"intercepts" : [ 0.676727831363678, -0.4145364761352539, 0.2881496250629425, 0.8979820609092712, -0.396506667137146, 0.09637556225061417, 0.6045925617218018, 0.7703754305839539 ],
			"name" : "Hidden_Layer_2"
		},
		"Layer_3" : 	{
			"NbInputs" : 8,
			"NbOutputs" : 6,
			"coeffs_0" : [ -0.18494470417499542, -0.4295361638069153, -0.5082838535308838, -0.3946099877357483, 0.23840054869651794, 0.7912697196006775 ],
			"coeffs_1" : [ 0.1571023315191269, -0.36643725633621216, -0.5762947201728821, 0.2765873670578003, -0.4406049847602844, 0.5118321180343628 ],
			"coeffs_2" : [ 0.08692803978919983, -0.11614560335874557, -0.33294960856437683, 0.23778806626796722, -0.5995256304740906, 0.46650663018226624 ],
			"coeffs_3" : [ 0.8157171607017517, 0.10445178300142288, 0.16569629311561584, 0.4718257784843445, 0.5242076516151428, -0.31141626834869385 ],
			"coeffs_4" : [ 0.36411386728286743, -0.0073316143825650215, 0.3563286364078522, -0.0019980755168944597, 0.4636210799217224, -0.6519295573234558 ],
			"coeffs_5" : [ -0.5161420702934265, 0.11516762524843216, 0.45510801672935486, -0.22301200032234192, -0.39775124192237854, -0.21374452114105225 ],
			"coeffs_6" : [ -0.3230380713939667, 0.057812415063381195, -0.7335559129714966, 0.6477779746055603, 0.6081113815307617, 0.4978978633880615 ],
			"coeffs_7" : [ 0.1263805627822876, -0.3889494836330414, 0.5204216837882996, 0.75022292137146, 0.20979195833206177, -0.08104754239320755 ],
			"intercepts" : [ 0.8050884008407593, -0.4617977738380432, -0.569695770740509, -0.07053317874670029, 0.6170582175254822, -0.40658220648765564 ],
			"name" : "Hidden_Layer_3"
		},
		"Layer_4" : 	{
			"NbInputs" : 6,
			"NbOutputs" : 1,
			"coeffs_0" : [ 0.9787637591362 ],
			"coeffs_1" : [ -0.029448237270116806 ],
			"coeffs_2" : [ 0.5562142729759216 ],
			"coeffs_3" : [ 1.0042555332183838 ],
			"coeffs_4" : [ 0.9054757356643677 ],
			"coeffs_5" : [ -0.749279797077179 ],
			"intercepts" : [ -0.22989222407341003 ],
			"name" : "Output_Layer"
		},
		"sizes" : [ 100, 4, 8, 6, 1 ]
	},
	"metadata" :  { "model" : "sklearn.neural_network._multilayer_perceptron.MLPRegressor", "version" : "1.4.1.post1" },
	"options" :  { "activation" : "relu", "alpha" : 0.0001, "batch_size" : "auto", "beta_1" : 0.9, "beta_2" : 0.999, "early_stopping" : false, "epsilon" : 1e-08, "hidden_layer_sizes" : [ 4, 8, 6 ], "learning_rate" : "constant", "learning_rate_init" : 0.001, "max_fun" : 15000, "max_iter" : 32, "momentum" : 0.9, "n_iter_no_change" : 10, "nesterovs_momentum" : true, "power_t" : 0.5, "random_state" : 1789, "shuffle" : false, "solver" : "adam", "tol" : 0.0001, "validation_fraction" : 0.1, "verbose" : false, "warm_start" : false }
}
BEAUTIFIED_JSON_END
('OPERATION_START', 'PREDICT')
[ 2.6275558 14.966546  -1.4201357 ... -0.6440252  3.4079986 -1.5342575]
('OPERATION_END_ELAPSED', 0.002, 'PREDICT')
MODEL_PERFS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'RandomReg_100_original', 'size': 1024, 'mse': 22859.648, 'mae': 120.18738, 'mape': 1.0091418, 'r2': 0.04749028283515777}
WRITING_PERF_CODE 'logs/auto_tests/regression/MLPRegressor/sklearn.neural_network._multilayer_perceptron.MLPRegressor_RandomReg_100_original_option_1.perf'

MODEL_PERFS_TIMINGS {'class_name': 'sklearn.neural_network._multilayer_perceptron.MLPRegressor', 'model_name': 'MLPRegressor', 'options': '{"hidden_layer_sizes" : [4, 8, 6]}', 'dataset': 'RandomReg_100_original', 'training_time_in_sec': 0.337, 'prediction_time_in_sec': 0.002}
